<!doctype html><html lang=en dir=auto data-theme=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Resurrecting valuable expired domains | mbrt blog</title><meta name=keywords content="cloud,data-science"><meta name=description content="I processed a large fraction of the Internet on a single machine, trying to find
some valuable but forgotten domain names to buy (probably 25 years too late).
Here&rsquo;s the story of what I found and how I did it, and not the story of
why I decided to try, because I don&rsquo;t know myself.
Spoiler alert: If you just want to know whether there is indeed something,
and you should scramble to scoop it up to become a
domain-flipper
billionaire, hold your horses: I believe there isn&rsquo;t much. The following
sections are more focused on the engineering side, so if you only care about the
conclusion, skip to Results."><meta name=author content="Michele Bertasi"><link rel=canonical href=https://blog.mbrt.dev/posts/domain-resurrect/><link crossorigin=anonymous href=/assets/css/stylesheet.343cc480b9ffc8f04ccbe5e968ad674880cab773ec19905e93033065c1e7a804.css integrity="sha256-NDzEgLn/yPBMy+XpaK1nSIDKt3PsGZBekwMwZcHnqAQ=" rel="preload stylesheet" as=style><link rel=icon href=https://blog.mbrt.dev/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://blog.mbrt.dev/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://blog.mbrt.dev/favicon-32x32.png><link rel=apple-touch-icon href=https://blog.mbrt.dev/apple-touch-icon.png><link rel=mask-icon href=https://blog.mbrt.dev/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://blog.mbrt.dev/posts/domain-resurrect/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51);color-scheme:dark}.list{background:var(--theme)}.toc{background:var(--entry)}}@media(prefers-color-scheme:light){.list::-webkit-scrollbar-thumb{border-color:var(--code-bg)}}</style></noscript><script>localStorage.getItem("pref-theme")==="dark"?document.querySelector("html").dataset.theme="dark":localStorage.getItem("pref-theme")==="light"?document.querySelector("html").dataset.theme="light":window.matchMedia("(prefers-color-scheme: dark)").matches?document.querySelector("html").dataset.theme="dark":document.querySelector("html").dataset.theme="light"</script><meta property="og:url" content="https://blog.mbrt.dev/posts/domain-resurrect/"><meta property="og:site_name" content="mbrt blog"><meta property="og:title" content="Resurrecting valuable expired domains"><meta property="og:description" content="I processed a large fraction of the Internet on a single machine, trying to find some valuable but forgotten domain names to buy (probably 25 years too late). Here’s the story of what I found and how I did it, and not the story of why I decided to try, because I don’t know myself.
Spoiler alert: If you just want to know whether there is indeed something, and you should scramble to scoop it up to become a domain-flipper billionaire, hold your horses: I believe there isn’t much. The following sections are more focused on the engineering side, so if you only care about the conclusion, skip to Results."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-10-31T00:00:00+00:00"><meta property="article:modified_time" content="2025-10-31T00:00:00+00:00"><meta property="article:tag" content="Cloud"><meta property="article:tag" content="Data-Science"><meta name=twitter:card content="summary"><meta name=twitter:title content="Resurrecting valuable expired domains"><meta name=twitter:description content="I processed a large fraction of the Internet on a single machine, trying to find
some valuable but forgotten domain names to buy (probably 25 years too late).
Here&rsquo;s the story of what I found and how I did it, and not the story of
why I decided to try, because I don&rsquo;t know myself.
Spoiler alert: If you just want to know whether there is indeed something,
and you should scramble to scoop it up to become a
domain-flipper
billionaire, hold your horses: I believe there isn&rsquo;t much. The following
sections are more focused on the engineering side, so if you only care about the
conclusion, skip to Results."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://blog.mbrt.dev/posts/"},{"@type":"ListItem","position":2,"name":"Resurrecting valuable expired domains","item":"https://blog.mbrt.dev/posts/domain-resurrect/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Resurrecting valuable expired domains","name":"Resurrecting valuable expired domains","description":"I processed a large fraction of the Internet on a single machine, trying to find some valuable but forgotten domain names to buy (probably 25 years too late). Here\u0026rsquo;s the story of what I found and how I did it, and not the story of why I decided to try, because I don\u0026rsquo;t know myself.\nSpoiler alert: If you just want to know whether there is indeed something, and you should scramble to scoop it up to become a domain-flipper billionaire, hold your horses: I believe there isn\u0026rsquo;t much. The following sections are more focused on the engineering side, so if you only care about the conclusion, skip to Results.\n","keywords":["cloud","data-science"],"articleBody":"I processed a large fraction of the Internet on a single machine, trying to find some valuable but forgotten domain names to buy (probably 25 years too late). Here’s the story of what I found and how I did it, and not the story of why I decided to try, because I don’t know myself.\nSpoiler alert: If you just want to know whether there is indeed something, and you should scramble to scoop it up to become a domain-flipper billionaire, hold your horses: I believe there isn’t much. The following sections are more focused on the engineering side, so if you only care about the conclusion, skip to Results.\nA valuable domain Domain names have a fixed registration period. They have to be periodically renewed, and sometimes they aren’t. The question was: is there some domain, among those forgotten, that is worth resurrecting?\nBut first, what makes a domain valuable? I asked a few people, and the first answer they came up with was this: a memorable or short name, or attached to a known brand, or something that rings cool.\nI believe this is too subjective. For example, food.com is arguably a good sounding domain, but what about food.de.com? Or asfood.com? Why is the latter estimated to cost $76000, while the former just $22/year? It’s not just the .com top level domain.\nI thought that a more quantitative (and therefore easier) way to derive the value of a domain would be to estimate its position in search results ranking.\nThe idea My idea was to compute Page Rank for every domain (assuming it’s a good proxy for their value), and use it to sort candidate expired domains. So, I started with one problem, and ended up with two:\nHow to calculate page ranks of the entire Internet. How to identify expired domains. There’s no direct way to get search scores from Google, so I thought to approximate that by computing Page Rank myself. Although not really in use anymore, it powered the early Google and made it extremely successful, before being replaced by the zoo of algorithms, heuristics, models and manual curation of today. I thought it should serve as a good base.\nAnd here’s where waiting 25 years helps, because thanks to Common Crawl, we don’t need to crawl anything. We have monthly datasets with most of the web, crawled and pre-processed for us, for free. Problem number one becomes: “how to calculate PageRank over that dataset”, which is a much easier problem to solve.\nFor the second problem (identifying expired domains), we could think of comparing several snapshots of Common Crawl, and look for domains that disappeared, but that’s expensive. There’s indeed an easier way: look at which domains are being referenced in links, but they themselves are not in the dataset. 1\nSingle machine? The Internet is big. Even though Common Crawl doesn’t have all of it, its latent snapshot is currently counting 421 TiB of content. Something of this size is not impossible to handle, but also not cheap. Processing it sequentially at 1 Gbit/s (which is not particularly slow) would take 42 days.\nFortunately, we can avoid most of this data and focus on the WAT subset, which is 76 TiB (16 TiB compressed) of JSON metadata about pages and their links.\nMore than answering the original question, then I became interested in the technical challenge: could I tackle the problem with a single machine in reasonable time? And it turns out the answer was yes.\nAfter keeping 32 cores 100% busy for about 13h, downloading 16 TiB of data, processing 7+ billion pages and 83+ billion links [logs, infrastructure], I got my answer in the form of less than 2 GiB of parquet files: a list of domain names, sorted by rank.\nBut I’m getting ahead of myself. How did I know that this had a chance to work? I tried to work it out from first principles by estimating resource usage and time: network bandwidth, memory, CPU and disk.\nNetwork The first problem was how to download 16 TiB of data. If this were to go through a slow link from the Internet, I would have been worried, but Common Crawl datasets are also in S3. A VM in the same region can download objects in parallel to the point of saturating its network bandwidth. Staying between 1 and 10 Gbit/s would take this process between 4h and 40h, which seemed acceptable.\nWith this first constraint, I got the first design decision down: it had to run in AWS from us-east-1 (where the crawls are located).\nDisk space The dataset contains about 39 million domains.2 Accounting for an average of 100 bytes per domain (to store the name), this would be about 3.5 GiB uncompressed (which I think is a generous upper bound).\nFor my purposes (domain reputation), I didn’t need to store, nor keep track of individual pages, but just domain names and their link counts. A list of triplets: source_domain, target_domain, count is the simplest way. And if each domain had links to an average of 50 other domains, this would result in 180 GiB of data. With compression, this was definitely going to be lower, and if the estimate was too low, I could store the names just once and have the list of edges as a table of integer indexes, making it at least one order of magnitude smaller.\nIf I processed the data in a streaming fashion from S3 to a table stored locally, it would result in some denormalization (because pages are processed in parallel and the dataset is not ordered), inflating the numbers above. Adding a 10x penalty results in 1.8 TiB of data, but since this is mostly ASCII text, we can account for a 70 - 80% compression, which brings it down to about 500 GiB.\nAll of this sounded manageable on a single disk / volume. Second decision down: stream the results to a local disk is good enough (no need for distributed storage).\nMemory The first part of the process (creating the edges table) can be done in streaming, so I had no memory usage concerns. Aggregating over the table can be memory intensive, but could be done off-core too (paying the penalty of spilling to disk). Page rank is parallelizable (as it runs in a distributed way), so it can also be turned into a sharded off-core computation.\nThis opened a sub-problem: how to compute aggregations over the dataset while spilling to disk?\nDisk speed Running Page Rank over multiple iterations would mean reading and writing the whole dataset several times, and 100 GiB of data written 10 times is about 1 TiB. EBS supports up to 1 GiB/s throughput, and even the default 125 MiB/s would make this possible in about 2h.\nSo, I wasn’t concerned about memory, nor disk.\nCPU This was the least concerning to me, as the problem didn’t seem to need any complex computation: decompress files, parse JSONL, turn URLs into domain names and count.\nWell, here I was wrong, as I underestimated the effort required by decompressing large gzip files (i.e. FLATE), which turned out to be the actual bottleneck.\nI found this out early while running a smaller scale experiment locally (with a handful of WAT files), so I had to adjust things slightly before the actual full run. See Mapper implementation below for more details.\nImplementation The informal feasibility check gave me the feeling that the bottleneck would be streaming the 16 TiB from S3, as the rest was orders of magnitude smaller. Optimizing for that would mean to stream process the data, one thread per core and store it on disk denormalized (to keep memory usage low). After that, compute PageRank one iteration at a time, while check pointing intermediate steps on disk.\nBased on that, I split the problem into two independent parts:\nA mapper, responsible for processing the Common Crawl WAT dataset and writing a set of parquet files with counters for links going from one domain to another (the edges). A reducer: which aggregates the edges (into nodes) and iteratively computes PageRank. Mapper The first implementation of the mapper was a Python script which used a ProcessPoolExecutor to process WAT files in parallel. Very simple and readable implementation, but while testing locally, to my surprise I discovered that it was terribly slow [see profile]. The code would take about a minute to process a single WAT object, which would translate to roughly 16000 CPU hours, given the 100k total files in the dataset.\nThe culprit was mostly domain name computation (eTLD+1), and after trying to replace it in many different ways (e.g. adding heuristics, or using fasttld and seeing a large drop in extracted domains) I decided to give up and rewrite the thing in Go.\nI thought this would be a good match for the language: strong standard library, especially for the web and networking domains, and fast concurrency. Unfortunately, Go doesn’t have an equivalent to the awesome fsspec library, so I had to roll my own, to abstract away how to read from a certain URL (local path, http or s3 URL). Another thing I wasn’t happy with was the support for good estimation of speed and completion time, so I rolled my own as well by using an exponentially weighted moving average (EWMA) and some prettier formatting.\nAlthough that was much faster (down from 60s to 9s per file), there was still low-hanging fruit. Looking at the CPU profile, I noticed a large amount of time spent in JSON parsing, and I suddenly remembered how slow that is in Go (it’s based on reflection and severely under-optimized). So I swapped it for bytedance/sonic, which is a drop-in replacement, and saw another 30% speed up. Down to 6s per file, I was projecting the map phase to finish in 160 CPU hours, which would be about 5h on a 32 cores machine.\nI was ready to start working on the reducer.\nReducer The reducer has a very simple job: take the denormalized input from the previous phase and aggregate it into unique rows of source_domain, target_domain, count on a set of parquet files. This is the type of thing that Polars should eat for breakfast [ref]:\nmap_df = pl.scan_parquet(tmp / \"part_*.parquet\") edges = ( map_df.group_by(\"src_dom\", \"tgt_dom\") .agg(pl.col(\"count\").sum().alias(\"count\")) ) edges.sink_parquet(out / \"edges.parquet\", compression=\"snappy\") But it turned out that out-of-core operations (i.e. swapping to disk) were not yet supported in the new streaming engine (pola-rs/polars#20947). On a 250 GiB dataset (compressed), this was not going to fly, and indeed, on my first run, memory usage exploded and the whole thing crashed.\nAfter several experiments, I was able to simulate out-of-core aggregations by:\nProcessing one map partition at a time Splitting each one into a set of smaller shards, partitioned by src_dom. This made it so that all rows with the same source domain are in the same partition. Aggregate shards with the same partition key together, one by one. This allowed splitting up the larger computation into sequentially independent steps, where I control the size of each. A map reduce could process this in parallel faster, but I just decided to process it sequentially, so it could fit on a single machine.\nPage Rank The second phase of the reducer computes Page Rank. We’ll see in a minute how this turned out to be insufficient, but it’s good to understand the basic version before complicating it.\nThe original paper from Page and Brinn describes the intuition behind it quite well:\nA page has a high rank if the sum of the ranks of its backlinks is high. This covers both the case when a page has many backlinks and when a page has a few highly ranked backlinks.\nThis reduces the problem of establishing the importance of pages to that of analyzing links, rather than contents, of those pages. The paper gives one more intuition on how to compute it, based on the “random surfer” model. The surfer picks a random page to start with and keeps clicking on random links until it “gets bored”, where it starts over from (i.e. “teleports to”) another random page.\nMore technically, the Page Rank of page P is the steady state probability that the random surfer is at page P. The more a page is ranked high, the higher the probability that a “random surfer” lands on that page. The formulation is very short:\nWhere:\nα is the damping factor (usually set at 0.85), which determines the probability of picking a random page over following outlinks. B(u) is the set of pages linking to page u directly. N is the total number of pages. The formula is iterative and proven to converge.\nGoing back to the intuitive explanation, the rank of a page is the combination of two factors: the “mass” of people navigating to it because they follow links in pages that point there, and the ones that just picked a random page. The combination is modulated by the damping factor α.3\nI introduced a small variation on that, given that I didn’t need the rank of every page, but the aggregate rank of their domains. In this setup, domains with large amounts of outlinks end up distributing rank equally to domains linked only once and linked thousands of times. To account for this skew, I divide contributions of a node by the total number of its outlinks. In this way, each single link counts as a “vote” for the target domains, and the more links point there, the more “votes” it gets.\nThe basic implementation of that in Polars is straightforward:\n# Contributions contribs = ( edges .join(scores, left_on=\"src_dom\", right_on=\"node\") .with_columns( ( pl.col(\"score\") * pl.col(\"count\") / pl.col(\"out_weight\") ).alias(\"contrib\") ) .group_by(\"tgt_dom\") .agg(pl.sum(\"contrib\").alias(\"contrib\")) .rename({\"tgt_dom\": \"node\"}) .select([\"node\", \"contrib\"]) ) # New scores new_scores = ( scores .join(contribs, on=\"node\", how=\"left\") .with_columns( (damping * pl.col(\"contrib\").fill_null(0.0) + (1 - damping) / N).alias(\"score\") ) .cast({\"out_weight\": pl.Int64}) .select([\"node\", \"out_weight\", \"score\"]) ) Off-core As with the edges reduce phase, this simple approach doesn’t work right out-of-the-box in Polars, given the lack of off-core computation. Again, I had to simulate it by shuffling contributions to the right shards and compute each one separately:\nThe actual code is here. I was ready to give this a try, so I let this run overnight, and got some surprises in the morning.\nPage Rank take II (variants) I knew the Internet was full of crap. I just didn’t know the extent of it. It’s literally, full - of - crap. After the first few obviously good results at the top of the ranking (google.com, facebook.com, etc.) a lot of dubious or outright bad domains came up. This is the predictable result of running a 25+ years old algorithm on today’s Internet. I thought I could ignore the problem, because I just needed to look at top results, but spam can rank high too!\nSee, for example, this analysis, where I try to understand why versiti.co and balsic-fastive.io rank so high in my run. It comes down to the fact of having two highly ranked domains (wikipedia.org and zendesk.com) linking to focusvision.com, which was then considered very highly ranked. This site was only partially crawled, so it had just a few links in it, and one turned out to be towards versiti.co.\nI had to descend a deeper rabbit hole on how to combat spam, but I didn’t need to remove all of it. I just needed to push it down enough for me to find some decent expired domain higher up. This was a much smaller problem to solve than the general case Google has to deal with.\nTo start, I needed to find a list of spam and trusted domains, and this is one area where being 25 years late went to my advantage, because there’s plenty of choice.\nAs a first set of attempts, I tried to implement some PageRank variations straight from the literature:\nTrustRank [Gyongyi, Z., et. al, 2004.], biases the teleport vector towards a curated set of trusted pages. Anti-Trust Rank [Krishnan, V. and Raj, R., 2006], starts from the opposite direction and demotes pages by looking at whether they point to a curated set of spam pages. MaxRank [Fercoq, O., 2012], modifies the graph by removing links going to spam, while minimizing a cost function that pays some cost on both landing on spam, and on removing links. I tried the first two on the full dataset and found them to perform quite badly. After checking some results empirically, I discovered two major problems:\nA lot of “trusted” domains point to spam (as well as good targets). A favorite trick from spammers and SEO sites is to “land” a link from a reputable place to their site. There are many ways to do this, but an easy one is to add a link pointing to a spam site in the comment section of a reputable site. Or edit a half forgotten Wikipedia article. Spam sites might point to both spam and non-spam. Spammers do this to muddy the waters, so it’s hard to propagate distrust from a small set of spam domains. Both spam and non-spam looks similar in that regard: they both point to some good and bad pages. A problem specific to Anti-Trust rank is that it doesn’t really take into consideration the proportion by which a domain is pointing to spam. The extreme example was Wikipedia, classified as the #3 most spammy domain on the Internet, just because in its 11M outlinks there’s some amount of spam. But this is to be expected in such a massive crowdsourced site.\nAlthough the first two algorithms are quite intuitive (the changes are simple and the papers are well written), I cannot say the same for the third (MaxRank). For one thing, it requires dynamic programming and a globally sorted “cost” vector, both of which will impact runtime. The second, and most important part, is that I cannot really explain intuitively why this should work at all. Modeling the problem as a cost that balances between keeping and removing links towards spam seemed quite arbitrary to me. But perhaps it’s me not understanding what the paper is trying to say. It’s certainly densely fitted with proofs for properties that don’t really move the needle of my understanding.\nLong story short, I decided to implement my own version, by combining the literature above and fixing a few issues I saw in the experimental data.\nPage Rank take III (Spamicity Trust Rank) I don’t know what to call it, so allow me to throw some arbitrary words together: Spamicity Trust Rank. It may be nonsense, but it combines the concepts I used:\nA Page Rank variant that still uses hyperlinks to infer “importance and influence” of domains. Trust Rank, because it uses a trusted set of domains as seeds. Spamicity, because it also assigns spam scores, based on a modified Anti-TrustRank pass. The implementation is done in two steps:\nspamicity, and trust_rank. The goal of the first pass is to discover as much spam as possible, by starting from a dataset of known spam domains. Anti-Trust Rank was designed with this goal in mind, but massively suffers from the problem presented above of punishing popular domains, regardless of them being spam or not (just because some links towards spam creeped in).\nTo counter this problem, I started from the algorithm as described by the original paper:\nInverse page-rank is computed by reversing the in-links and out-links in the web graph. In other words, it merely involves running pagerank on the transpose of the web graph matrix.\nBut add a twist: scale the spam contribution by the factor in which the source domain points to the target domain. The intuition here is that if a domain points to spam, it should be classified as spam, but the score should be reduced by the proportion of how much it actually does point to spam. I also include a damping factor, to progressively attenuate the score over distance (if a links to b that links to c, we expect the score of c to influence b more than it would influence a).\nThe more precise formulation is as follows:\nAnd the code (using Polars):\ncontribs = ( edges .join(nodes, left_on=\"src_dom\", right_on=\"node\") .with_columns( ( pl.col(\"count\") * pl.col(\"score\") ).alias(\"contrib\") ) .group_by(\"tgt_dom\") .agg(pl.sum(\"contrib\").alias(\"contrib\")) .select([\"tgt_dom\", \"contrib\"]) ) contribs = ( contribs.group_by(\"tgt_dom\") .agg(pl.col(\"contrib\").sum().alias(\"contrib\")) ) updated = ( nodes .join(contribs, left_on=\"node\", right_on=\"tgt_dom\", how=\"left\") .with_columns( ( ( damping * pl.col(\"contrib\").fill_null(strategy=\"zero\") / pl.col(\"out_weight\") ).fill_nan(0) + (1 - damping) * pl.col(\"bias\") ) .alias(\"score\") ) .drop(\"contrib\") ) To explain with an example: suppose that src1.com points to tgt.com and spam.com, with weights 3 and 1 (i.e. its number of out-links).\nTo calculate the spamicity score of src1.com, we add the scores of its target domains (0 and 0.1), scaled down by the damping factor (0.85), and do a weighted average based on the number of outlinks:\ntgt.com contributes with a score of 0 spam.com has a score of 0.1, dampened by 0.85 (constant factor), scaled by ¼, as there’s only one out-link from the source, over 4 total. Given those spam scores, we run a biased PageRank that takes spamicity into consideration. The first intuition is that it’s more likely that when a user goes to a random page, they land on a popular domain, so we can use domain popularity datasets as a basis. The second is about spam domains: you can trick a user to click to a link that points to spam, however when they land there, it’s unlikely that they will stay and follow any further links (therefore they will jump to somewhere else).\nThese intuitions are modeled by biasing the teleport vector with domain popularity, and by scaling down contributions of spam domains – as classified by the first step – so that their impact on other domains will be limited. In other words, the popularity of a spam domain doesn’t translate into popularity of its outgoing links [code]:\n# Compute contributions and penalize based on spamicity # up to a certain threshold contribs = ( edges .join(nodes, left_on=\"src_dom\", right_on=\"node\") .with_columns( ( pl.col(\"score\") * pl.col(\"count\") / pl.col(\"out_weight\") * (1.0 - (max_spam_penalty * pl.col(\"spamicity\"))) ) .alias(\"contrib\"), ) .group_by(\"tgt_dom\") .agg(pl.sum(\"contrib\").alias(\"contrib\")) .select([\"tgt_dom\", \"contrib\"]) ) # Sum all contributions going to the same target contribs = ( contribs.group_by(\"tgt_dom\") .agg(pl.col(\"contrib\").sum().alias(\"contrib\")) ) # Compute the updated scores by summing up the two factors: # Contributions from backlinks and initial popularity estimates. updated = ( nodes .join(contribs, left_on=\"node\", right_on=\"tgt_dom\", how=\"left\") .with_columns( ( damping * pl.col(\"contrib\").fill_null(strategy=\"zero\") + (1 - damping) * pl.col(\"bias\") ) .alias(\"score\") ) .drop(\"contrib\") ) See how contributions are penalized (up to a certain max) for domains considered spammy, and the teleport contribution is biased by the “bias” column, which is initialized with larger weight for popular domains and uniformly lower weights for the rest.\nDangling domains After getting a ranked list of domains, it was time to find the dangling ones. For that, I looked for those that simply had no outlinks. If they have many domains pointing to them, but none going out, there’s a chance the domain wasn’t crawled because it expired.\nWell, not quite. There are many reasons why domains have no outlinks in CommonCrawl: robots.txt didn’t allow the crawl, or the domain is not serving http content at all, or the site was down, but the domain is still registered to someone.\nA good way to filter through this is by combining the filter for no outlinks with a DNS resolution. If that returns NXDOMAIN, then there’s a much better chance that the domain actually expired. The code is quite simple:\nasync def resolve(domain: str, sem: asyncio.Semaphore) -\u003e tuple[str, bool]: async with sem: try: _ = await resolver.resolve(domain) return domain, True except dns.NXDOMAIN: return domain, False except Exception: # Treat other errors as \"something went wrong, but domain may exist\" return domain, True This resulted into a list of candidates (I kept about a hundred) that I checked manually. And this is the part where I would have loved to say: “and this is how I made my first million”. However, the results mostly split into three categories:\nDomains that looked interesting, but someone was already sitting on them, likely waiting for a buyer. Domains that were boosted by much more clever spam sources (e.g. fake blogs and fake companies, likely machine / AI generated). Shady domains that I definitely didn’t want to touch. Perhaps the first category might become a future good buy (if the owner just loses hope of selling and lets it expire), but I assume that several drop-catchers are already watching and will be faster than regular people in securing them right then.\nResults? When I said that I was 25 years late to the problem, I meant: a lot of people thought about this very same problem for a very long time. So much that there is plenty of terminology, research papers and valuable companies built around understanding and exploiting it: this is the weird universe of SEO, Spamdexing, Google bombing, Domain name speculation and Domain drop catching.\nSo, what’s the probability that there indeed is some hidden gem, forgotten by everybody, waiting just for me to take? Probably close to zero. But I wanted to try anyway.\nAt this point, I could continue optimizing and improving the ranking algorithms, but decided not to. The first category of domains I found above (domains that are interesting but not available), made me think that whatever is mildly interesting, was already found by somebody.\nOK, so what was the point of this post? Perhaps it looks like a flex, or a pointless exercise of someone with too much time on their hands. Maybe there’s some of that, but I had a lot of fun working on this project and wanted to share it. I might be in a small niche, but I also love reading posts like this one, so I wanted to contribute to the genre with something of my own.\nSome takeaways from this experiment:\nI feel like out-of-core aggregations are generally easy to fake with streaming capabilities, even when they are not built-in in the data framework one is using. See Generalized out-of-core aggregations. One more empirical proof of the value of back-of-the-envelope design, combined with small scale experiments (see Single machine?). The Internet is unbelievably full of crap. Even with very low expectations, I was still surprised by how much this is true. If you feel like I got things horribly wrong, and there are ways to make easy money with this data, be my guest! You can run things from scratch, starting from my code (messy but includes all experiments as well) or save a bunch of time and money by starting with the final data:\nScored domains dataset Links dataset Both of which are based on the September Common Crawl dataset, and are massively smaller (down to 35 GiB from 76 TiB).\nI didn’t get rich in the process, but I learned a thing or two about optimizing data ingestion, polars and PageRank variants. The journey was more important than the destination, etc. etc.\nAppendix Or, an unordered list of things I didn’t know where to put but still thought interesting.\nGeneralized out-of-core aggregations I think we can extrapolate a way to generalize to out-of-core aggregations from the particular example of the reducer implementation:\nUse the columns in the group-by clause as sharding key Calculate the number of necessary shards to fit a single one in memory, based on statistics on the number of unique elements in each column. Stream the source table to a set of separate tables, partitioned by the sharding key. Process shards one by one, instead of in parallel. Step number 2 is perhaps the hardest to get correctly, as memory usage depends on many factors. It will be hard to guess a good number of shards without prior knowledge. A good approximation might be to abound on the number of shards and then compute some of them in parallel in step number 4. If memory usage grows too large, kill some of these shards and postpone their computation to subsequent sequential steps.\nRewrite the mapper in Rust? Before someone starts screaming: “bro, you should have started with Rust, it’s faster, safer, and blah blah”, I want to show my little benchmark, where I simply ran the most compute intensive part of the mapper (i.e. gzip decompress) by using the fastest available library in Rust, and couldn’t really see a speed improvement. It turns out, the Go version is very well optimized.\nIn summary, I don’t expect a Rust implementation to be significantly (i.e. orders of magnitude) faster than the one I came up with. Not to mention that I’m more familiar with Go and Python, and they ran fast enough for my purposes.\nInfrastructure Based on the back-of-the-envelope estimates above and a small scale run locally from my laptop (done by downloading WAT files by hand and processing them locally), I concluded that each file would take an average of 6s of CPU time to process. Given that the dataset has 100k files, this roughly means 160 CPU hours.\nSince compute was the bottleneck (and not network), I just needed a compute-optimized instance with enough disk to get the job done. This pointed me towards a c6id.8xlarge VM, with 32 cores and 1.9 TiB of SSD, which costs about $1.6/h. The whole process should have taken less than 24 hours, and therefore less than $38 in total.\nInfrastructure as code CloudFormation fell out of favor among platform and cloud engineers nowadays, in favor of Terraform or fancier solutions like Pulumi and Crossplane. I still find the ease of setting up and use of CloudFormation simply unbeatable, as you just need the AWS CLI and an AWS account. No need to deploy extra things, or manage the state somewhere. Given that I had to run within AWS, it felt like a very natural choice.\nAnd hey, GitHub Copilot and other AI assistants really like it, so they speed up the creation a lot by making good suggestions.\nSee config and deploy script.\nWe’ll see later that this is a good starting point, but not sufficient. ↩︎\nInterestingly, this is one order of magnitude less than the reported number of domains from DNIB. ↩︎\nI couldn’t find an explanation for why the damping factor is usually set to 0.85. I ran a small scale ablation on notable pages with different values and concluded empirically that this indeed looks reasonable. When it’s set to a lower value (e.g. 0.5), ranks seem to vanish very quickly, and for higher values, they propagate unreasonably further away from the source. For example, a single link from google.com would positively impact pages 3-4 degrees separated away, thereby boosting a lot of spam. ↩︎\n","wordCount":"5131","inLanguage":"en","datePublished":"2025-10-31T00:00:00Z","dateModified":"2025-10-31T00:00:00Z","author":{"@type":"Person","name":"Michele Bertasi"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://blog.mbrt.dev/posts/domain-resurrect/"},"publisher":{"@type":"Organization","name":"mbrt blog","logo":{"@type":"ImageObject","url":"https://blog.mbrt.dev/favicon.ico"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://blog.mbrt.dev/ accesskey=h title="mbrt blog (Alt + H)">mbrt blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://blog.mbrt.dev/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://blog.mbrt.dev/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://blog.mbrt.dev/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://blog.mbrt.dev/>Home</a>&nbsp;»&nbsp;<a href=https://blog.mbrt.dev/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Resurrecting valuable expired domains</h1><div class=post-meta><span title='2025-10-31 00:00:00 +0000 UTC'>October 31, 2025</span>&nbsp;·&nbsp;<span>25 min</span>&nbsp;·&nbsp;<span>Michele Bertasi</span></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#a-valuable-domain aria-label="A valuable domain">A valuable domain</a></li><li><a href=#the-idea aria-label="The idea">The idea</a></li><li><a href=#single-machine aria-label="Single machine?">Single machine?</a><ul><li><a href=#network aria-label=Network>Network</a></li><li><a href=#disk-space aria-label="Disk space">Disk space</a></li><li><a href=#memory aria-label=Memory>Memory</a></li><li><a href=#disk-speed aria-label="Disk speed">Disk speed</a></li><li><a href=#cpu aria-label=CPU>CPU</a></li></ul></li><li><a href=#implementation aria-label=Implementation>Implementation</a><ul><li><a href=#mapper aria-label=Mapper>Mapper</a></li><li><a href=#reducer aria-label=Reducer>Reducer</a></li><li><a href=#page-rank aria-label="Page Rank">Page Rank</a></li><li><a href=#off-core aria-label=Off-core>Off-core</a></li><li><a href=#page-rank-take-ii-variants aria-label="Page Rank take II (variants)">Page Rank take II (variants)</a></li><li><a href=#page-rank-take-iii-spamicity-trust-rank aria-label="Page Rank take III (Spamicity Trust Rank)">Page Rank take III (Spamicity Trust Rank)</a></li><li><a href=#dangling-domains aria-label="Dangling domains">Dangling domains</a></li></ul></li><li><a href=#results aria-label=Results?>Results?</a></li><li><a href=#appendix aria-label=Appendix>Appendix</a><ul><li><a href=#generalized-out-of-core-aggregations aria-label="Generalized out-of-core aggregations">Generalized out-of-core aggregations</a></li><li><a href=#rewrite-the-mapper-in-rust aria-label="Rewrite the mapper in Rust?">Rewrite the mapper in Rust?</a></li><li><a href=#infrastructure aria-label=Infrastructure>Infrastructure</a></li><li><a href=#infrastructure-as-code aria-label="Infrastructure as code">Infrastructure as code</a></li></ul></li></ul></div></details></div><div class=post-content><p>I processed a large fraction of the Internet on a single machine, trying to find
some valuable but forgotten domain names to buy (probably 25 years too late).
Here&rsquo;s the story of what I found and how I did it, and <strong>not</strong> the story of
<strong>why</strong> I decided to try, because I don&rsquo;t know myself.</p><p><strong>Spoiler alert</strong>: If you just want to know whether there is indeed something,
and you should scramble to scoop it up to become a
<a href=https://en.wikipedia.org/wiki/Domain_name_speculation>domain-flipper</a>
billionaire, hold your horses: I believe there isn&rsquo;t much. The following
sections are more focused on the engineering side, so if you only care about the
conclusion, skip to <a href=/posts/domain-resurrect/#results>Results</a>.</p><h2 id=a-valuable-domain>A valuable domain<a hidden class=anchor aria-hidden=true href=#a-valuable-domain>#</a></h2><p>Domain names have a fixed registration period. They have to be periodically
renewed, and sometimes they aren&rsquo;t. The question was: is there some domain,
among those forgotten, that is worth resurrecting?</p><p>But first, what makes a domain valuable? I asked a few people, and the first
answer they came up with was this: a memorable or short name, or attached to a
known brand, or something that rings cool.</p><p>I believe this is too subjective. For example, <code>food.com</code> is arguably a good
sounding domain, but what about <code>food.de.com</code>? Or <code>asfood.com</code>? Why is the
latter estimated to cost $76000, while the former just $22/year? It&rsquo;s not just
the <code>.com</code> top level domain.</p><p>I thought that a more quantitative (and therefore easier) way to derive the
value of a domain would be to estimate its position in search results ranking.</p><h2 id=the-idea>The idea<a hidden class=anchor aria-hidden=true href=#the-idea>#</a></h2><p>My idea was to compute <a href=https://en.wikipedia.org/wiki/PageRank>Page Rank</a> for
every domain (assuming it&rsquo;s a good proxy for their value), and use it to sort
candidate expired domains. So, I started with one problem, and ended up with
two:</p><ul><li>How to calculate page ranks of the <em>entire Internet</em>.</li><li>How to identify expired domains.</li></ul><p>There&rsquo;s no direct way to get search scores from Google, so I thought to
approximate that by computing Page Rank myself. Although not really in use
anymore, it powered the early Google and made it extremely successful, before
being replaced by the zoo of algorithms, heuristics, models and manual curation
of today. I thought it should serve as a good base.</p><p>And here&rsquo;s where waiting 25 years helps, because thanks to <a href=https://commoncrawl.org/>Common
Crawl</a>, we don&rsquo;t need to crawl anything. We have
monthly datasets with most of the web, crawled and pre-processed for us, for
free. Problem number one becomes: &ldquo;how to calculate PageRank over that dataset&rdquo;,
which is a <em>much easier problem to solve</em>.</p><p>For the second problem (identifying expired domains), we could think of
comparing several snapshots of Common Crawl, and look for domains that
disappeared, but that&rsquo;s expensive. There&rsquo;s indeed an easier way: look at which
domains are being referenced in links, but they themselves are <em>not</em> in the
dataset. <sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup></p><h2 id=single-machine>Single machine?<a hidden class=anchor aria-hidden=true href=#single-machine>#</a></h2><p>The Internet is big. Even though Common Crawl doesn&rsquo;t have all of it, its latent
snapshot is <a href=https://commoncrawl.org/blog/september-2025-crawl-archive-now-available>currently
counting</a>
421 TiB of content. Something of this size is not impossible to handle, but also
not cheap. Processing it sequentially at 1 Gbit/s (which is not particularly
slow) <a href="https://numbat.dev/?q=421+TiB+%2F+%281+Gbit%2Fs%29+-%3E+days%E2%8F%8E">would take 42
days</a>.</p><p>Fortunately, we can avoid most of this data and focus on the <a href=https://commoncrawl.org/get-started#WAT-Format>WAT
subset</a>, which is <strong>76 TiB</strong> (16
TiB compressed) of JSON metadata about pages and their links.</p><p>More than answering the original question, then I became interested in the
technical challenge: could I tackle the problem with a single machine in
reasonable time? And it turns out the answer was yes.</p><p>After keeping 32 cores 100% busy for about 13h, downloading 16 TiB of data,
processing 7+ billion pages and 83+ billion links
[<a href=https://github.com/mbrt/domain-resurrect/blob/fdb34189c583f899d3f45659c128315c5c6421ac/docs/logs/20250925-144437.log>logs</a>,
<a href=/posts/domain-resurrect/#infrastructure>infrastructure</a>], I got my answer in the form of less than 2
GiB of parquet files: a list of domain names, sorted by rank.</p><p>But I&rsquo;m getting ahead of myself. How did I know that this had a chance to work?
I tried to work it out from first principles by estimating resource usage and
time: network bandwidth, memory, CPU and disk.</p><h3 id=network>Network<a hidden class=anchor aria-hidden=true href=#network>#</a></h3><p>The first problem was how to download 16 TiB of data. If this were to go through
a slow link from the Internet, I would have been worried, but Common Crawl
datasets are also in S3. A VM in the same region can download objects in
parallel to the point of saturating its network bandwidth. Staying between 1 and
10 Gbit/s <a href="https://numbat.dev/?q=16+TiB+%2F+%2810+Gbit%2Fs%29+-%3E+h%E2%8F%8E">would
take</a> this
process between 4h and 40h, which seemed acceptable.</p><p>With this first constraint, I got the first design decision down: it had to run
in AWS from <code>us-east-1</code> (where the crawls are located).</p><h3 id=disk-space>Disk space<a hidden class=anchor aria-hidden=true href=#disk-space>#</a></h3><p>The dataset contains about <a href=https://commoncrawl.org/blog/september-2025-crawl-archive-now-available>39
million</a>
domains.<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup> Accounting for an average of 100 bytes per domain (to store the name),
this would be about 3.5 GiB uncompressed (which I think is a generous upper
bound).</p><p>For my purposes (domain reputation), I didn&rsquo;t need to store, nor keep track of
individual pages, but just domain names and their link counts. A list of
triplets: <code>source_domain, target_domain, count</code> is the simplest way. And if each
domain had links to an average of 50 other domains, this <a href="https://numbat.dev/?q=39+million+*+100+B%E2%8F%8E_+-%3E+GiB%E2%8F%8E_+*+50%E2%8F%8E">would
result</a>
in 180 GiB of data. With compression, this was definitely going to be lower, and
if the estimate was too low, I could store the names just once and have the list
of edges as a table of integer indexes, making it at least one order of
magnitude smaller.</p><p>If I processed the data in a streaming fashion from S3 to a table stored
locally, it would result in some denormalization (because pages are processed in
parallel and the dataset is not ordered), inflating the numbers above. Adding a
10x penalty results in 1.8 TiB of data, but since this is mostly ASCII text, we
can account for a 70 - 80% compression, which brings it down to about 500 GiB.</p><p>All of this sounded manageable on a single disk / volume. Second decision down:
stream the results to a local disk is good enough (no need for distributed
storage).</p><h3 id=memory>Memory<a hidden class=anchor aria-hidden=true href=#memory>#</a></h3><p>The first part of the process (creating the edges table) can be done in
streaming, so I had no memory usage concerns. Aggregating over the table can be
memory intensive, but could be done off-core too (paying the penalty of spilling
to disk). Page rank is parallelizable (as it runs in a distributed way), so it
can also be turned into a sharded off-core computation.</p><p>This opened a sub-problem: how to compute aggregations over the dataset while
spilling to disk?</p><h3 id=disk-speed>Disk speed<a hidden class=anchor aria-hidden=true href=#disk-speed>#</a></h3><p>Running Page Rank over multiple iterations would mean reading and writing the
whole dataset several times, and 100 GiB of data written 10 times is about 1
TiB. <a href=https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-storage-compare-volume-types.html>EBS
supports</a>
up to 1 GiB/s throughput, and even the default 125 MiB/s would make this
possible <a href="https://numbat.dev/?q=%281+TiB+%2F+125+MiB%29+s+-%3E+h%E2%8F%8E">in
about</a> 2h.</p><p>So, I wasn&rsquo;t concerned about memory, nor disk.</p><h3 id=cpu>CPU<a hidden class=anchor aria-hidden=true href=#cpu>#</a></h3><p>This was the least concerning to me, as the problem didn&rsquo;t seem to need any
complex computation: decompress files, parse JSONL, turn URLs into domain names
and count.</p><p>Well, here I was wrong, as I underestimated the effort required by decompressing
large gzip files (i.e. <a href=https://en.wikipedia.org/wiki/Deflate>FLATE</a>), which
turned out to be the actual bottleneck.</p><p>I found this out early while running a smaller scale experiment locally (with a
handful of WAT files), so I had to adjust things slightly before the actual full
run. See <a href=/posts/domain-resurrect/#mapper>Mapper implementation</a> below for more details.</p><h2 id=implementation>Implementation<a hidden class=anchor aria-hidden=true href=#implementation>#</a></h2><p>The informal feasibility check gave me the feeling that the bottleneck would be
streaming the 16 TiB from S3, as the rest was orders of magnitude smaller.
Optimizing for that would mean to stream process the data, one thread per core
and store it on disk denormalized (to keep memory usage low). After that,
compute PageRank one iteration at a time, while check pointing intermediate
steps on disk.</p><p><img loading=lazy src=/posts/domain-resurrect/high-level.svg></p><p>Based on that, I split the problem into two independent parts:</p><ul><li>A mapper, responsible for processing the Common Crawl <a href=https://commoncrawl.org/get-started#WAT-Format>WAT
dataset</a> and writing a set of
parquet files with counters for links going from one domain to another (the
edges).</li><li>A reducer: which aggregates the edges (into nodes) and iteratively computes
PageRank.</li></ul><h3 id=mapper>Mapper<a hidden class=anchor aria-hidden=true href=#mapper>#</a></h3><p>The first implementation of the mapper was a <a href=https://github.com/mbrt/domain-resurrect/blob/f91b54065ef5e9a156d95cd1e2ed781cb58b4fd7/docs/slow_map.py>Python
script</a>
which used a <code>ProcessPoolExecutor</code> to process WAT files in parallel. Very simple
and readable implementation, but while testing locally, to my surprise I
discovered that it was terribly slow [see
<a href=https://github.com/mbrt/domain-resurrect/raw/f91b54065ef5e9a156d95cd1e2ed781cb58b4fd7/docs/profile/slow-map-profile.svg>profile</a>].
The code would take about a minute to process a single WAT object, which would
translate to roughly 16000 CPU hours, given the 100k total files in the dataset.</p><p><a href=/posts/domain-resurrect/map-python-flame.svg><img loading=lazy src=/posts/domain-resurrect/map-python-flame.svg></a></p><p>The culprit was mostly domain name
<a href=https://github.com/mbrt/domain-resurrect/blob/f91b54065ef5e9a156d95cd1e2ed781cb58b4fd7/docs/slow_map.py#L100-L121>computation</a>
(eTLD+1), and after trying to replace it in many different ways (e.g. adding
heuristics, or using <a href=https://github.com/jophy/fasttld><code>fasttld</code></a> and seeing a
large drop in extracted domains) I decided to give up and rewrite the thing in
Go.</p><p>I thought this would be a good match for the language: strong standard library,
especially for the web and networking domains, and fast concurrency.
Unfortunately, Go doesn&rsquo;t have an equivalent to the awesome
<a href=https://filesystem-spec.readthedocs.io/en/latest/><code>fsspec</code></a> library, so I had
to <a href=https://github.com/mbrt/domain-resurrect/blob/536322ecebd38c4aacb602a60b16265b47cee535/reader.go#L34-L56>roll my
own</a>,
to abstract away how to read from a certain URL (local path, http or s3 URL).
Another thing I wasn&rsquo;t happy with was the support for good estimation of speed
and completion time, so I <a href=https://github.com/mbrt/domain-resurrect/blob/536322ecebd38c4aacb602a60b16265b47cee535/mapper.go#L535>rolled my
own</a>
as well by using an exponentially weighted moving average
(<a href=https://en.wikipedia.org/wiki/Moving_average#Exponential_moving_average>EWMA</a>)
and some prettier
<a href=https://github.com/mbrt/domain-resurrect/blob/536322ecebd38c4aacb602a60b16265b47cee535/mapper.go#L101>formatting</a>.</p><p>Although that was much faster (down from 60s to 9s per file), there was still
low-hanging fruit. Looking at the <a href=https://github.com/mbrt/domain-resurrect/raw/refs/heads/main/docs/profile/map-go-json-flame.svg>CPU
profile</a>,
I noticed a large amount of time spent in JSON parsing, and I suddenly
remembered how slow that is in Go (it&rsquo;s based on reflection and severely
under-optimized). So I swapped it for
<a href=https://github.com/bytedance/sonic><code>bytedance/sonic</code></a>, which is a drop-in
replacement, and saw another 30% speed up. Down to 6s per file, I was projecting
the map phase to finish in 160 CPU hours, which would be about 5h on a 32 cores
machine.</p><p><a href=/posts/domain-resurrect/map-go-sonic-flame.svg><img loading=lazy src=/posts/domain-resurrect/map-go-sonic-flame.svg></a></p><p>I was ready to start working on the reducer.</p><h3 id=reducer>Reducer<a hidden class=anchor aria-hidden=true href=#reducer>#</a></h3><p>The reducer has a very simple job: take the denormalized input from the previous
phase and aggregate it into unique rows of <code>source_domain, target_domain, count</code>
on a set of parquet files. This is the type of thing that
<a href=https://pola.rs/>Polars</a> should eat for breakfast
[<a href=https://github.com/mbrt/domain-resurrect/blob/20797e0c519464c8db26bbe344fc6f88cba6a8f1/reducer.py#L57>ref</a>]:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=display:flex><span>map_df <span style=color:#ff79c6>=</span> pl<span style=color:#ff79c6>.</span>scan_parquet(tmp <span style=color:#ff79c6>/</span> <span style=color:#f1fa8c>&#34;part_*.parquet&#34;</span>)
</span></span><span style=display:flex><span>edges <span style=color:#ff79c6>=</span> (
</span></span><span style=display:flex><span>    map_df<span style=color:#ff79c6>.</span>group_by(<span style=color:#f1fa8c>&#34;src_dom&#34;</span>, <span style=color:#f1fa8c>&#34;tgt_dom&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>.</span>agg(pl<span style=color:#ff79c6>.</span>col(<span style=color:#f1fa8c>&#34;count&#34;</span>)<span style=color:#ff79c6>.</span>sum()<span style=color:#ff79c6>.</span>alias(<span style=color:#f1fa8c>&#34;count&#34;</span>))
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>edges<span style=color:#ff79c6>.</span>sink_parquet(out <span style=color:#ff79c6>/</span> <span style=color:#f1fa8c>&#34;edges.parquet&#34;</span>, compression<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;snappy&#34;</span>)
</span></span></code></pre></div><p>But it turned out that out-of-core operations (i.e. swapping to disk) were not
yet supported in the new streaming engine
(<a href=https://github.com/pola-rs/polars/issues/20947>pola-rs/polars#20947</a>). On a
250 GiB dataset (compressed), this was not going to fly, and indeed, on my first
run, memory usage exploded and the whole thing crashed.</p><p>After several experiments, I was able to simulate out-of-core aggregations by:</p><ul><li>Processing one <em>map</em> partition at a time</li><li>Splitting each one into a set of smaller shards, partitioned by <code>src_dom</code>.
This made it so that all rows with the same source domain are <em>in the same
partition</em>.</li><li>Aggregate shards with the same partition key together, one by one.</li></ul><p><img loading=lazy src=/posts/domain-resurrect/shuffle-reduce.svg></p><p>This allowed splitting up the larger computation into sequentially independent
steps, where I control the size of each. A map reduce could process this in
parallel faster, but I just decided to process it sequentially, so it could fit
on a single machine.</p><h3 id=page-rank>Page Rank<a hidden class=anchor aria-hidden=true href=#page-rank>#</a></h3><p>The second phase of the reducer computes Page Rank. We&rsquo;ll see in a minute how
this turned out to be insufficient, but it&rsquo;s good to understand the basic
version before complicating it.</p><p>The <a href=https://www.cis.upenn.edu/~mkearns/teaching/NetworkedLife/pagerank.pdf>original
paper</a>
from Page and Brinn describes the intuition behind it quite well:</p><blockquote><p>A page has a high rank if the sum of the ranks of its backlinks is high. This
covers both the case when a page has many backlinks and when a page has a few
highly ranked backlinks.</p></blockquote><p>This reduces the problem of establishing the importance of pages to that of
analyzing links, rather than contents, of those pages. The paper gives one more
intuition on how to compute it, based on the &ldquo;random surfer&rdquo; model. The surfer
picks a random page to start with and keeps clicking on random links until it
&ldquo;gets bored&rdquo;, where it starts over from (i.e. &ldquo;teleports to&rdquo;) another random
page.</p><p>More technically, the Page Rank of page <code>P</code> is the steady state probability that
the random surfer is at page <code>P</code>. The more a page is ranked high, the higher the
probability that a &ldquo;random surfer&rdquo; lands on that page. The formulation is very
short:</p><p><img loading=lazy src=/posts/domain-resurrect/page-rank.svg></p><p>Where:</p><ul><li>α is the damping factor (usually set at 0.85), which determines the
probability of picking a random page over following outlinks.</li><li><code>B(u)</code> is the set of pages linking to page <code>u</code> directly.</li><li><code>N</code> is the total number of pages.</li></ul><p>The formula is iterative and proven to converge.</p><p>Going back to the intuitive explanation, the rank of a page is the combination
of two factors: the &ldquo;mass&rdquo; of people navigating to it because they follow links
in pages that point there, and the ones that just picked a random page. The
combination is modulated by the damping factor α.<sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup></p><p>I introduced a small variation on that, given that I didn&rsquo;t need the rank of
<em>every page</em>, but the aggregate rank of their domains. In this setup, domains
with large amounts of outlinks end up distributing rank equally to domains
linked only once and linked thousands of times. To account for this skew, I
divide contributions of a node by the total number of its outlinks. In this way,
each single link counts as a &ldquo;vote&rdquo; for the target domains, and the more links
point there, the more &ldquo;votes&rdquo; it gets.</p><p>The <a href=https://github.com/mbrt/domain-resurrect/blob/dcbff2f17f6512e1b514a4e866b605a329612716/reducer_small.py#L64>basic
implementation</a>
of that in Polars is straightforward:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=display:flex><span><span style=color:#6272a4># Contributions</span>
</span></span><span style=display:flex><span>contribs <span style=color:#ff79c6>=</span> (
</span></span><span style=display:flex><span>    edges
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>.</span>join(scores, left_on<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;src_dom&#34;</span>, right_on<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;node&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>.</span>with_columns(
</span></span><span style=display:flex><span>        (
</span></span><span style=display:flex><span>            pl<span style=color:#ff79c6>.</span>col(<span style=color:#f1fa8c>&#34;score&#34;</span>) <span style=color:#ff79c6>*</span> pl<span style=color:#ff79c6>.</span>col(<span style=color:#f1fa8c>&#34;count&#34;</span>) <span style=color:#ff79c6>/</span>
</span></span><span style=display:flex><span>            pl<span style=color:#ff79c6>.</span>col(<span style=color:#f1fa8c>&#34;out_weight&#34;</span>)
</span></span><span style=display:flex><span>        )<span style=color:#ff79c6>.</span>alias(<span style=color:#f1fa8c>&#34;contrib&#34;</span>)
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>.</span>group_by(<span style=color:#f1fa8c>&#34;tgt_dom&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>.</span>agg(pl<span style=color:#ff79c6>.</span>sum(<span style=color:#f1fa8c>&#34;contrib&#34;</span>)<span style=color:#ff79c6>.</span>alias(<span style=color:#f1fa8c>&#34;contrib&#34;</span>))
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>.</span>rename({<span style=color:#f1fa8c>&#34;tgt_dom&#34;</span>: <span style=color:#f1fa8c>&#34;node&#34;</span>})
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>.</span>select([<span style=color:#f1fa8c>&#34;node&#34;</span>, <span style=color:#f1fa8c>&#34;contrib&#34;</span>])
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span><span style=color:#6272a4># New scores</span>
</span></span><span style=display:flex><span>new_scores <span style=color:#ff79c6>=</span> (
</span></span><span style=display:flex><span>    scores
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>.</span>join(contribs, on<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;node&#34;</span>, how<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;left&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>.</span>with_columns(
</span></span><span style=display:flex><span>        (damping <span style=color:#ff79c6>*</span> pl<span style=color:#ff79c6>.</span>col(<span style=color:#f1fa8c>&#34;contrib&#34;</span>)<span style=color:#ff79c6>.</span>fill_null(<span style=color:#bd93f9>0.0</span>) <span style=color:#ff79c6>+</span> (<span style=color:#bd93f9>1</span> <span style=color:#ff79c6>-</span> damping) <span style=color:#ff79c6>/</span> N)<span style=color:#ff79c6>.</span>alias(<span style=color:#f1fa8c>&#34;score&#34;</span>)
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>.</span>cast({<span style=color:#f1fa8c>&#34;out_weight&#34;</span>: pl<span style=color:#ff79c6>.</span>Int64})
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>.</span>select([<span style=color:#f1fa8c>&#34;node&#34;</span>, <span style=color:#f1fa8c>&#34;out_weight&#34;</span>, <span style=color:#f1fa8c>&#34;score&#34;</span>])
</span></span><span style=display:flex><span>)
</span></span></code></pre></div><h3 id=off-core>Off-core<a hidden class=anchor aria-hidden=true href=#off-core>#</a></h3><p>As with the <a href=/posts/domain-resurrect/#reducer>edges reduce phase</a>, this simple approach doesn&rsquo;t work
right out-of-the-box in Polars, given the lack of off-core computation. Again, I
had to simulate it by shuffling contributions to the right shards and compute
each one separately:</p><p><img loading=lazy src=/posts/domain-resurrect/sharded-page-rank.svg></p><p>The actual code is
<a href=https://github.com/mbrt/domain-resurrect/blob/dcbff2f17f6512e1b514a4e866b605a329612716/reducer_page_rank.py#L296>here</a>.
I was ready to give this a try, so I let this run overnight, and got some
surprises in the morning.</p><h3 id=page-rank-take-ii-variants>Page Rank take II (variants)<a hidden class=anchor aria-hidden=true href=#page-rank-take-ii-variants>#</a></h3><p>I knew the Internet was full of crap. I just didn&rsquo;t know the extent of it. It&rsquo;s
literally, <em>full - of - crap</em>. After the first few obviously good results at the
top of the ranking (<code>google.com</code>, <code>facebook.com</code>, etc.) a lot of dubious or
outright bad domains came up. This is the predictable result of running a 25+
years old algorithm on today&rsquo;s Internet. I thought I could ignore the problem,
because I just needed to look at <em>top results</em>, but spam can rank high too!</p><p>See, for example, <a href=https://github.com/mbrt/domain-resurrect/blob/dcbff2f17f6512e1b514a4e866b605a329612716/docs/page_rank_analysis.ipynb>this
analysis</a>,
where I try to understand why <code>versiti.co</code> and <code>balsic-fastive.io</code> rank so high
in my run. It comes down to the fact of having two highly ranked domains
(<code>wikipedia.org</code> and <code>zendesk.com</code>) linking to <code>focusvision.com</code>, which was then
considered very highly ranked. This site was only partially crawled, so it had
just a few links in it, and one turned out to be towards <code>versiti.co</code>.</p><p><img loading=lazy src=/posts/domain-resurrect/versiti-spam.svg></p><p>I had to descend a deeper rabbit hole on how to combat spam, but I didn&rsquo;t need
to remove all of it. I just needed to push it down enough for me to find some
decent expired domain higher up. This was a much smaller problem to solve than
the general case Google has to deal with.</p><p>To start, I needed to find a list of spam and trusted domains, and this is one
area where being 25 years late went to my advantage, because there&rsquo;s <a href=https://github.com/mbrt/domain-resurrect/blob/536322ecebd38c4aacb602a60b16265b47cee535/hack/run-full.sh#L12-L17>plenty of
choice</a>.</p><p>As a first set of attempts, I tried to implement some PageRank variations
straight from the literature:</p><ul><li>TrustRank [<a href=https://www.vldb.org/conf/2004/RS15P3.PDF>Gyongyi, Z., et. al,
2004.</a>], biases the teleport vector
towards a curated set of trusted pages.</li><li>Anti-Trust Rank [<a href=http://i.stanford.edu/~kvijay/krishnan-raj-airweb06.pdf>Krishnan, V. and Raj, R.,
2006</a>], starts from
the opposite direction and demotes pages by looking at whether they point to a
curated set of spam pages.</li><li>MaxRank [<a href=https://arxiv.org/pdf/1203.1457>Fercoq, O., 2012</a>], modifies the
graph by removing links going to spam, while minimizing a cost function that
pays some cost on both landing on spam, and on removing links.</li></ul><p>I tried the first two on the full dataset and found them to perform quite badly.
After checking some results empirically, I discovered two major problems:</p><ul><li>A lot of &ldquo;trusted&rdquo; domains point to spam (as well as good targets). A favorite
trick from spammers and SEO sites is to &ldquo;land&rdquo; a link from a reputable place
to their site. There are many ways to do this, but an easy one is to add a
link pointing to a spam site in the comment section of a reputable site. Or
edit a half forgotten Wikipedia article.</li><li>Spam sites might point to both spam and non-spam. Spammers do this to muddy
the waters, so it&rsquo;s hard to propagate distrust from a small set of spam
domains. Both spam and non-spam looks similar in that regard: they both point
to some good and bad pages.</li></ul><p>A problem specific to Anti-Trust rank is that it doesn&rsquo;t really take into
consideration the proportion by which a domain is pointing to spam. The extreme
example was Wikipedia, classified as the #3 most spammy domain on the Internet,
just because in its 11M outlinks there&rsquo;s some amount of spam. But this is to be
expected in such a massive crowdsourced site.</p><p>Although the first two algorithms are quite intuitive (the changes are simple
and the papers are well written), I cannot say the same for the third (MaxRank).
For one thing, it requires dynamic programming and a globally sorted &ldquo;cost&rdquo;
vector, both of which will impact runtime. The second, and most important part,
is that I cannot really explain intuitively why this should work at all.
Modeling the problem as a cost that balances between keeping and removing links
towards spam seemed quite arbitrary to me. But perhaps it&rsquo;s me not understanding
what the paper is trying to say. It&rsquo;s certainly densely fitted with proofs for
properties that don&rsquo;t really move the needle of my understanding.</p><p>Long story short, I decided to implement my own version, by combining the
literature above and fixing a few issues I saw in the experimental data.</p><h3 id=page-rank-take-iii-spamicity-trust-rank>Page Rank take III (Spamicity Trust Rank)<a hidden class=anchor aria-hidden=true href=#page-rank-take-iii-spamicity-trust-rank>#</a></h3><p>I don&rsquo;t know what to call it, so allow me to throw some arbitrary words
together: Spamicity Trust Rank. It may be nonsense, but it combines the concepts
I used:</p><ul><li>A Page Rank variant that still uses hyperlinks to infer &ldquo;importance and
influence&rdquo; of domains.</li><li>Trust Rank, because it uses a trusted set of domains as seeds.</li><li>Spamicity, because it also assigns spam scores, based on a modified
Anti-TrustRank pass.</li></ul><p>The implementation is done in two steps:</p><ul><li><a href=https://github.com/mbrt/domain-resurrect/blob/dcbff2f17f6512e1b514a4e866b605a329612716/reducer.py#L456><code>spamicity</code></a>,
and</li><li><a href=https://github.com/mbrt/domain-resurrect/blob/dcbff2f17f6512e1b514a4e866b605a329612716/reducer.py#L683><code>trust_rank</code></a>.</li></ul><p>The goal of the first pass is to discover as much spam as possible, by starting
from a dataset of known spam domains. Anti-Trust Rank was designed with this
goal in mind, but massively suffers from the problem presented above of
punishing popular domains, regardless of them being spam or not (just because
some links towards spam creeped in).</p><p>To counter this problem, I started from the algorithm as described by the
original paper:</p><blockquote><p>Inverse page-rank is computed by reversing the in-links and out-links in the
web graph. In other words, it merely involves running pagerank on the
transpose of the web graph matrix.</p></blockquote><p>But add a twist: scale the spam contribution by the factor in which the source
domain points to the target domain. The intuition here is that if a domain
points to spam, it should be classified as spam, but the score should be reduced
by the proportion of how much it actually does point to spam. I also include a
damping factor, to progressively attenuate the score over distance (if <code>a</code> links
to <code>b</code> that links to <code>c</code>, we expect the score of <code>c</code> to influence <code>b</code> more than
it would influence <code>a</code>).</p><p>The more precise formulation is as follows:</p><p><img loading=lazy src=/posts/domain-resurrect/spamicity.svg></p><p>And the
<a href=https://github.com/mbrt/domain-resurrect/blob/eb84decafa2ed2dc468a260af132b48246dc871f/reducer.py#L289-L352>code</a>
(using Polars):</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=display:flex><span>contribs <span style=color:#ff79c6>=</span> (
</span></span><span style=display:flex><span>    edges
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>.</span>join(nodes, left_on<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;src_dom&#34;</span>, right_on<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;node&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>.</span>with_columns(
</span></span><span style=display:flex><span>        (
</span></span><span style=display:flex><span>            pl<span style=color:#ff79c6>.</span>col(<span style=color:#f1fa8c>&#34;count&#34;</span>) <span style=color:#ff79c6>*</span> pl<span style=color:#ff79c6>.</span>col(<span style=color:#f1fa8c>&#34;score&#34;</span>)
</span></span><span style=display:flex><span>        )<span style=color:#ff79c6>.</span>alias(<span style=color:#f1fa8c>&#34;contrib&#34;</span>)
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>.</span>group_by(<span style=color:#f1fa8c>&#34;tgt_dom&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>.</span>agg(pl<span style=color:#ff79c6>.</span>sum(<span style=color:#f1fa8c>&#34;contrib&#34;</span>)<span style=color:#ff79c6>.</span>alias(<span style=color:#f1fa8c>&#34;contrib&#34;</span>))
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>.</span>select([<span style=color:#f1fa8c>&#34;tgt_dom&#34;</span>, <span style=color:#f1fa8c>&#34;contrib&#34;</span>])
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>contribs <span style=color:#ff79c6>=</span> (
</span></span><span style=display:flex><span>    contribs<span style=color:#ff79c6>.</span>group_by(<span style=color:#f1fa8c>&#34;tgt_dom&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>.</span>agg(pl<span style=color:#ff79c6>.</span>col(<span style=color:#f1fa8c>&#34;contrib&#34;</span>)<span style=color:#ff79c6>.</span>sum()<span style=color:#ff79c6>.</span>alias(<span style=color:#f1fa8c>&#34;contrib&#34;</span>))
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>updated <span style=color:#ff79c6>=</span> (
</span></span><span style=display:flex><span>    nodes
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>.</span>join(contribs, left_on<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;node&#34;</span>, right_on<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;tgt_dom&#34;</span>, how<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;left&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>.</span>with_columns(
</span></span><span style=display:flex><span>        (
</span></span><span style=display:flex><span>            (
</span></span><span style=display:flex><span>                damping <span style=color:#ff79c6>*</span> pl<span style=color:#ff79c6>.</span>col(<span style=color:#f1fa8c>&#34;contrib&#34;</span>)<span style=color:#ff79c6>.</span>fill_null(strategy<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;zero&#34;</span>) <span style=color:#ff79c6>/</span>
</span></span><span style=display:flex><span>                pl<span style=color:#ff79c6>.</span>col(<span style=color:#f1fa8c>&#34;out_weight&#34;</span>)
</span></span><span style=display:flex><span>            )<span style=color:#ff79c6>.</span>fill_nan(<span style=color:#bd93f9>0</span>)
</span></span><span style=display:flex><span>            <span style=color:#ff79c6>+</span> (<span style=color:#bd93f9>1</span> <span style=color:#ff79c6>-</span> damping) <span style=color:#ff79c6>*</span> pl<span style=color:#ff79c6>.</span>col(<span style=color:#f1fa8c>&#34;bias&#34;</span>)
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>.</span>alias(<span style=color:#f1fa8c>&#34;score&#34;</span>)
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>.</span>drop(<span style=color:#f1fa8c>&#34;contrib&#34;</span>)
</span></span><span style=display:flex><span>)
</span></span></code></pre></div><p>To explain with an example: suppose that <code>src1.com</code> points to <code>tgt.com</code> and
<code>spam.com</code>, with weights 3 and 1 (i.e. its number of out-links).</p><p><img loading=lazy src=/posts/domain-resurrect/spamicity-example.svg></p><p>To calculate the spamicity score of <code>src1.com</code>, we add the scores of its target
domains (0 and 0.1), scaled down by the damping factor (0.85), and do a weighted
average based on the number of outlinks:</p><ul><li><code>tgt.com</code> contributes with a score of 0</li><li><code>spam.com</code> has a score of 0.1, dampened by 0.85 (constant factor), scaled by
¼, as there&rsquo;s only one out-link from the source, over 4 total.</li></ul><p>Given those spam scores, we run a biased PageRank that takes spamicity into
consideration. The first intuition is that it&rsquo;s more likely that when a user
goes to a random page, they land on a popular domain, so we can use domain
popularity datasets as a basis. The second is about spam domains: you can trick
a user to click to a link that points to spam, however when they land there,
it&rsquo;s unlikely that they will stay and follow any further links (therefore they
will jump to somewhere else).</p><p>These intuitions are modeled by biasing the teleport vector with domain
popularity, and by scaling down contributions of spam domains – as classified by
the first step – so that their impact on other domains will be limited. In other
words, the popularity of a spam domain doesn&rsquo;t translate into popularity of its
outgoing links
[<a href=https://github.com/mbrt/domain-resurrect/blob/5c185598aab95bcb6a685b9484d71f040674b3aa/reducer.py#L661>code</a>]:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=display:flex><span><span style=color:#6272a4># Compute contributions and penalize based on spamicity</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># up to a certain threshold</span>
</span></span><span style=display:flex><span>contribs <span style=color:#ff79c6>=</span> (
</span></span><span style=display:flex><span>    edges
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>.</span>join(nodes, left_on<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;src_dom&#34;</span>, right_on<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;node&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>.</span>with_columns(
</span></span><span style=display:flex><span>        (
</span></span><span style=display:flex><span>            pl<span style=color:#ff79c6>.</span>col(<span style=color:#f1fa8c>&#34;score&#34;</span>) <span style=color:#ff79c6>*</span> pl<span style=color:#ff79c6>.</span>col(<span style=color:#f1fa8c>&#34;count&#34;</span>) <span style=color:#ff79c6>/</span> pl<span style=color:#ff79c6>.</span>col(<span style=color:#f1fa8c>&#34;out_weight&#34;</span>)
</span></span><span style=display:flex><span>            <span style=color:#ff79c6>*</span> (<span style=color:#bd93f9>1.0</span> <span style=color:#ff79c6>-</span> (max_spam_penalty <span style=color:#ff79c6>*</span> pl<span style=color:#ff79c6>.</span>col(<span style=color:#f1fa8c>&#34;spamicity&#34;</span>)))
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>.</span>alias(<span style=color:#f1fa8c>&#34;contrib&#34;</span>),
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>.</span>group_by(<span style=color:#f1fa8c>&#34;tgt_dom&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>.</span>agg(pl<span style=color:#ff79c6>.</span>sum(<span style=color:#f1fa8c>&#34;contrib&#34;</span>)<span style=color:#ff79c6>.</span>alias(<span style=color:#f1fa8c>&#34;contrib&#34;</span>))
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>.</span>select([<span style=color:#f1fa8c>&#34;tgt_dom&#34;</span>, <span style=color:#f1fa8c>&#34;contrib&#34;</span>])
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span><span style=color:#6272a4># Sum all contributions going to the same target</span>
</span></span><span style=display:flex><span>contribs <span style=color:#ff79c6>=</span> (
</span></span><span style=display:flex><span>    contribs<span style=color:#ff79c6>.</span>group_by(<span style=color:#f1fa8c>&#34;tgt_dom&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>.</span>agg(pl<span style=color:#ff79c6>.</span>col(<span style=color:#f1fa8c>&#34;contrib&#34;</span>)<span style=color:#ff79c6>.</span>sum()<span style=color:#ff79c6>.</span>alias(<span style=color:#f1fa8c>&#34;contrib&#34;</span>))
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span><span style=color:#6272a4># Compute the updated scores by summing up the two factors:</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># Contributions from backlinks and initial popularity estimates.</span>
</span></span><span style=display:flex><span>updated <span style=color:#ff79c6>=</span> (
</span></span><span style=display:flex><span>    nodes
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>.</span>join(contribs, left_on<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;node&#34;</span>, right_on<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;tgt_dom&#34;</span>, how<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;left&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>.</span>with_columns(
</span></span><span style=display:flex><span>        (
</span></span><span style=display:flex><span>            damping <span style=color:#ff79c6>*</span> pl<span style=color:#ff79c6>.</span>col(<span style=color:#f1fa8c>&#34;contrib&#34;</span>)<span style=color:#ff79c6>.</span>fill_null(strategy<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;zero&#34;</span>)
</span></span><span style=display:flex><span>            <span style=color:#ff79c6>+</span> (<span style=color:#bd93f9>1</span> <span style=color:#ff79c6>-</span> damping) <span style=color:#ff79c6>*</span> pl<span style=color:#ff79c6>.</span>col(<span style=color:#f1fa8c>&#34;bias&#34;</span>)
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>.</span>alias(<span style=color:#f1fa8c>&#34;score&#34;</span>)
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>.</span>drop(<span style=color:#f1fa8c>&#34;contrib&#34;</span>)
</span></span><span style=display:flex><span>)
</span></span></code></pre></div><p>See how contributions are penalized (up to a certain max) for domains considered
spammy, and the teleport contribution is biased by the &ldquo;bias&rdquo; column, which is
initialized with larger weight for popular domains and uniformly lower weights
for the rest.</p><h3 id=dangling-domains>Dangling domains<a hidden class=anchor aria-hidden=true href=#dangling-domains>#</a></h3><p>After getting a ranked list of domains, it was time to find the dangling ones.
For that, I looked for those that simply had no outlinks. If they have many
domains pointing to them, but none going out, there&rsquo;s a chance the domain wasn&rsquo;t
crawled because it expired.</p><p>Well, not quite. There are many reasons why domains have no outlinks in
<em>CommonCrawl</em>: <code>robots.txt</code> didn&rsquo;t allow the crawl, or the domain is not serving
http content at all, or the site was down, but the domain is <em>still registered
to someone</em>.</p><p>A good way to filter through this is by combining the filter for no outlinks
with a DNS resolution. If that returns <code>NXDOMAIN</code>, then there&rsquo;s a much better
chance that the domain actually expired. The
<a href=https://github.com/mbrt/domain-resurrect/blob/536322ecebd38c4aacb602a60b16265b47cee535/find_available.py#L64-L73>code</a>
is quite simple:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=display:flex><span><span style=color:#ff79c6>async</span> <span style=color:#ff79c6>def</span> <span style=color:#50fa7b>resolve</span>(domain: <span style=color:#8be9fd;font-style:italic>str</span>, sem: asyncio<span style=color:#ff79c6>.</span>Semaphore) <span style=color:#ff79c6>-&gt;</span> <span style=color:#8be9fd;font-style:italic>tuple</span>[<span style=color:#8be9fd;font-style:italic>str</span>, <span style=color:#8be9fd;font-style:italic>bool</span>]:
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>async</span> <span style=color:#ff79c6>with</span> sem:
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>try</span>:
</span></span><span style=display:flex><span>            _ <span style=color:#ff79c6>=</span> <span style=color:#ff79c6>await</span> resolver<span style=color:#ff79c6>.</span>resolve(domain)
</span></span><span style=display:flex><span>            <span style=color:#ff79c6>return</span> domain, <span style=color:#ff79c6>True</span>
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>except</span> dns<span style=color:#ff79c6>.</span>NXDOMAIN:
</span></span><span style=display:flex><span>            <span style=color:#ff79c6>return</span> domain, <span style=color:#ff79c6>False</span>
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>except</span> Exception:
</span></span><span style=display:flex><span>            <span style=color:#6272a4># Treat other errors as &#34;something went wrong, but domain may exist&#34;</span>
</span></span><span style=display:flex><span>            <span style=color:#ff79c6>return</span> domain, <span style=color:#ff79c6>True</span>
</span></span></code></pre></div><p>This resulted into a list of candidates (I kept about a hundred) that I checked
manually. And this is the part where I would have loved to say: &ldquo;and this is how
I made my first million&rdquo;. However, the results mostly split into three
categories:</p><ul><li>Domains that looked interesting, but someone was already sitting on them,
likely waiting for a buyer.</li><li>Domains that were boosted by much more clever spam sources (e.g. fake blogs
and fake companies, likely machine / AI generated).</li><li>Shady domains that I definitely didn&rsquo;t want to touch.</li></ul><p>Perhaps the first category might become a future good buy (if the owner just
loses hope of selling and lets it expire), but I assume that several
<a href=https://en.wikipedia.org/wiki/Domain_drop_catching>drop-catchers</a> are already
watching and will be faster than regular people in securing them right then.</p><h2 id=results>Results?<a hidden class=anchor aria-hidden=true href=#results>#</a></h2><p>When I said that I was 25 years late to the problem, I meant: a lot of people
thought about this very same problem for a very long time. So much that there is
plenty of terminology, research papers and valuable companies built around
understanding and exploiting it: this is the weird universe of
<a href=https://en.wikipedia.org/wiki/Search_engine_optimization>SEO</a>,
<a href=https://en.wikipedia.org/wiki/Spamdexing>Spamdexing</a>, <a href=https://en.wikipedia.org/wiki/Google_bombing>Google
bombing</a>, <a href=https://en.wikipedia.org/wiki/Domain_name_speculation>Domain name
speculation</a> and <a href=https://en.wikipedia.org/wiki/Domain_drop_catching>Domain
drop catching</a>.</p><p>So, what&rsquo;s the probability that there indeed <strong>is</strong> some hidden gem, forgotten by
everybody, waiting just for me to take? Probably close to zero. But I wanted to
try anyway.</p><p>At this point, I could continue optimizing and improving the ranking algorithms,
but decided not to. The first category of domains I found above (domains that
are interesting but not available), made me think that whatever is mildly
interesting, was already found by somebody.</p><p>OK, so what was the point of this post? Perhaps it looks like a flex, or a
pointless exercise of someone with too much time on their hands. Maybe there&rsquo;s
some of that, but I had a lot of fun working on this project and wanted to share
it. I might be in a small niche, but I also love reading posts like this one, so
I wanted to contribute to the genre with something of my own.</p><p>Some takeaways from this experiment:</p><ul><li>I feel like out-of-core aggregations are generally easy to fake with streaming
capabilities, even when they are not built-in in the data framework one is
using. See <a href=/posts/domain-resurrect/#generalized-out-of-core-aggregations>Generalized out-of-core
aggregations</a>.</li><li>One more empirical proof of the value of back-of-the-envelope design, combined
with small scale experiments (see <a href=/posts/domain-resurrect/#single-machine>Single machine?</a>).</li><li>The Internet is unbelievably full of crap. Even with very low expectations, I
was still surprised by how much this is true.</li></ul><p>If you feel like I got things horribly wrong, and there are ways to make easy
money with this data, be my guest! You can run things from scratch, starting
from <a href=https://github.com/mbrt/domain-resurrect>my code</a> (messy but includes all
experiments as well) or save a bunch of time and money by starting with the
final data:</p><ul><li><a href=https://huggingface.co/datasets/mbrt/domain-resurrect>Scored domains</a> dataset</li><li><a href=https://huggingface.co/datasets/mbrt/domain-resurrect-edges>Links</a> dataset</li></ul><p>Both of which are based on the <a href=https://data.commoncrawl.org/crawl-data/CC-MAIN-2025-38/index.html>September Common Crawl
dataset</a>,
and are <em>massively smaller</em> (down to 35 GiB from 76 TiB).</p><p>I didn&rsquo;t get rich in the process, but I learned a thing or two about optimizing
data ingestion, <a href=https://pola.rs/>polars</a> and PageRank variants. The journey
was more important than the destination, etc. etc.</p><hr><h2 id=appendix>Appendix<a hidden class=anchor aria-hidden=true href=#appendix>#</a></h2><p>Or, an unordered list of things I didn&rsquo;t know where to put but still thought interesting.</p><h3 id=generalized-out-of-core-aggregations>Generalized out-of-core aggregations<a hidden class=anchor aria-hidden=true href=#generalized-out-of-core-aggregations>#</a></h3><p>I think we can extrapolate a way to generalize to out-of-core aggregations from
the particular example of the <a href=/posts/domain-resurrect/#reducer>reducer implementation</a>:</p><ol><li>Use the columns in the group-by clause as sharding key</li><li>Calculate the number of necessary shards to fit a single one in memory, based
on statistics on the number of unique elements in each column.</li><li>Stream the source table to a set of separate tables, partitioned by the
sharding key.</li><li>Process shards one by one, instead of in parallel.</li></ol><p>Step number 2 is perhaps the hardest to get correctly, as memory usage depends
on many factors. It will be hard to guess a good number of shards without prior
knowledge. A good approximation might be to abound on the number of shards and
then compute some of them in parallel in step number 4. If memory usage grows
too large, kill some of these shards and postpone their computation to
subsequent sequential steps.</p><h3 id=rewrite-the-mapper-in-rust>Rewrite the mapper in Rust?<a hidden class=anchor aria-hidden=true href=#rewrite-the-mapper-in-rust>#</a></h3><p>Before someone starts screaming: &ldquo;bro, you should have started with Rust, it&rsquo;s
faster, safer, and blah blah&rdquo;, I want to show <a href=https://github.com/mbrt/domain-resurrect/blob/dcbff2f17f6512e1b514a4e866b605a329612716/mapper-rs/benches/flate.rs>my little
benchmark</a>,
where I simply ran the most compute intensive part of the mapper (i.e. gzip
decompress) by using the fastest available library in Rust, and couldn&rsquo;t really
see a speed improvement. It turns out, the Go version is very well optimized.</p><p>In summary, I don&rsquo;t expect a Rust implementation to be significantly (i.e.
orders of magnitude) faster than the one I came up with. Not to mention that I&rsquo;m
more familiar with Go and Python, and they ran fast enough for my purposes.</p><h3 id=infrastructure>Infrastructure<a hidden class=anchor aria-hidden=true href=#infrastructure>#</a></h3><p>Based on the <a href=/posts/domain-resurrect/#single-machine>back-of-the-envelope estimates</a> above and a small
scale run locally from my laptop (done by downloading WAT files by hand and
processing them locally), I <a href=/posts/domain-resurrect/#mapper>concluded</a> that each file would take an
average of 6s of CPU time to process. Given that the dataset has 100k files,
this roughly means 160 CPU hours.</p><p>Since compute was the bottleneck (and not network), I just needed a
compute-optimized instance with enough disk to get the job done. This pointed me
towards a <a href=https://instances.vantage.sh/aws/ec2/c6id.8xlarge>c6id.8xlarge</a> VM,
with 32 cores and 1.9 TiB of SSD, which costs about $1.6/h. The whole process
should have taken less than 24 hours, and therefore less than $38 in total.</p><h3 id=infrastructure-as-code>Infrastructure as code<a hidden class=anchor aria-hidden=true href=#infrastructure-as-code>#</a></h3><p><a href=https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/Welcome.html>CloudFormation</a>
fell out of favor among platform and cloud engineers nowadays, in favor of
Terraform or fancier solutions like <a href=https://www.pulumi.com/>Pulumi</a> and
<a href=https://www.crossplane.io/>Crossplane</a>. I still find the ease of setting up
and use of CloudFormation simply unbeatable, as you just need the AWS CLI and an
AWS account. No need to deploy extra things, or manage the state somewhere.
Given that I <em>had</em> to run within AWS, it felt like a very natural choice.</p><p>And hey, GitHub Copilot and other AI assistants really like it, so they speed up
the creation a lot by making good suggestions.</p><p>See
<a href=https://github.com/mbrt/domain-resurrect/blob/dcbff2f17f6512e1b514a4e866b605a329612716/template.yamlQ>config</a>
and <a href=https://github.com/mbrt/domain-resurrect/blob/dcbff2f17f6512e1b514a4e866b605a329612716/hack/deploy-infra.sh>deploy
script</a>.</p><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>We&rsquo;ll see <a href=/posts/domain-resurrect/#dangling-domains>later</a> that this is a good starting point,
but not sufficient.&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p>Interestingly, this is one order of magnitude less than the reported
number of domains from
<a href=https://www.dnib.com/articles/the-domain-name-industry-brief-q2-2025>DNIB</a>.&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3><p>I couldn&rsquo;t find an explanation for why the damping factor is usually set
to 0.85. I ran a small scale ablation on notable pages with different values
and concluded empirically that this indeed looks reasonable. When it&rsquo;s set
to a lower value (e.g. 0.5), ranks seem to vanish very quickly, and for
higher values, they propagate unreasonably further away from the source. For
example, a single link from <code>google.com</code> would positively impact pages 3-4
degrees separated away, thereby boosting a lot of spam.&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><footer class=post-footer><ul class=post-tags><li><a href=https://blog.mbrt.dev/tags/cloud/>Cloud</a></li><li><a href=https://blog.mbrt.dev/tags/data-science/>Data-Science</a></li></ul><nav class=paginav><a class=next href=https://blog.mbrt.dev/posts/transactional-object-storage/><span class=title>Next »</span><br><span>Glassdb: transactional object storage</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Resurrecting valuable expired domains on x" href="https://x.com/intent/tweet/?text=Resurrecting%20valuable%20expired%20domains&amp;url=https%3a%2f%2fblog.mbrt.dev%2fposts%2fdomain-resurrect%2f&amp;hashtags=cloud%2cdata-science"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Resurrecting valuable expired domains on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fblog.mbrt.dev%2fposts%2fdomain-resurrect%2f&amp;title=Resurrecting%20valuable%20expired%20domains&amp;summary=Resurrecting%20valuable%20expired%20domains&amp;source=https%3a%2f%2fblog.mbrt.dev%2fposts%2fdomain-resurrect%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Resurrecting valuable expired domains on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fblog.mbrt.dev%2fposts%2fdomain-resurrect%2f&title=Resurrecting%20valuable%20expired%20domains"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Resurrecting valuable expired domains on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fblog.mbrt.dev%2fposts%2fdomain-resurrect%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Resurrecting valuable expired domains on whatsapp" href="https://api.whatsapp.com/send?text=Resurrecting%20valuable%20expired%20domains%20-%20https%3a%2f%2fblog.mbrt.dev%2fposts%2fdomain-resurrect%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Resurrecting valuable expired domains on telegram" href="https://telegram.me/share/url?text=Resurrecting%20valuable%20expired%20domains&amp;url=https%3a%2f%2fblog.mbrt.dev%2fposts%2fdomain-resurrect%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Resurrecting valuable expired domains on ycombinator" href="https://news.ycombinator.com/submitlink?t=Resurrecting%20valuable%20expired%20domains&u=https%3a%2f%2fblog.mbrt.dev%2fposts%2fdomain-resurrect%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://blog.mbrt.dev/>mbrt blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script></body></html>