<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Glassdb: transactional object storage | mbrt blog</title>
<meta name=keywords content="db,design,cloud"><meta name=description content="I was frustrated by the gap between stateless and stateful applications in the
cloud. While I could easily spin up a stateless application as a &ldquo;serverless&rdquo;
function in any major cloud provider and pretty much forget about it, persisting
data between requests was a game of pick two among three: cheap, strongly
consistent, portable.
Could I solve portability and lack of transactions myself with a single
client-side solution? I thought it would be possible through object storage
(e.g. AWS S3), which is strongly consistent,
ubiquitous and cheap."><meta name=author content="Michele Bertasi"><link rel=canonical href=https://blog.mbrt.dev/posts/transactional-object-storage/><link crossorigin=anonymous href=/assets/css/stylesheet.d6fcd20a4fb86efa4dfac8ec95da60244cc8871042183da1ef28e3a762ad79c8.css integrity="sha256-1vzSCk+4bvpN+sjsldpgJEzIhxBCGD2h7yjjp2Ktecg=" rel="preload stylesheet" as=style><link rel=icon href=https://blog.mbrt.dev/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://blog.mbrt.dev/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://blog.mbrt.dev/favicon-32x32.png><link rel=apple-touch-icon href=https://blog.mbrt.dev/apple-touch-icon.png><link rel=mask-icon href=https://blog.mbrt.dev/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://blog.mbrt.dev/posts/transactional-object-storage/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="https://blog.mbrt.dev/posts/transactional-object-storage/"><meta property="og:site_name" content="mbrt blog"><meta property="og:title" content="Glassdb: transactional object storage"><meta property="og:description" content="I was frustrated by the gap between stateless and stateful applications in the cloud. While I could easily spin up a stateless application as a “serverless” function in any major cloud provider and pretty much forget about it, persisting data between requests was a game of pick two among three: cheap, strongly consistent, portable.
Could I solve portability and lack of transactions myself with a single client-side solution? I thought it would be possible through object storage (e.g. AWS S3), which is strongly consistent, ubiquitous and cheap."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-11-16T00:00:00+00:00"><meta property="article:modified_time" content="2024-11-16T00:00:00+00:00"><meta property="article:tag" content="Db"><meta property="article:tag" content="Design"><meta property="article:tag" content="Cloud"><meta name=twitter:card content="summary"><meta name=twitter:title content="Glassdb: transactional object storage"><meta name=twitter:description content="I was frustrated by the gap between stateless and stateful applications in the
cloud. While I could easily spin up a stateless application as a &ldquo;serverless&rdquo;
function in any major cloud provider and pretty much forget about it, persisting
data between requests was a game of pick two among three: cheap, strongly
consistent, portable.
Could I solve portability and lack of transactions myself with a single
client-side solution? I thought it would be possible through object storage
(e.g. AWS S3), which is strongly consistent,
ubiquitous and cheap."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://blog.mbrt.dev/posts/"},{"@type":"ListItem","position":2,"name":"Glassdb: transactional object storage","item":"https://blog.mbrt.dev/posts/transactional-object-storage/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Glassdb: transactional object storage","name":"Glassdb: transactional object storage","description":"I was frustrated by the gap between stateless and stateful applications in the cloud. While I could easily spin up a stateless application as a \u0026ldquo;serverless\u0026rdquo; function in any major cloud provider and pretty much forget about it, persisting data between requests was a game of pick two among three: cheap, strongly consistent, portable.\nCould I solve portability and lack of transactions myself with a single client-side solution? I thought it would be possible through object storage (e.g. AWS S3), which is strongly consistent, ubiquitous and cheap.\n","keywords":["db","design","cloud"],"articleBody":"I was frustrated by the gap between stateless and stateful applications in the cloud. While I could easily spin up a stateless application as a “serverless” function in any major cloud provider and pretty much forget about it, persisting data between requests was a game of pick two among three: cheap, strongly consistent, portable.\nCould I solve portability and lack of transactions myself with a single client-side solution? I thought it would be possible through object storage (e.g. AWS S3), which is strongly consistent, ubiquitous and cheap.\nYes, I could rely on a “serverless database” and just build a thin conversion layer between DynamoDB (AWS), Cosmos DB (Azure) and Firestore (GCP). So, does it make sense to instead reinvent a database almost from scratch? Not in a practical sense, but think about it this way: I learned a lot about database design, so it was still worth it, and I think you will find my journey useful too.\nAnd before I forget, here’s the source code.\nThe beginning My thinking started a few years back, when I realized I could easily deploy serverless solutions – like AWS Lambda or Google Cloud Run – to deploy a function that handles HTTP requests, without worrying about host OS updates, Kubernetes clusters, VMs, and all the underlying infrastructure. And since the serverless billing is a pay-per-request (with a decent free tier), I also paid little to nothing for most of the services I was running for myself.\nThe deal is easy: you provide the code and the cloud provider runs it. Pay no money, do no maintenance, and get a functional web service in return. Sounds like a good deal?\nIt was until I realized that this worked only if I didn’t need to persist data across requests. All database options had downsides: Google SQL for Postgres costs money whether I use it or not, upgrades were manual and required downtime. Spanner was strongly consistent, but very expensive (shared instances didn’t exist) and not portable. Datastore (now surpassed by Firestore) was cheap, but had restrictions on transactions (reads before writes) and wasn’t portable outside Google Cloud either. AWS provided similar services (RDS portable but expensive and manual, DynamoDB hands-off and cheap, but not portable to other clouds and eventually consistent).\nThe idea I decided to give the wheel reinvention another round of my energies. With the excuse of learning something, I thought: I’m already using object storage (Google Cloud Storage and AWS S3) to store larger files. Can I abuse their strong consistency properties and turn them into transactional databases? And yes, GCS is slow, but it’s extremely scalable. And how slow is too slow? I ran some tests on reading and writing 100KiB objects, and the 90th percentile looked like this:\nRead: 63.1ms Write: 105ms Metadata: 41.3ms Which is far from what you get from a local SQLite (10x slower), but fine in many scenarios and also works with multiple writers. In fact, as many writers as I want.\nThe other question was: when does this become too expensive compared to a traditional managed database solution like Google Cloud SQL? The cheapest managed Postgres instance comes at $8.57 per month. To pay the same, we would need an average of 0.66 writes / s or 8.3 reads / s (at $0.004 per 10k read ops and $0.05 per 10k write ops). If we instead use the smallest dedicated instance as a reference, estimates would grow to 2.7 writes / s or 34 reads / s.\nFor infrequent or bursty traffic, this makes sense so far.\nThe simplest database After I got the cost component confirmed, the next step was to come up with the simplest working solution – disregarding all performance considerations. “First make it work, then make it pretty” was my working mode. I started with GCS, because Google Cloud was the one I knew better at the time.\nThe first design choice I made was the architecture. The simplest possible is the one with no server component. I wanted to see how far I could go with a client-side library and just object storage.\nIn the same line of thinking, the simplest possible database is SQLite. You can make it work by running each transaction locally in the client and returning OK only if both the transaction and the upload to object storage succeed. To avoid race conditions between two different clients, you can either use locks, or implement a form of OCC (Optimistic Concurrency Control), by checking that no concurrent updates happen between a transaction’s reads and commit.\nI decided to go with the second option, because it was much easier to implement.\nFor that, I needed two APIs: read and conditional write – both available in GCS. These primitives, although primitive, allowed me to build a database. I could serialize a SQLite database into an object and for every transaction do the following:\nDownload the DB locally from GCS, keep note of its metadata version, and open it with SQLite. Execute the transaction locally. If it succeeds, take this new database and try to write it conditionally to GCS (using the previously read metadata version). If it fails, repeat from #1, executing the transaction again. On success, return success to the user. Step number two is atomic and succeeds only if no other client modified the object in the meantime, which is what makes the algorithm work.\nWell, this is slow. The whole database must be downloaded and uploaded in full at every transaction – a bit of a problem if it’s gigabytes in size. Clients are also guaranteed to conflict at every transaction, adding retries on top. Even if they access different tables, they still need to update the same global object, resulting in no parallelism.\nEven worse, GCS officially allows one update per second per object, resulting in a maximum of one transaction per second, killing the performance of the database. In practice, because of the way rate limiting is implemented (tokens are checked and reset every minute), GCS allows short bursts of requests to go faster, with penalties coming afterward. This results in very inconsistent performance, with latencies going from tens of milliseconds to tens of seconds, the moment clients hit the same object consistently.\nDistributed transactions With the single object approach only going so far, I needed to look into distributing the load to multiple objects. The main problem I needed to solve became implementing ACID transactions across multiple objects. Even though updates to the same object are guaranteed to be strongly consistent – the contents of a write are available to all readers, no matter where they are, immediately after the write completes – this only applies to single objects. Operations involving multiple objects at once, such as copying or renaming multiple objects, are not atomic. Furthermore, the version number of an object has nothing to do with that of another, so there’s no global ordering for writes either.\nLuckily, there’s a whole literature of distributed transactions algorithms designed to solve this problem space with different tradeoffs. And the design space at this stage was:\nObject storage Provides strong consistency for operations on single objects. Latency can be high (50ms - 150ms). Does not allow frequent updates to the same objects (\u003c 1s). No backend component, only a client library. No direct communication channel between clients. This suggested that I look into algorithms that minimize the number of write operations, while keeping “correctness”. We’ll get back to this in a moment, but as an aside, what do I mean by correctness?\nAside: Isolation and Consistency When thinking about correctness, my mind went to university lectures on databases about ACID properties, especially isolation. But I found it difficult to relate that (stale) knowledge with material in the literature and on the web. The main issue, I believe, is due to overlapping terminology and concepts between research on traditional relational databases and shared memory systems. These concepts, previously treated separately, became simultaneously useful to distributed systems, which borrowed terminology from both sides.\nThe crux of the issue is that about ACID properties (Atomicity, Consistency, Isolation, Durability), everybody agrees on atomicity (a transaction is applied in full or not at all) and durability (after a commit, the effects of a transaction are persistent even after system crashes). But not so much on the other two. Isolation levels in the SQL standard are not only imprecise, but also stuck in the nineties. This is a problem known since 1995, which opened the door to narrow and misleading interpretations by database vendors. Oracle famously claimed their implementation of snapshot isolation to be “serializable” – even though it wasn’t – through some legally anal interpretation.1 Furthermore, isolation levels apply to single host databases, but fail to adequately take replication into consideration, which introduce new anomalies and so require more expressiveness. In addition, consistency means different things between the SQL standard and in distributed systems’ literature. In ACID, C has traditionally meant “respects constraints specified by the user” such as foreign keys and unique constraints. Whereas, the C in the CAP and PACELC theorems refers to the ability of a read to receive the value of the most recent write.\nOn top of all this, literature and blogs use “isolation levels” in a very liberal way, sometimes meaning to say “consistency level”, or a combination of an isolation and a consistency guarantee. If I wanted to design a database, I first needed to clarify things for myself.\nAnd in my quest for clarity, I bumped into Jepsen’s diagram over and over. Jepsen takes Bailis’ approach of unifying everything in a single hierarchy of ever greater “consistency”. This is a nice simplification, but it forgets to mention that the unified hierarchy is actually two hierarchies fused together: one of consistency and one of isolation guarantees. Isolation provides rules for how the system behaves when transactions run concurrently, and consistency specifies when writes are available to reads after they complete. Thanks to Daniel Abadi I realized that a database can provide guarantees for both hierarchies at the same and this is what we sometimes call “isolation level” and sometimes “consistency model”, making the terminology very confusing.\nMy revised diagram makes this distinction more prominent. Arrows show guarantees of inclusion (e.g. Linearizable is Sequential plus additional guarantees). Dotted lines show how “isolation levels” are defined as a combination of isolation and consistency guarantees. Note how Strict Serializable is the strongest level, as it implies all the guarantees.\nIn essence, a guarantee is defined in terms of which anomalies it prevents. The more it prevents, the stronger it is. And since anomalies are unexpected results that happen when transactions run concurrently, the more anomalies are allowed in an isolation level, the more care a developer has to put to prevent them from popping up as bugs in the application logic. For example, dirty reads, allowed in the Read Uncommitted isolation guarantee, happen when a transaction is allowed to read a value modified by another transaction that isn’t committed yet. Because of this, transactions that abort may still influence other transactions.\nIsolation anomalies break the illusion of transactions running independently of each other, and consistency anomalies break the illusion of a system being in a single machine (i.e. replication lags become visible).\nNotable levels Isolation levels seemed quite abstract to me, until I looked at notable databases and their position in the hierarchy. Two things were clear: the first is that database vendors go to great lengths in explaining why their database is “consistent enough” and how “rare those anomalies are in practice”. The second thing is that there’s great variability in levels:\nFor example, Postgres provides Read Committed by default, which means that the same process can update a row in a transaction and then the next transaction could still read the outdated value. Yes, these are two separate transactions, but this is surprising to most developers, especially when the database is hosted as a single replica. But since Postgres is also Sequentially Consistent, at least that the transactions’ ordering is the same for everyone, even though it doesn’t necessarily follow the real time in which transactions were applied.\nKing of the pack in terms of isolation is Spanner – which is Strict Serializable – meaning that transactions run as if they were serially executed, and they follow the real time order globally. This is the most correct model. FaunaDB is also Strict Serializable through a different mechanism.2\nUnsurprisingly, Cassandra, being a true NoSQL database, provides a weak, eventually consistent model, because the focus is on availability at all costs. An interesting middle ground is CockroachDB. Very close to Spanner, but occasionally allows for transactions to read stale data, in the name of more optimistic concurrency controls.3\nWhich isolation level to target? Weaker isolation levels allow for more optimizations in the database implementation, but also allow for more classes of anomalies. This means they are usually faster, but require application developers to implement more logic themselves to avoid bugs. The balance between performance and correctness is a pendulum that swung from the NoSQL years – faster is better – to the NewSQL era, after the Spanner paper in 2012, where correct is better. Practice and papers like ACID rain (from Bailis) showed that designing correct applications with weak consistency databases is a lot harder: they require more effort and time to develop and understand, so why bother, if stronger consistency is fast enough? After all, developer time is more expensive than machine time.\nFor me the answer was clear: the ideal isolation level is the highest one before things become too slow for the application at hand.\nNo isolation Given this background, my thinking went to this basic algorithm for a transaction:\n- Read what you need - Write what you need - On commit: do nothing - On abort: undo the writes one by one This clearly doesn’t provide Atomicity, nor basic isolation such as Read Committed, because transactions commit one key at a time and concurrent transactions can read new values before they are committed. This however provides Linearizable consistency, thanks to the strong consistency offered by S3 and GCS, which state that any read initiated after a successful write, return the contents of that latest write. Without effort, we have basically achieved strong consistency with no isolation.\nThe challenge then was to introduce stronger isolation guarantees while keeping acceptable performance.\nInterface Before going further down the rabbit hole of picking a transaction algorithm, I needed to take a step back and decide on which interface I wanted to provide. The nice part of my simplest database is that it provides a recognizable SQL interface through SQLite out of the box. But, by deciding to split the data among multiple objects in a bucket, I was making that a lot harder to achieve.\nI looked around for inspiration and found that many modern databases are transactional key value stores at their core. Some, like FoundationDB, stop there. They are in a way just key value stores. Others – like CockroachDB – take a step further and build a Postgres compatible SQL layer on top of it.\nOne more source of inspiration was Ben Johnson’s BoltDB, a very elegant transactional embedded key value store written in Go. You can see its influence in my GlassDB example:\nfunc compareAndSwap(ctx context.Context, key, oldval, newval []byte) (bool, error) { coll := db.Collection([]byte(\"my-collection\")) swapped := false err := db.Tx(ctx, func(tx *glassdb.Tx) error { b, err := tx.Read(coll, key) if err != nil { return err } if bytes.Equal(b, oldval) { swapped = true return tx.Write(coll, key, newval) } return nil }) return swapped, err } This snippet implements a simple compare-and-swap for a key in a collection – which is just a set of keys with the same prefix. The db.Tx method wraps the operations into a single transaction, so that it can be retried by GlassDB transparently.\nI will not spend too much time discussing how to use it. If you are curious, please check out the repository and the relative reference documentation. The only thing I wanted to show is that you can put arbitrary code inside a transaction, and reads and writes to the database will be, well, transactional.\nChoosing the algorithm As I mentioned earlier, my goal was to choose an algorithm that guarantees the strongest possible isolation levels, while retaining a reasonable level of performance. The first thing that came to mind then was to use Strict Two Phase Locking (S2PL), similarly to Spanner. This alone would guarantee serializable isolation, and combining that with the linearizable consistency of GCS, I would get the top spot in isolation levels: strict serializability.\nThe algorithm itself is quite intuitive:\nA key must be locked as read-only or read-write, before you can read it. It must be locked as read-write, before you can modify it. You can release all the locks only after committing. My approach was to first make it work, and then make it fast.\nMake it work To make the algorithm above work, I needed locking and atomic commits.\nLocks GCS objects are made of two independent parts: content and metadata. Metadata contains user-controlled key values and a version that automatically changes at every write (or metadata change). Given this, it’s easy to update an object’s metadata to mark it as “locked”. The algorithm is basically a compare-and-swap:\nRead the object’s metadata with its version number. Is the “locked” tag set to “true”? If yes, wait a bit and repeat #1. If no, conditionally write the metadata, setting “locked” to “true”. The condition constrains the write to succeed only if the metadata version is the same as the one we read in step #1. If the write failed, go to #1. If it succeeded, we got the lock. Unlocks work similarly, but it does a compare-and-swap for “locked=false” instead.\nThis algorithm is quite simplistic: it doesn’t distinguish between read and write locks, and crashes in the application may result in forever-locks. We’ll talk about crashes in a bit, but before we need to add read locks:\nUse a lock-type and a lockers tag, containing respectively the type of lock (read, write, none) and the IDs of the transactions holding the lock. Locking means updating the lock-type and adding the ID of the transaction holding the lock to the lockers tag (we can modify multiple tags atomically). Unlocking is the opposite: remove the ID from lockers and if that was the last locker, set the lock-type as unlocked. Some additional intuitive ground rules as well:\nMultiple transactions can hold read locks to the same key at the same time. Read locks can be upgraded to write, but only if there was a single transaction holding it. Write locks are exclusive. Commits Commits must appear instantaneously: a transaction is either committed or not, and nothing in between. How do we do it with transactions updating multiple keys? How do we realize those updates at the same time? It’s clear that we can’t update one key at a time while releasing the locks, because if the process crashes in the middle, we would get an inconsistent state.\nOne way would be to use transaction logs. Transactions stash the updates they are going to do into a log, before actually doing them. This allows replays after crashes. When the log is committed, the transaction is considered committed and a process picking up the work after a crash will read it and continue committing the writes.\nThe way I implemented this was to include all the values written by a transaction into its transaction log upon commit. A transaction keeps the keys untouched until commit, and only after the log is written successfully it unlocks the keys while updating their values.\nDeadlocks and crashes Distributed locks require timeouts. If a process opens a transaction, locks a few keys and then crashes, those keys would become inaccessible. Even if the crash was temporary, we still would depend on all clients to come back online reasonably quickly after crashes, which might be unreasonable.\nThe way to solve this is by introducing TTLs on locks. While locking a key, a transaction also keeps track of the time it’s taking to commit. After a timeout, it will create (or update) its transaction log by stating that the transaction is still pending, along with a refresh timeout. Competing transactions that want to check whether a lock is still valid, just need to read the corresponding transaction log. If the timestamp is too far back in the past, or no log appears during the entire timeout period, the lock is considered “expired”.\nHowever, the naive algorithm of observing a lock and just taking it over after a period of inactivity, is incorrect. The reason is that if a transaction blocks only temporarily, after the verify stage and before writing the transaction log, there’s no way for this transaction to check whether it’s still allowed to commit or it has lost some of its locks.\nThis cannot be solved by introducing more checks before committing, because the delay could happen exactly during the write for the transaction log, which would be impossible to block.\nA way to solve this, is to use the transaction log itself as a synchronization point. A transaction (2) that wants to take over an expired lock held by (1), can try to conditionally write to the transaction log for (1), stating that it is aborted. If it succeeds (i.e. the log didn’t change in the meantime), then (1) is officially aborted and all its locks are invalid. If it failed (because e.g. the transaction successfully committed, or refreshed the locks), then (2) will have to wait longer.\nBy making sure changes to transaction logs happen atomically and are never overwritten incorrectly, we make sure that even in the presence of concurrent changes, we always communicate the intention and the state of each transaction.\nIn the example above, if (2) succeeds in writing the transaction log for (1), the write to the same log that (1) tries to do for its commit will fail, because its condition would be set for a past version. At that point, (1) will just abort.\nCollections, create and delete keys The way we handle creating and deleting keys in transactions is by using pseudo-keys. There’s one such key per collection, representing the “list of keys” present in that collection. To ensure consistency when iterating through keys or adding and removing keys, transactions just lock the corresponding pseudo-key for each affected collection:\nCreating a key: lock the collection (i.e. its pseudo-key) in write and create the key (locked in write). Deleting a key: lock the collection and the key in write. Iterating through a collection: lock the collection in read. This mechanism allows keeping the collection unlocked while keys are just modified, but not added or deleted (which should be most of the time).\nMake it fast Up until this point, I had a working algorithm with strong isolation and consistency, but was it fast? Given that object storage itself is not particularly fast, to maximize throughput I wanted to avoid transactions operating on different keys to interfere with each other. This excluded any kind of global lock, or single transaction log, because they would generate contention from all the transactions. I also wanted to maximize parallelism, given that object storage is designed for horizontal scalability in mind.\nIn the quest of making things faster, we still should not forget the larger context and constraints:\nNo server component No communication between different clients Transaction logs The easiest way to avoid transactions to interfere with each other is by having a different log object for each transaction, rather than a single object for all. The client assigns a random ID to each transaction and that is part of the path of the log object. Each client is then able to read a transaction log without the need to communicate with anyone, because given an ID, its path is deterministic.\nBut how do we map keys to transactions affecting them? The answer is in objects metadata. When locking, we keep track of the ID of the locker(s). A reader is then able to check whether the object is still locked or not, by which transaction and what is the state of that transaction, just by reading its metadata. We have three cases to consider while testing a key:\nThe lock-type is set to none: The key is unlocked. The lock-type is set to read or write, and: There’s one transaction listed in locked-by that is pending (by looking at the corresponding logs): the key is locked. Otherwise, the key is unlocked. Optimistic locking Fast is always relative and situational: faster compared to what? And under which conditions? Which use cases are we willing to sacrifice in order to improve on the most important ones? The key decision for me was to optimize for low data contention workloads. If writes to the same key at the same time are rare, I could optimize for transactions that don’t conflict with each other, to the expense of transactions that do. This idea points in the direction of optimistic locking, where transactions use data resources without acquiring locks. Transactions verify that no other transaction affected the same data only on commit. It also optimizes the case where object storage shines, which is not latency, but throughput.\nThe algorithm works like this:\nModify: Read keys and tentatively write (i.e. stage locally) changes. Verify: Check that other transactions did not modify read values after the transaction started. Commit: If no conflicts are detected: make the changes effective (commit), otherwise retry the transaction. Optimistic locking avoids locking, but implicitly requires transactions to validate and commit one at a time. This is because verify and commit must be executed atomically. If not, we would risk losing writes:\nTo avoid this problem and maintain the isolation level achieved by the base algorithm, I used a modified version of optimistic locking. One that, well – uses locks. The difference from the base algorithm is that transactions run without locks until the verify phase, where they acquire all the locks in parallel, verify for conflicts safely and only then, commit a transaction log (or retry):\nRead all the key values it needs without locks. Write values in a staging area (in memory). When committing: Lock all the keys that were read or written in parallel. Check that the read keys haven’t changed in the meantime. If yes, retry the transaction from #1. Write a transaction log that includes which keys were modified along with their new values. (async) Write the new values back to the modified keys. This avoids global synchronization points. If two transactions touch different keys, they will not compete for the same objects, so they are able to proceed in parallel and locks are kept for the shortest amount of time.\nNote that in #3b, when retrying, locks are not released. This has the side effect that transactions will be retried at most once, as the second run will happen while still holding the locks, turning it effectively into pessimistic locking. No other transaction will be able to commit those keys in the meantime.\nRead-only transactions We are already optimizing for high throughput, low contention workloads. In this scenario, read-only transactions are also going to run undisturbed most of the time. Can we take advantage of that and run the standard optimistic concurrency control algorithm without locking? Remember that verify steps of transactions must run sequentially, to prevent losing updates. In the general case of read-write transactions, both the current transaction can interfere with others, and others can interfere with the current. For read-only transactions, however, we don’t have to worry about the first problem. They don’t write anything, so they can’t interfere with other transactions’ verify and commit steps. Therefore, the only thing to do is to make sure no transaction modified keys while our read-only transaction was reading them, i.e. between our first and last read.\nWe can achieve this in a relatively simple way:\nRead phase: read all required keys while keeping track of their current version. Verify phase: check the current version of all the keys read again: If the versions didn’t change, and the keys were not locked in read-write, we can return successfully. Otherwise, there was a concurrent write and we retry. Note that in (2) we not only check that the object version stayed the same, but that the metadata version did. The actual check is slightly more complicated, as we can avoid retrying in a few cases even when metadata did change, for example when the key was locked in read and unlocked, or when locked, but the transaction aborted.\nTo avoid retrying an unlucky transaction forever, after the first retry we fall back to the regular algorithm, which will acquire (read) locks to keep writers away. This approach guarantees progress eventually.\nIf the transaction succeeded the first time around, since we didn’t really make any change in values, nor in the state of locks, we can avoid writing a transaction log as well. The happy path requires no writes and just one value plus one metadata read per key.\nThis approach is different from the one taken by Spanner, which implements Multiversion concurrency control. MVCC allows reads to go much faster, by reading a consistent snapshot of the database. This is hard to do correctly, because the linearizable consistency of the database depends on picking a version compatible with the real time ordering of all transactions globally. Picking anything older than that would lead to weaker consistency models. Spanner achieves this the hard way, through atomic clocks and bounded time uncertainty.\nGiven that we don’t keep multiple versions of objects and we don’t have access to atomic clocks, nor we have transactions coordinators, I chose to revert to two-phase locking in case of any conflicts. This makes sure we stay in the strict serializability isolation level.\nSingle read-modify-write We can optimize transactions that read and write to a single key by using native object storage compare-and-swap (CAS) operations. The algorithm becomes:\nRead the key contents. During validation: read the key metadata If it’s locked: retry (go to #1) Otherwise, check that the version is the same it was during the read. If not, retry (go to #1) Do a conditional write on the key with the new value, and set the condition to be that content and metadata versions match what we read in #2. In case of conflicts, we can retry the transaction with the base algorithm, so that the transaction is not starved in case of high contention.\nLocal cache The last optimization I wanted to talk about is the local cache. The cache is useful to eliminate reads when a client is the only one updating a certain key. A set of transactions like these would require reading the same key 100 times, even in the case no other transaction touched it:\nfor i := 0; i \u003c 100; i++ { db.Tx(ctx, func(tx *glassdb.Tx) error { b, _ := tx.Read(coll, key) return tx.Write(coll, key, string(b) + “a”) }) } For this reason, I introduced a local LRU cache that keeps in memory the values committed by the most recent transactions. The cache has a limited and configurable size, to make sure it won’t eat all the available memory.\nThe way this works is by checking the cache during every read in a transaction. If there’s a value, the transaction uses that. If, during validation, the value turned out to be outdated, the key is marked as stale and the transaction retried. During the retry, the cache would be invalid, and so the transaction will go and fetch the fresh value from object storage.\nBenchmarks How performant is this thing? Details are on the glassdb repo, but in summary, low conflicts workloads lead to a somewhat linear throughput with the number of transactions.\nThe graph above shows three different types of transactions, where the bold line is the median number of transactions per second and the error band includes the 10th and 90th percentiles. The dataset above was composed of 50k keys and each client is doing 10 transactions in parallel (10% read + write 2 keys, 60% read 2 keys, 30% weak reads4 of a single key).\nGiven that keys are selected randomly by transactions, you can see how higher percentiles are affected by the higher concurrency, because of the higher probability of conflicts.\nInstead, latency – which is not the strong point of the solution – is higher than most databases, but stays relatively flat even with higher concurrency. Exception being with higher levels of contention.\nIs that all? The article is pretty long, and even if it doesn’t cover everything – and I would have really liked to talk about the source code, testing, correctness, profiling and more optimizations – it covers the most important aspects of the design. I hope I was able to transmit some of the key lessons I learned along the way and demystified some of the opaque sounding, but actually not so hard to understand, terminology, used and abused in database design and classification.\nIf you want to explore some of the source code, I recommend starting from the transaction algorithm. There aren’t many comments, but hey it’s written in Go, it should be understandable :)\nOne more thing: did I forget about S3? At the beginning, I praised how easy it would be to have the same solution across cloud providers. Did this come to pass? Not really. Or, at least, I spent so much time making this work on GCS that I never had the time to port it to other cloud providers. It also turns out to be not so easy, because Google Cloud provides a superset of conditional updates with respect to AWS S3.\nWell, no problem. I learned something and solved something. The rest can wait for another release.\nFeedback on Hacker News.\nSee Introduction to Isolation Levels in the Fauna blog for more details on this. ↩︎\nSee also Fauna architectural overview. ↩︎\nSee CockroachDB’s blog post on consistency and the Jepsen report. ↩︎\nA weak read is a read that is allowed to get a stale value, bounded by a user specified limit. For example, a weak read with a 10s staleness bound, is allowed to read the latest value of a key, or any value written in the previous 10s from the cache, but no earlier. ↩︎\n","wordCount":"5754","inLanguage":"en","datePublished":"2024-11-16T00:00:00Z","dateModified":"2024-11-16T00:00:00Z","author":{"@type":"Person","name":"Michele Bertasi"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://blog.mbrt.dev/posts/transactional-object-storage/"},"publisher":{"@type":"Organization","name":"mbrt blog","logo":{"@type":"ImageObject","url":"https://blog.mbrt.dev/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://blog.mbrt.dev/ accesskey=h title="mbrt blog (Alt + H)">mbrt blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://blog.mbrt.dev/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://blog.mbrt.dev/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://blog.mbrt.dev/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://blog.mbrt.dev/>Home</a>&nbsp;»&nbsp;<a href=https://blog.mbrt.dev/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Glassdb: transactional object storage</h1><div class=post-meta><span title='2024-11-16 00:00:00 +0000 UTC'>November 16, 2024</span>&nbsp;·&nbsp;28 min&nbsp;·&nbsp;Michele Bertasi</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#the-beginning aria-label="The beginning">The beginning</a></li><li><a href=#the-idea aria-label="The idea">The idea</a><ul><li><a href=#the-simplest-database aria-label="The simplest database">The simplest database</a></li></ul></li><li><a href=#distributed-transactions aria-label="Distributed transactions">Distributed transactions</a><ul><li><a href=#aside-isolation-and-consistency aria-label="Aside: Isolation and Consistency">Aside: Isolation and Consistency</a></li><li><a href=#notable-levels aria-label="Notable levels">Notable levels</a></li><li><a href=#which-isolation-level-to-target aria-label="Which isolation level to target?">Which isolation level to target?</a></li></ul></li><li><a href=#no-isolation aria-label="No isolation">No isolation</a></li><li><a href=#interface aria-label=Interface>Interface</a></li><li><a href=#choosing-the-algorithm aria-label="Choosing the algorithm">Choosing the algorithm</a></li><li><a href=#make-it-work aria-label="Make it work">Make it work</a><ul><li><a href=#locks aria-label=Locks>Locks</a></li><li><a href=#commits aria-label=Commits>Commits</a></li><li><a href=#deadlocks-and-crashes aria-label="Deadlocks and crashes">Deadlocks and crashes</a></li><li><a href=#collections-create-and-delete-keys aria-label="Collections, create and delete keys">Collections, create and delete keys</a></li></ul></li><li><a href=#make-it-fast aria-label="Make it fast">Make it fast</a><ul><li><a href=#transaction-logs aria-label="Transaction logs">Transaction logs</a></li><li><a href=#optimistic-locking aria-label="Optimistic locking">Optimistic locking</a></li><li><a href=#read-only-transactions aria-label="Read-only transactions">Read-only transactions</a></li><li><a href=#single-read-modify-write aria-label="Single read-modify-write">Single read-modify-write</a></li><li><a href=#local-cache aria-label="Local cache">Local cache</a></li></ul></li><li><a href=#benchmarks aria-label=Benchmarks>Benchmarks</a></li><li><a href=#is-that-all aria-label="Is that all?">Is that all?</a></li></ul></div></details></div><div class=post-content><p>I was frustrated by the gap between stateless and stateful applications in the
cloud. While I could easily spin up a stateless application as a &ldquo;serverless&rdquo;
function in any major cloud provider and pretty much forget about it, persisting
data between requests was a game of pick two among three: cheap, strongly
consistent, portable.</p><p>Could I solve portability and lack of transactions myself with <em>a single
client-side solution?</em> I thought it would be possible through object storage
(e.g. <a href=https://aws.amazon.com/s3/>AWS S3</a>), which is strongly consistent,
ubiquitous and cheap.</p><p>Yes, I could rely on a &ldquo;serverless database&rdquo; and just build a thin conversion
layer between <a href=https://aws.amazon.com/dynamodb/>DynamoDB</a> (AWS), <a href=https://azure.microsoft.com/en-us/products/cosmos-db/>Cosmos
DB</a> (Azure) and
<a href=https://firebase.google.com/docs/firestore/>Firestore</a> (GCP). So, does it make
sense to instead reinvent a database almost from scratch? Not in a practical
sense, but think about it this way: I learned a lot about database design, so it
was still worth it, and I think you will find my journey useful too.</p><p>And before I forget, <a href=https://github.com/mbrt/glassdb>here&rsquo;s the source code</a>.</p><h2 id=the-beginning>The beginning<a hidden class=anchor aria-hidden=true href=#the-beginning>#</a></h2><p>My thinking started a few years back, when I realized I could easily deploy
serverless solutions – like <a href=https://aws.amazon.com/lambda/>AWS Lambda</a> or
<a href=https://cloud.google.com/run>Google Cloud Run</a> – to deploy a <em>function</em> that
handles HTTP requests, without worrying about host OS updates, Kubernetes
clusters, VMs, and all the underlying infrastructure. And since the serverless
billing is a pay-per-request (with a decent free tier), I also paid little to
nothing for most of the services I was running for myself.</p><p>The deal is easy: you provide the code and the cloud provider runs it. Pay no
money, do no maintenance, and get a functional web service in return. Sounds
like a good deal?</p><p>It was until I realized that this worked only if I didn&rsquo;t need to persist data
across requests. All database options had downsides: <a href=https://cloud.google.com/sql/docs/postgres/pricing>Google SQL for
Postgres</a> costs money
whether I use it or not, <a href=https://cloud.google.com/sql/docs/postgres/upgrade-major-db-version-inplace>upgrades were
manual</a>
and required downtime. <a href=https://cloud.google.com/spanner>Spanner</a> was strongly
consistent, but very expensive (shared instances didn&rsquo;t exist) and not portable.
<a href=https://cloud.google.com/datastore/docs/concepts/overview>Datastore</a> (now
surpassed by <a href=https://cloud.google.com/firestore>Firestore</a>) was cheap, but had
restrictions on transactions (reads before writes) and wasn&rsquo;t portable outside
Google Cloud either. AWS provided similar services
(<a href=https://aws.amazon.com/rds/>RDS</a> portable but expensive and manual,
<a href=https://aws.amazon.com/dynamodb/>DynamoDB</a> hands-off and cheap, but not
portable to other clouds and eventually consistent).</p><h2 id=the-idea>The idea<a hidden class=anchor aria-hidden=true href=#the-idea>#</a></h2><p>I decided to give the wheel reinvention another round of my energies. With the
excuse of learning something, I thought: I&rsquo;m already using object storage
(<a href=https://cloud.google.com/storage>Google Cloud Storage</a> and <a href=https://aws.amazon.com/s3/>AWS
S3</a>) to store larger files. Can I abuse their
<a href=https://cloud.google.com/storage/docs/consistency>strong consistency</a>
properties and turn them into transactional databases? And yes, GCS is slow, but
it&rsquo;s extremely scalable. And how slow is too slow? I ran some tests on reading
and writing 100KiB objects, and the 90th percentile looked like this:</p><ul><li>Read: 63.1ms</li><li>Write: 105ms</li><li>Metadata: 41.3ms</li></ul><p>Which is far from what you get from a local SQLite (10x slower), but fine in
many scenarios and also works with multiple writers. In fact, a<em>s many writers
as I want</em>.</p><p>The other question was: when does this become too expensive compared to a
traditional managed database solution like Google Cloud SQL? The cheapest
managed Postgres instance comes at $8.57 per month. To pay the same, <a href="https://numbat.dev/?q=unit+query+%3D+1+piece%E2%8F%8Eunit+qps+%3D+query+%2F+s%E2%8F%8E%288.57%24+%2F+0.004%24+*+10+thousand%29+*+query+%2F+month%E2%8F%8E_+-%3E+qps%E2%8F%8E_+%2F+0.05%24+*+0.004%24%E2%8F%8E">we would
need</a>
an average of 0.66 writes / s or 8.3 reads / s (at $0.004 per 10k read ops and
$0.05 per 10k write ops). If we instead use the smallest dedicated instance as a
reference, estimates would grow to 2.7 writes / s or 34 reads / s.</p><p>For infrequent or bursty traffic, this makes sense so far.</p><h3 id=the-simplest-database>The simplest database<a hidden class=anchor aria-hidden=true href=#the-simplest-database>#</a></h3><p>After I got the cost component confirmed, the next step was to come up with the
<em>simplest</em> working solution – disregarding all performance considerations.
&ldquo;First make it work, then make it pretty&rdquo; was my working mode. I started with
GCS, because Google Cloud was the one I knew better at the time.</p><p>The first design choice I made was the architecture. The simplest possible is
the one with <em>no server component</em>. I wanted to see how far I could go with a
client-side library and just object storage.</p><p><img alt="glassdb lib" loading=lazy src=/posts/transactional-object-storage/glassdb-lib.svg></p><p>In the same line of thinking, the simplest possible database is
<a href=https://www.sqlite.org/>SQLite</a>. You can make it work by running each
transaction locally in the client and returning OK <em>only if</em> both the
transaction <em>and</em> the upload to object storage succeed. To avoid race conditions
between two different clients, you can either use locks, or implement a form of
OCC (Optimistic Concurrency Control), by checking that no concurrent updates
happen between a transaction&rsquo;s reads and commit.</p><p>I decided to go with the second option, because it was <em>much easier to
implement</em>.</p><p>For that, I needed two APIs: read and <a href=https://cloud.google.com/storage/docs/request-preconditions>conditional
write</a> – both
available in GCS. These primitives, although primitive, allowed me to build a
database. I could serialize a SQLite database into an object and for every
transaction do the following:</p><ol><li>Download the DB locally from GCS, keep note of its metadata version, and open
it with SQLite. Execute the transaction locally.</li><li>If it succeeds, take this new database and try to write it conditionally to
GCS (using the previously read metadata version).</li><li>If it fails, repeat from #1, executing the transaction again.</li><li>On success, return success to the user.</li></ol><p>Step number two is atomic and succeeds only if no other client modified the
object in the meantime, which is what makes the algorithm work.</p><p>Well, this is slow. The whole database must be downloaded and uploaded in full
at every transaction – a bit of a problem if it&rsquo;s gigabytes in size. Clients are
also guaranteed to conflict at every transaction, adding retries on top. Even if
they access different tables, they still need to update the same global object,
resulting in no parallelism.</p><p>Even worse, GCS officially allows <a href=https://cloud.google.com/storage/quotas>one update per
second</a> per object, resulting in a
maximum of one transaction per second, killing the performance of the database.
In practice, because of the way rate limiting is implemented (tokens are checked
and reset every minute), GCS allows short bursts of requests to go faster, with
penalties coming afterward. This results in very inconsistent performance, with
latencies going from tens of milliseconds to tens of seconds, the moment clients
hit the same object consistently.</p><h2 id=distributed-transactions>Distributed transactions<a hidden class=anchor aria-hidden=true href=#distributed-transactions>#</a></h2><p>With the single object approach only going so far, I needed to look into
distributing the load to multiple objects. The main problem I needed to solve
became implementing ACID transactions across multiple objects. Even though
updates to the <em>same object</em> are guaranteed to be strongly consistent – the
contents of a write are available to all readers, no matter where they are,
immediately after the write completes – this only applies to <em>single objects</em>.
Operations involving multiple objects at once, such as copying or renaming
multiple objects, <a href=https://cloud.google.com/storage/docs/consistency>are not
atomic</a>. Furthermore, the
version number of an object has nothing to do with that of another, so there&rsquo;s
no global ordering for writes either.</p><p>Luckily, there&rsquo;s a whole literature of distributed transactions algorithms
designed to solve this problem space with different tradeoffs. And the design
space at this stage was:</p><ul><li>Object storage<ul><li>Provides strong consistency for operations on single objects.</li><li>Latency can be high (50ms - 150ms).</li><li>Does not allow frequent updates to the same objects (&lt; 1s).</li></ul></li><li>No backend component, only a client library.</li><li>No direct communication channel between clients.</li></ul><p>This suggested that I look into algorithms that <em>minimize the number of write
operations</em>, while keeping &ldquo;correctness&rdquo;. We&rsquo;ll get back to this in a moment,
but as an aside, what do I mean by <em>correctness</em>?</p><h3 id=aside-isolation-and-consistency>Aside: Isolation and Consistency<a hidden class=anchor aria-hidden=true href=#aside-isolation-and-consistency>#</a></h3><p>When thinking about correctness, my mind went to university lectures on
databases about <a href=https://en.wikipedia.org/wiki/ACID>ACID properties</a>,
especially isolation. But I found it difficult to relate that (stale) knowledge
with material in the literature and on the web. The main issue, I believe, is
due to overlapping terminology and concepts between research on traditional
relational databases and shared memory systems. These concepts, previously
treated separately, became simultaneously useful to distributed systems, which
borrowed terminology from both sides.</p><p>The crux of the issue is that about ACID properties (Atomicity, Consistency,
Isolation, Durability), everybody agrees on <em>atomicity</em> (a transaction is
applied in full or not at all) and <em>durability</em> (after a commit, the effects of
a transaction are persistent even after system crashes). But not so much on the
other two. <em>Isolation levels</em> in the SQL standard are not only imprecise, but
also stuck in the nineties. This is a problem known <a href=https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-95-51.pdf>since
1995</a>,
which opened the door to narrow and misleading interpretations by database
vendors. Oracle famously claimed their implementation of snapshot isolation to
be &ldquo;serializable&rdquo; – even though <a href=https://blog.dbi-services.com/oracle-serializable-is-not-serializable/>it
wasn&rsquo;t</a>
– through some legally anal interpretation.<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup> Furthermore, isolation levels
apply to single host databases, but fail to adequately take <em>replication</em> into
consideration, which <em>introduce new anomalies</em> and so require more
expressiveness. In addition, <em>consistency</em> means <a href=http://sites.computer.org/debull/A16mar/p3.pdf>different
things</a> between the SQL standard
and in distributed systems&rsquo; literature. In ACID, <em>C</em> has traditionally meant
&ldquo;respects constraints specified by the user&rdquo; such as foreign keys and unique
constraints. Whereas, the <em>C</em> in the
<a href=https://en.wikipedia.org/wiki/CAP_theorem>CAP</a> and
<a href=https://en.wikipedia.org/wiki/PACELC_theorem>PACELC</a> theorems refers to the
ability of a <em>read to receive the value of the most recent write</em>.</p><p>On top of all this, literature and blogs use &ldquo;isolation levels&rdquo; in a very
liberal way, sometimes meaning to say &ldquo;consistency level&rdquo;, or a combination of
an isolation and a consistency guarantee. If I wanted to design a database, I
first needed to clarify things for myself.</p><p>And in my quest for clarity, I bumped into <a href=https://jepsen.io/consistency>Jepsen&rsquo;s
diagram</a> over and over. Jepsen takes
<a href=http://www.vldb.org/pvldb/vol7/p181-bailis.pdf>Bailis</a>&rsquo; approach of unifying
everything in a single hierarchy of ever greater &ldquo;consistency&rdquo;. This is a nice
simplification, but it forgets to mention that the unified hierarchy is actually
<em>two hierarchies</em> fused together: one of consistency and one of isolation
guarantees. <strong>Isolation provides rules for how the system behaves when
transactions run concurrently</strong>, and <strong>consistency specifies when writes are
available to reads</strong> after they complete. Thanks to <a href=https://fauna.com/blog/demystifying-database-systems-part-4-isolation-levels-vs-consistency-levels>Daniel
Abadi</a>
I realized that a database can provide guarantees for both hierarchies at the
same and this is what we sometimes call &ldquo;isolation level&rdquo; and sometimes
&ldquo;consistency model&rdquo;, making the terminology very confusing.</p><p><img alt="isolation and consistency levels" loading=lazy src=/posts/transactional-object-storage/isolation-consistency-levels.svg></p><p>My revised diagram makes this distinction more prominent. Arrows show guarantees
of inclusion (e.g.
<a href=https://jepsen.io/consistency/models/linearizable>Linearizable</a> <em>is</em>
<a href=https://jepsen.io/consistency/models/sequential>Sequential</a> <em>plus additional
guarantees</em>). Dotted lines show how &ldquo;isolation levels&rdquo; are defined as a
combination of isolation and consistency guarantees. Note how <a href=https://jepsen.io/consistency/models/strict-serializable>Strict
Serializable</a> is the
strongest level, as it implies all the guarantees.</p><p>In essence, a guarantee is defined in terms of which <em>anomalies it prevents</em>.
The more it prevents, the stronger it is. And since anomalies are unexpected
results that happen when transactions run concurrently, the more anomalies are
allowed in an isolation level, the more care a developer has to put to prevent
them from popping up as bugs in the application logic. For example, <a href=https://en.wikipedia.org/wiki/Isolation_%5C%28database_systems%5C%29#Dirty_reads>dirty
reads</a>,
allowed in the <a href=https://jepsen.io/consistency/models/read-uncommitted>Read
Uncommitted</a> isolation
guarantee, happen when a transaction is allowed to read a value modified by
another transaction that isn&rsquo;t committed yet. Because of this, transactions that
abort may still influence other transactions.</p><p>Isolation anomalies break the illusion of <em>transactions running independently</em>
of each other, and consistency anomalies break the illusion of a system being in
a single machine (i.e. replication lags become visible).</p><h3 id=notable-levels>Notable levels<a hidden class=anchor aria-hidden=true href=#notable-levels>#</a></h3><p>Isolation levels seemed quite abstract to me, until I looked at notable
databases and their position in the hierarchy. Two things were clear: the first
is that database vendors go to great lengths in explaining why their database is
&ldquo;consistent enough&rdquo; and how &ldquo;rare those anomalies are in practice&rdquo;. The second
thing is that there&rsquo;s great variability in levels:</p><p><img alt="isolation and consistency comparison" loading=lazy src=/posts/transactional-object-storage/isolation-consistency-cmp.svg></p><p>For example, Postgres provides <a href=https://jepsen.io/consistency/models/read-committed>Read
Committed</a> by default,
which means that the same process can update a row in a transaction and then the
next transaction could still read the outdated value. Yes, these are two
separate transactions, but this is surprising to most developers, especially
when the database is hosted as a single replica. But since Postgres is also
<a href=https://jepsen.io/consistency/models/sequential>Sequentially Consistent</a>, at
least that the transactions&rsquo; ordering is <em>the same for everyone</em>, even though it
doesn&rsquo;t necessarily follow the real time in which transactions were applied.</p><p>King of the pack in terms of isolation is
<a href=https://cloud.google.com/spanner>Spanner</a> – which is <a href=https://jepsen.io/consistency/models/strict-serializable>Strict
Serializable</a> –
meaning that transactions run <em>as if they were serially executed,</em> and <em>they
follow the real time order globally</em>. This is the most correct model.
<a href=https://fauna.com/>FaunaDB</a> is also Strict Serializable through a <a href=https://fauna.com/blog/consistency-without-clocks-faunadb-transaction-protocol>different
mechanism</a>.<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup></p><p>Unsurprisingly, <a href=https://cassandra.apache.org/>Cassandra</a>, being a true NoSQL
database, provides a weak, eventually consistent model, because the focus is on
availability at all costs. An interesting middle ground is
<a href=https://www.cockroachlabs.com/>CockroachDB</a>. Very close to Spanner, but
occasionally allows for transactions to read stale data, in the name of more
optimistic concurrency controls.<sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup></p><h3 id=which-isolation-level-to-target>Which isolation level to target?<a hidden class=anchor aria-hidden=true href=#which-isolation-level-to-target>#</a></h3><p>Weaker isolation levels allow for more optimizations in the database
implementation, but also allow for more classes of <em>anomalies</em>. This means they
are usually faster, but require application developers to implement more logic
themselves to avoid bugs. The balance between performance and correctness is a
pendulum that swung from the NoSQL years – faster is better – to the NewSQL era,
after the <a href=https://static.googleusercontent.com/media/research.google.com/en//archive/spanner-osdi2012.pdf>Spanner
paper</a>
in 2012, where correct is better. Practice and papers like <a href=http://www.bailis.org/papers/acidrain-sigmod2017.pdf>ACID
rain</a> (from Bailis) showed
that designing correct applications with weak consistency databases <em>is a lot
harder</em>: they require more effort and time to develop <a href=http://www.bailis.org/blog/understanding-weak-isolation-is-a-serious-problem/>and
understand</a>,
so why bother, if stronger consistency is fast enough? After all, developer time
is more expensive than machine time.</p><p>For me the answer was clear: the ideal isolation level is the highest one
<em>before things become too slow</em> for the application at hand.</p><h2 id=no-isolation>No isolation<a hidden class=anchor aria-hidden=true href=#no-isolation>#</a></h2><p>Given this background, my thinking went to this basic algorithm for a
transaction:</p><pre tabindex=0><code>- Read what you need
- Write what you need
- On commit: do nothing
- On abort: undo the writes one by one
</code></pre><p>This clearly doesn&rsquo;t provide <em>Atomicity</em>, nor basic isolation such as <a href=https://jepsen.io/consistency/models/read-uncommitted>Read
Committed</a>, because
transactions commit one key at a time and concurrent transactions can read new
values <em>before they are committed</em>. This however provides
<a href=https://jepsen.io/consistency/models/linearizable>Linearizable</a> consistency,
thanks to the strong consistency offered <a href=https://docs.aws.amazon.com/AmazonS3/latest/userguide/Welcome.html#ConsistencyModel>by
S3</a>
and <a href=https://cloud.google.com/storage/docs/consistency>GCS</a>, which state that
any read initiated after a successful write, return the contents of that latest
write. Without effort, we have basically achieved <em>strong consistency with no
isolation</em>.</p><p><img alt="no isolation" loading=lazy src=/posts/transactional-object-storage/no-isolation.svg></p><p>The challenge then was to introduce stronger isolation guarantees while keeping
acceptable performance.</p><h2 id=interface>Interface<a hidden class=anchor aria-hidden=true href=#interface>#</a></h2><p>Before going further down the rabbit hole of picking a transaction algorithm, I
needed to take a step back and decide on which interface I wanted to provide.
The nice part of my <a href=/posts/transactional-object-storage/#the-simplest-database>simplest database</a> is that it
provides a recognizable SQL interface through SQLite out of the box. But, by
deciding to split the data among multiple objects in a bucket, I was making that
<em>a lot harder</em> to achieve.</p><p>I looked around for inspiration and found that many modern databases are
transactional key value stores at their core. Some, like
<a href=https://www.foundationdb.org/>FoundationDB</a>, stop there. They <em>are in a way</em>
just key value stores. Others – like CockroachDB – take a step further and build
a Postgres compatible <a href=https://www.cockroachlabs.com/docs/v22.1/architecture/sql-layer>SQL
layer</a> <a href=https://www.cockroachlabs.com/docs/v22.1/architecture/overview>on
top</a> of it.</p><p>One more source of inspiration was Ben Johnson&rsquo;s
<a href=http://github.com/boltdb/bolt>BoltDB</a>, a very elegant transactional embedded
key value store written in Go. You can see its influence in my <a href="https://github.com/mbrt/glassdb?tab=readme-ov-file#usage-example">GlassDB
example</a>:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>func</span> <span style=color:#50fa7b>compareAndSwap</span>(ctx context.Context, key, oldval, newval []<span style=color:#8be9fd>byte</span>) (<span style=color:#8be9fd>bool</span>, <span style=color:#8be9fd>error</span>) {
</span></span><span style=display:flex><span>	coll <span style=color:#ff79c6>:=</span> db.<span style=color:#50fa7b>Collection</span>([]<span style=color:#8be9fd;font-style:italic>byte</span>(<span style=color:#f1fa8c>&#34;my-collection&#34;</span>))
</span></span><span style=display:flex><span>	swapped <span style=color:#ff79c6>:=</span> <span style=color:#ff79c6>false</span>
</span></span><span style=display:flex><span>	err <span style=color:#ff79c6>:=</span> db.<span style=color:#50fa7b>Tx</span>(ctx, <span style=color:#8be9fd;font-style:italic>func</span>(tx <span style=color:#ff79c6>*</span>glassdb.Tx) <span style=color:#8be9fd>error</span> {
</span></span><span style=display:flex><span>		b, err <span style=color:#ff79c6>:=</span> tx.<span style=color:#50fa7b>Read</span>(coll, key)
</span></span><span style=display:flex><span>		<span style=color:#ff79c6>if</span> err <span style=color:#ff79c6>!=</span> <span style=color:#ff79c6>nil</span> {
</span></span><span style=display:flex><span>			<span style=color:#ff79c6>return</span> err
</span></span><span style=display:flex><span>		}
</span></span><span style=display:flex><span>		<span style=color:#ff79c6>if</span> bytes.<span style=color:#50fa7b>Equal</span>(b, oldval) {
</span></span><span style=display:flex><span>			swapped = <span style=color:#ff79c6>true</span>
</span></span><span style=display:flex><span>			<span style=color:#ff79c6>return</span> tx.<span style=color:#50fa7b>Write</span>(coll, key, newval)
</span></span><span style=display:flex><span>		}
</span></span><span style=display:flex><span>		<span style=color:#ff79c6>return</span> <span style=color:#ff79c6>nil</span>
</span></span><span style=display:flex><span>	})
</span></span><span style=display:flex><span>	<span style=color:#ff79c6>return</span> swapped, err
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>This snippet implements a simple
<a href=https://en.wikipedia.org/wiki/Compare-and-swap>compare-and-swap</a> for a key in
a <em>collection</em> – which is just a set of keys with the same prefix. The <em>db.Tx</em>
method wraps the operations into a single transaction, so that it can be retried
by GlassDB transparently.</p><p>I will not spend too much time discussing how to use it. If you are curious,
please check out the <a href=https://github.com/mbrt/glassdb>repository</a> and the
relative <a href=https://pkg.go.dev/github.com/mbrt/glassdb>reference documentation</a>.
The only thing I wanted to show is that you can put arbitrary code inside a
transaction, and <em>reads and writes to the database</em> will be, well,
transactional.</p><h2 id=choosing-the-algorithm>Choosing the algorithm<a hidden class=anchor aria-hidden=true href=#choosing-the-algorithm>#</a></h2><p>As I mentioned earlier, my goal was to choose an algorithm that guarantees the
strongest possible isolation levels, while retaining a reasonable level of
performance. The first thing that came to mind then was to use <a href=https://en.wikipedia.org/wiki/Two-phase_locking>Strict Two Phase
Locking (S2PL)</a>, similarly to
Spanner. This alone would guarantee
<a href=https://jepsen.io/consistency/models/serializable>serializable</a> isolation, and
combining that with the
<a href=https://jepsen.io/consistency/models/linearizable>linearizable</a> consistency of
GCS, I would get the top spot in isolation levels: <a href=https://jepsen.io/consistency/models/strict-serializable>strict
serializability</a>.</p><p>The algorithm itself is quite intuitive:</p><ol><li>A key must be locked as <em>read-only</em> or <em>read-write</em>, before you can read it.</li><li>It must be locked as <em>read-write</em>, before you can modify it.</li><li>You can release all the locks only after committing.</li></ol><p>My approach was to first <em>make it work</em>, and then <em>make it fast</em>.</p><h2 id=make-it-work>Make it work<a hidden class=anchor aria-hidden=true href=#make-it-work>#</a></h2><p>To make the algorithm above work, I needed locking and atomic commits.</p><h3 id=locks>Locks<a hidden class=anchor aria-hidden=true href=#locks>#</a></h3><p>GCS objects are made of two independent parts: content and metadata. Metadata
contains user-controlled key values and a version that automatically changes at
every write (or metadata change). Given this, it&rsquo;s easy to update an object&rsquo;s
metadata to mark it as &ldquo;locked&rdquo;. The algorithm is basically a compare-and-swap:</p><ol><li>Read the object&rsquo;s metadata with its version number. Is the &ldquo;locked&rdquo; tag set
to &ldquo;true&rdquo;?</li><li>If yes, wait a bit and repeat #1.</li><li>If no, <em>conditionally</em> write the metadata, setting &ldquo;locked&rdquo; to &ldquo;true&rdquo;. The
condition constrains the write to succeed only if the metadata version is the
same as the one we read in step #1.</li><li>If the write failed, go to #1. If it succeeded, we got the lock.</li></ol><p>Unlocks work similarly, but it does a compare-and-swap for &ldquo;locked=false&rdquo;
instead.</p><p>This algorithm is quite simplistic: it doesn&rsquo;t distinguish between read and
write locks, and crashes in the application may result in forever-locks. We&rsquo;ll
talk about crashes in a bit, but before we need to add read locks:</p><ul><li>Use a <em>lock-type</em> and a <em>lockers</em> tag, containing respectively the type of
lock (read, write, none) and the IDs of the transactions holding the lock.</li><li>Locking means updating the <em>lock-type</em> and adding the ID of the transaction
holding the lock to the <em>lockers</em> tag (we can modify multiple tags
atomically).</li><li>Unlocking is the opposite: remove the ID from <em>lockers</em> and if that was the
last locker, set the <em>lock-type</em> as <em>unlocked</em>.</li></ul><p>Some additional intuitive ground rules as well:</p><ul><li>Multiple transactions can hold <em>read</em> locks to the same key at the same time.</li><li>Read locks can be upgraded to <em>write</em>, but only if there was a single
transaction holding it.</li><li>Write locks are <em>exclusive</em>.</li></ul><h3 id=commits>Commits<a hidden class=anchor aria-hidden=true href=#commits>#</a></h3><p>Commits must appear instantaneously: a transaction is either committed or not,
and nothing in between. How do we do it with transactions updating multiple
keys? How do we realize those updates at the same time? It&rsquo;s clear that we can&rsquo;t
update one key at a time while releasing the locks, because if the process
crashes in the middle, we would get an inconsistent state.</p><p>One way would be to use <em>transaction logs</em>. Transactions stash the updates they
are going to do into a log, before actually doing them. This allows <em>replays</em>
after crashes. When the log is committed, the transaction is considered
committed and a process picking up the work after a crash will read it and
continue committing the writes.</p><p>The way I implemented this was to include all the values written by a
transaction into its transaction log upon commit. A transaction keeps the keys
untouched until commit, and only after the log is written successfully it
unlocks the keys while updating their values.</p><h3 id=deadlocks-and-crashes>Deadlocks and crashes<a hidden class=anchor aria-hidden=true href=#deadlocks-and-crashes>#</a></h3><p>Distributed locks require timeouts. If a process opens a transaction, locks a
few keys and then crashes, those keys would become inaccessible. Even if the
crash was temporary, we still would depend on all clients to come back online
reasonably quickly after crashes, which might be unreasonable.</p><p>The way to solve this is by introducing TTLs on locks. While locking a key, a
transaction also keeps track of the time it&rsquo;s taking to commit. After a timeout,
it will create (or update) its transaction log by stating that the transaction
is still pending, along with a refresh timeout. Competing transactions that want
to check whether a lock is still valid, just need to read the corresponding
transaction log. If the timestamp is too far back in the past, or no log appears
during the entire timeout period, the lock is considered &ldquo;expired&rdquo;.</p><p>However, the naive algorithm of observing a lock and just taking it over after a
period of inactivity, is incorrect. The reason is that if a transaction blocks
only temporarily, after the verify stage and before writing the transaction log,
there&rsquo;s no way for this transaction to check whether it&rsquo;s still allowed to
commit or it has lost some of its locks.</p><p><img alt="lock takeover bug" loading=lazy src=/posts/transactional-object-storage/lock-takeover-bug.svg></p><p>This cannot be solved by introducing more checks before committing, because the
delay could happen exactly <em>during</em> the write for the transaction log, which
would be impossible to block.</p><p>A way to solve this, is to use the <em>transaction log itself</em> as a synchronization
point. A transaction (2) that wants to take over an expired lock held by (1),
can try to conditionally write to the transaction log for (1), stating that it
is aborted. If it succeeds (i.e. the log didn&rsquo;t change in the meantime), then
(1) is officially aborted and all its locks are invalid. If it failed (because
e.g. the transaction successfully committed, or refreshed the locks), then (2)
will have to wait longer.</p><p>By making sure changes to transaction logs happen atomically and are never
overwritten incorrectly, we make sure that even in the presence of concurrent
changes, we always communicate the intention and the state of each transaction.</p><p>In the example above, if (2) succeeds in writing the transaction log for (1),
the write to the same log that (1) tries to do for its commit will fail, because
its condition would be set for a past version. At that point, (1) will just
abort.</p><h3 id=collections-create-and-delete-keys>Collections, create and delete keys<a hidden class=anchor aria-hidden=true href=#collections-create-and-delete-keys>#</a></h3><p>The way we handle creating and deleting keys in transactions is by using
pseudo-keys. There&rsquo;s one such key per collection, representing the &ldquo;list of
keys&rdquo; present in that collection. To ensure consistency when iterating through
keys or adding and removing keys, transactions just lock the corresponding
pseudo-key for each affected collection:</p><ul><li>Creating a key: lock the collection (i.e. its pseudo-key) in write and create
the key (locked in write).</li><li>Deleting a key: lock the collection and the key in write.</li><li>Iterating through a collection: lock the collection in read.</li></ul><p>This mechanism allows keeping the collection unlocked while keys are just
modified, but not added or deleted (which should be most of the time).</p><h2 id=make-it-fast>Make it fast<a hidden class=anchor aria-hidden=true href=#make-it-fast>#</a></h2><p>Up until this point, I had a working algorithm with strong isolation and
consistency, but was it fast? Given that object storage itself is not
particularly fast, to maximize throughput I wanted to avoid <em>transactions
operating on different keys to interfere with each other</em>. This excluded any
kind of global lock, or single transaction log, because they would generate
contention from all the transactions. I also wanted to maximize parallelism,
given that object storage is designed for horizontal scalability in mind.</p><p>In the quest of making things faster, we still should not forget the larger context and constraints:</p><ul><li>No server component</li><li>No communication between different clients</li></ul><h3 id=transaction-logs>Transaction logs<a hidden class=anchor aria-hidden=true href=#transaction-logs>#</a></h3><p>The easiest way to avoid transactions to interfere with each other is by having
a different log object for each transaction, rather than a single object for
all. The client assigns a random ID to each transaction and that is part of the
path of the log object. Each client is then able to read a transaction log
without the need to communicate with anyone, because given an ID, its path is
deterministic.</p><p>But how do we map keys to transactions affecting them? The answer is in objects
metadata. When locking, we keep track of the ID of the locker(s). A reader is
then able to check whether the object is still locked or not, by which
transaction and what is the state of that transaction, just by reading its
metadata. We have three cases to consider while testing a key:</p><ol><li>The <em>lock-type</em> is set to <em>none</em>: The key is <em>unlocked</em>.</li><li>The <em>lock-type</em> is set to <em>read</em> or <em>write</em>, and:<ol><li>There&rsquo;s one transaction listed in <em>locked-by</em> that is pending (by looking
at the corresponding logs): the key is <em>locked</em>.</li><li>Otherwise, the key is <em>unlocked</em>.</li></ol></li></ol><h3 id=optimistic-locking>Optimistic locking<a hidden class=anchor aria-hidden=true href=#optimistic-locking>#</a></h3><p>Fast is always relative and situational: faster compared to what? And under
which conditions? Which use cases are we willing to sacrifice in order to
improve on the most important ones? The key decision for me was to optimize for
low data contention workloads. If writes to the same key at the same time are
rare, I could optimize for transactions that don&rsquo;t conflict with each other, to
the expense of transactions that do. This idea points in the direction of
<a href=https://en.wikipedia.org/wiki/Optimistic_concurrency_control>optimistic
locking</a>, where
transactions use data resources without acquiring locks. Transactions verify
that no other transaction affected the same data only on commit. It also
optimizes the case where object storage shines, which is not latency, but
throughput.</p><p>The algorithm works like this:</p><ol><li><strong>Modify:</strong> Read keys and tentatively write (i.e. stage locally) changes.</li><li><strong>Verify:</strong> Check that other transactions did not modify read values <em>after</em>
the transaction started.</li><li><strong>Commit:</strong> If no conflicts are detected: make the changes effective
(commit), otherwise retry the transaction.</li></ol><p>Optimistic locking avoids locking, <em>but implicitly requires transactions to
validate and commit one at a time</em>. This is because verify and commit must be
executed atomically. If not, we would risk <a href=https://jepsen.io/consistency/phenomena/lost-write>losing
writes</a>:</p><p><img alt="parallel verify bug" loading=lazy src=/posts/transactional-object-storage/parallel-verify-bug.svg></p><p>To avoid this problem and maintain the isolation level achieved by the base
algorithm, I used a modified version of optimistic locking. One that, well –
uses locks. The difference from the base algorithm is that transactions run
without locks until the <em>verify phase</em>, where they acquire all the locks in
parallel, verify for conflicts safely and only then, commit a transaction log
(or retry):</p><p><img alt="transaction example" loading=lazy src=/posts/transactional-object-storage/glassdb-tx-conflict.svg></p><ol><li>Read all the key values it needs without locks.</li><li>Write values in a staging area (in memory).</li><li>When committing:<ol><li>Lock all the keys that were read or written in parallel.</li><li>Check that the read keys haven&rsquo;t changed in the meantime. If yes, retry
the transaction from #1.</li><li>Write a transaction log that includes which keys were modified along with
their new values.</li></ol></li><li>(async) Write the new values back to the modified keys.</li></ol><p>This avoids global synchronization points. If two transactions touch different
keys, they will not compete for the same objects, so they are able to proceed in
parallel and locks are kept for the shortest amount of time.</p><p>Note that in #3b, when retrying, locks are not released. This has the side effect that transactions will be retried at most once, as the second run will happen while still holding the locks, turning it effectively into pessimistic locking. No other transaction will be able to commit those keys in the meantime.</p><h3 id=read-only-transactions>Read-only transactions<a hidden class=anchor aria-hidden=true href=#read-only-transactions>#</a></h3><p>We are already optimizing for high throughput, low contention workloads. In this
scenario, read-only transactions are also going to run undisturbed most of the
time. Can we take advantage of that and run the standard optimistic concurrency
control algorithm without locking? Remember that <em>verify steps</em> of transactions
must run sequentially, to prevent losing updates. In the general case of
read-write transactions, both the current transaction can interfere with others,
and others can interfere with the current. For read-only transactions, however,
we don&rsquo;t have to worry about the first problem. They don&rsquo;t write anything, so
they can&rsquo;t interfere with other transactions&rsquo; <em>verify</em> and <em>commit</em> steps.
Therefore, the only thing to do is to make sure no transaction modified keys
while our read-only transaction was reading them, i.e. between our first and
last read.</p><p>We can achieve this in a relatively simple way:</p><ol><li>Read phase: read all required keys while keeping track of their current
version.</li><li>Verify phase: check the current version of all the keys read again:<ol><li>If the versions didn&rsquo;t change, and the keys were not locked in read-write,
we can return successfully.</li><li>Otherwise, there was a concurrent write and we retry.</li></ol></li></ol><p>Note that in (2) we not only check that the object version stayed the same, but
that the metadata version did. The actual check is slightly more complicated, as
we can avoid retrying in a few cases even when metadata did change, for example
when the key was locked in read and unlocked, or when locked, but the
transaction aborted.</p><p>To avoid retrying an unlucky transaction forever, after the first retry we fall
back to the regular algorithm, which will acquire (read) locks to keep writers
away. This approach guarantees progress eventually.</p><p>If the transaction succeeded the first time around, since we didn&rsquo;t really make
any change in values, nor in the state of locks, we can avoid writing a
transaction log as well. The happy path requires no writes and just one value
plus one metadata read per key.</p><p>This approach is different from the one taken by Spanner, which implements
<a href=https://en.wikipedia.org/wiki/Multiversion_concurrency_control>Multiversion concurrency
control</a>. MVCC
allows reads to go much faster, by reading a <em>consistent</em> snapshot of the
database. This is hard to do correctly, because the
<a href=https://jepsen.io/consistency/models/linearizable>linearizable</a> consistency of
the database depends on picking a version compatible with the real time ordering
of all transactions globally. Picking anything older than that would lead to
weaker consistency models. Spanner achieves this <a href=https://timilearning.com/posts/mit-6.824/lecture-13-spanner/#read-only-transactions>the hard
way</a>,
through atomic clocks and bounded time uncertainty.</p><p>Given that we don&rsquo;t keep multiple versions of objects and we don&rsquo;t have access
to atomic clocks, nor we have transactions coordinators, I chose to revert to
two-phase locking in case of any conflicts. This makes sure we stay in the
strict serializability isolation level.</p><h3 id=single-read-modify-write>Single read-modify-write<a hidden class=anchor aria-hidden=true href=#single-read-modify-write>#</a></h3><p>We can optimize transactions that read and write to a single key by using native
object storage compare-and-swap (CAS) operations. The algorithm becomes:</p><ol><li>Read the key contents.</li><li>During validation: read the key metadata<ol><li>If it&rsquo;s locked: retry (go to #1)</li><li>Otherwise, check that the version is the same it was during the read. If
not, retry (go to #1)</li></ol></li><li>Do a conditional write on the key with the new value, and set the condition
to be that content and metadata versions match what we read in #2.</li></ol><p>In case of conflicts, we can retry the transaction with the base algorithm, so
that the transaction is not starved in case of high contention.</p><h3 id=local-cache>Local cache<a hidden class=anchor aria-hidden=true href=#local-cache>#</a></h3><p>The last optimization I wanted to talk about is the local cache. The cache is
useful to eliminate reads when a client is the only one updating a certain key.
A set of transactions like these would require reading the same key 100 times,
even in the case no other transaction touched it:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span><span style=color:#ff79c6>for</span> i <span style=color:#ff79c6>:=</span> <span style=color:#bd93f9>0</span>; i &lt; <span style=color:#bd93f9>100</span>; i<span style=color:#ff79c6>++</span> {
</span></span><span style=display:flex><span>	db.<span style=color:#50fa7b>Tx</span>(ctx, <span style=color:#8be9fd;font-style:italic>func</span>(tx <span style=color:#ff79c6>*</span>glassdb.Tx) <span style=color:#8be9fd>error</span> {
</span></span><span style=display:flex><span>		b, _ <span style=color:#ff79c6>:=</span> tx.<span style=color:#50fa7b>Read</span>(coll, key)
</span></span><span style=display:flex><span>		<span style=color:#ff79c6>return</span> tx.<span style=color:#50fa7b>Write</span>(coll, key, <span style=color:#8be9fd;font-style:italic>string</span>(b) <span style=color:#ff79c6>+</span> “a”)
</span></span><span style=display:flex><span>	})
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>For this reason, I introduced a local <a href=https://en.wikipedia.org/wiki/Cache_replacement_policies#LRU>LRU
cache</a> that keeps
in memory the values committed by the most recent transactions. The cache has a
limited and configurable size, to make sure it won&rsquo;t eat all the available
memory.</p><p>The way this works is by checking the cache during every read in a transaction.
If there&rsquo;s a value, the transaction uses that. If, during validation, the value
turned out to be outdated, the key is marked as stale and the transaction
retried. During the retry, the cache would be invalid, and so the transaction
will go and fetch the fresh value from object storage.</p><h2 id=benchmarks>Benchmarks<a hidden class=anchor aria-hidden=true href=#benchmarks>#</a></h2><p>How performant is this thing? Details are on the
<a href=https://github.com/mbrt/glassdb/blob/main/README.md#performance>glassdb</a> repo,
but in summary, low conflicts workloads lead to a somewhat linear throughput
with the number of transactions.</p><p><img alt="throughput graph" loading=lazy src=/posts/transactional-object-storage/tx-throughput.png></p><p>The graph above shows three different types of transactions, where the bold line
is the median number of transactions per second and the error band includes the
10th and 90th percentiles. The dataset above was composed of 50k keys and each
client is doing 10 transactions in parallel (10% read + write 2 keys, 60% read
2 keys, 30% weak reads<sup id=fnref:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup> of a single key).</p><p>Given that keys are selected randomly by transactions, you can see how higher
percentiles are affected by the higher concurrency, because of the higher
probability of conflicts.</p><p>Instead, latency – which is not the strong point of the solution – is higher
than most databases, but stays relatively flat even with higher concurrency.
Exception being with higher levels of contention.</p><p><img alt="latency graph" loading=lazy src=/posts/transactional-object-storage/tx-latency.png></p><h2 id=is-that-all>Is that all?<a hidden class=anchor aria-hidden=true href=#is-that-all>#</a></h2><p>The article is pretty long, and even if it doesn&rsquo;t cover everything – and I
would have really liked to talk about the source code, testing, correctness,
profiling and more optimizations – it covers the most important aspects of the
design. I hope I was able to transmit some of the key lessons I learned along
the way and demystified some of the opaque sounding, but actually not so hard to
understand, terminology, used and abused in database design and classification.</p><p>If you want to explore some of the source code, I recommend starting from the
<a href=https://github.com/mbrt/glassdb/blob/920129dabc5164b70c9dc04817a97cb67f4f609f/internal/trans/algo.go#L116>transaction
algorithm</a>.
There aren&rsquo;t many comments, but hey it&rsquo;s written in Go, it should be
understandable :)</p><p>One more thing: did I forget about S3? At the beginning, I praised how easy it
would be to have the same solution across cloud providers. Did this come to
pass? Not really. Or, at least, I spent so much time making this work on GCS
that I never had the time to port it to other cloud providers. It also turns out
to be not so easy, because Google Cloud provides a superset of conditional
updates with respect to AWS S3.</p><p>Well, no problem. I learned something and solved something. The rest can wait
for another release.</p><p>Feedback on <a href="https://news.ycombinator.com/item?id=42164058">Hacker News</a>.</p><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>See <a href=https://fauna.com/blog/introduction-to-transaction-isolation-levels>Introduction to Isolation Levels</a> in the Fauna blog for more details on this.&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p>See also <a href=https://assets.ctfassets.net/po4qc9xpmpuh/2LkoSujDTxtMWdzso84WDw/0abe5c3a121b83096ab99830c01e147a/Fauna_architectural_overview.pdf>Fauna architectural overview</a>.&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3><p>See CockroachDB&rsquo;s <a href=https://www.cockroachlabs.com/blog/consistency-model/>blog post</a> on consistency and the <a href=https://jepsen.io/analyses/cockroachdb-beta-20160829>Jepsen report</a>.&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:4><p>A weak read is a read that is allowed to get a stale value, bounded by a user specified limit. For example, a weak read with a 10s staleness bound, is allowed to read the latest value of a key, or any value written in the previous 10s from the cache, but no earlier.&#160;<a href=#fnref:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><footer class=post-footer><ul class=post-tags><li><a href=https://blog.mbrt.dev/tags/db/>Db</a></li><li><a href=https://blog.mbrt.dev/tags/design/>Design</a></li><li><a href=https://blog.mbrt.dev/tags/cloud/>Cloud</a></li></ul><nav class=paginav><a class=next href=https://blog.mbrt.dev/posts/better-design-docs/><span class=title>Next »</span><br><span>Better design docs</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Glassdb: transactional object storage on x" href="https://x.com/intent/tweet/?text=Glassdb%3a%20transactional%20object%20storage&amp;url=https%3a%2f%2fblog.mbrt.dev%2fposts%2ftransactional-object-storage%2f&amp;hashtags=db%2cdesign%2ccloud"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Glassdb: transactional object storage on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fblog.mbrt.dev%2fposts%2ftransactional-object-storage%2f&amp;title=Glassdb%3a%20transactional%20object%20storage&amp;summary=Glassdb%3a%20transactional%20object%20storage&amp;source=https%3a%2f%2fblog.mbrt.dev%2fposts%2ftransactional-object-storage%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Glassdb: transactional object storage on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fblog.mbrt.dev%2fposts%2ftransactional-object-storage%2f&title=Glassdb%3a%20transactional%20object%20storage"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Glassdb: transactional object storage on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fblog.mbrt.dev%2fposts%2ftransactional-object-storage%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Glassdb: transactional object storage on whatsapp" href="https://api.whatsapp.com/send?text=Glassdb%3a%20transactional%20object%20storage%20-%20https%3a%2f%2fblog.mbrt.dev%2fposts%2ftransactional-object-storage%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Glassdb: transactional object storage on telegram" href="https://telegram.me/share/url?text=Glassdb%3a%20transactional%20object%20storage&amp;url=https%3a%2f%2fblog.mbrt.dev%2fposts%2ftransactional-object-storage%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Glassdb: transactional object storage on ycombinator" href="https://news.ycombinator.com/submitlink?t=Glassdb%3a%20transactional%20object%20storage&u=https%3a%2f%2fblog.mbrt.dev%2fposts%2ftransactional-object-storage%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2024 <a href=https://blog.mbrt.dev/>mbrt blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>