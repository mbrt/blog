[{"content":"Running a Site Reliability Engineering (SRE) organization correctly is difficult and expensive. Spare the frustration, perhaps what you need is Sysadmins.\nSince the term was coined by Ben Treynor in 2003 at Google, lots of ink was spent on praising SRE practices. Not enough on when it is appropriate to have SREs. This post is a take on that angle.\nDisclaimer: I was an SRE at Google and this piece represents only my own views.\nWhat is an SRE? Among the various definitions floating around, let\u0026rsquo;s use the most common from the Google SRE books. SRE is both a set of practices and a job title. Ultimately, the idea is to solve operational problems through automation and share responsibilities with developers. These are the most important principles of SRE:\nOperations is a software problem (i.e. you need Software Engineers). Manage by Service Level Objectives (SLOs) (i.e. measure to take decisions about reliability vs feature velocity). Work to minimize toil (i.e. manual work is bad). Automate this year\u0026rsquo;s job away (again, manual work is bad. Use automation). Move fast by reducing the cost of failure (i.e. reduced impact of faults increases dev velocity). Share ownership with developers (i.e. SRE is not a gatekeeper, devs co-own the outcomes). SREs can fulfill their mission from different angles (consultation for devs, on-call expertise, improvements in internal platform or migrating to newer infrastructure). But SREs are primarily software engineers. They are encouraged to improve the reliability of systems through software, as opposed to manual work.\nThe biggest contribution from the SRE, and more widely from the DevOps movement, is recognizing that reliability work can be specialized engineering work and at the same time should not be treated as a silo from development.\nWhat can go wrong This is all nice and more or less obvious nowadays. But what can go wrong?\nMany common problems and solutions are discussed in the canonical Google SRE books. The focus there is however on “day 1\u0026quot;. The goal of the books is to move from the “old world\u0026quot; of ops into a new one, where devs and ops are not siloed and reliability improvements are measurable.\nHere I want to focus only on day 2 problems: I have SREs implemented By The Book and it\u0026rsquo;s 2023. Is it all ponies and rainbows?\nUnfortunately Things can go wrong at all levels. They can go wrong for SRE teams in many more ways than developer teams. I think this comes from the fact that even though SRE is a young discipline, it also crystallized quickly. In its 20 years of life, many things changed in the software domain. The definition of SRE did not (and stayed quite narrow).\nMost of the problems I\u0026rsquo;ve encountered so far can be categorized as follows:\nPower dynamics where SRE is at the short end. Mismatch in expectations between SREs and business needs. Lack of influence in setting priorities. Failure to acknowledge the rise of platforms. What the hell am I talking about? Let\u0026rsquo;s go in order.\nThe vicious cycle of being Somewhat Irrelevant Here\u0026rsquo;s a handy picture:\nWhen a team starts, it never does with a lot of influence or trust from other parts of the organization. This happens despite all efforts and it\u0026rsquo;s normal. Everyone needs to prove themselves when they begin a new job and this applies to newly formed teams as well.\nThis lack of trust translates into lack of influence to the reference dev organization. Which leads to a lack of Big Enough Projects and therefore Impact. There will be a seniority ceiling nobody in the team can ever smash. The exact level depends on the specifics, but it\u0026rsquo;s usually quite lower with respect to the one in the dev organization.\nIf this stays true for long enough, senior people move to teams where they can more easily find impact. The remaining people cannot get promoted and the vicious cycle continues. Interestingly, this can\u0026rsquo;t be fixed by adding senior engineers to the team. You can try, but internal transfers will be joining begrudgingly (and so either refuse to go or leave at the first opportunity) and external hires will have a very hard time ramping up (and follow a similar destiny).\nSuccess stories do happen in these teams as well. They are usually single people who can swim upstream and find a niche for themselves. These stories are nice and give hope, but they are never reproducible. Try the exact same steps and you will fail. In these teams people are disconnected from each other (as projects are relatively small) so you can\u0026rsquo;t look up to more senior people and do the same to get promoted. It\u0026rsquo;s very hard to escape the vicious cycle of being Somewhat Irrelevant.\nJustifying proactive action is hard Much of what SRE does (and wants to do) is proactive. You work today to solve tomorrow\u0026rsquo;s problems. The challenge is that SRE does not operate in isolation. You always need to convince dev partners to be onboard with your plans. And this is hard because:\nSee above: you might lack influence. Adding features is always more shiny than fixing old crap. In economic downturns or when the company has to play catch-up with competitors, the focus is on today, not tomorrow. Before devs are convinced of doing major changes to accommodate reliability improvements, you will need multiple major incidents. At that point, the necessity of improvements will be clear, but did you really need SRE to realize it? Perhaps you\u0026rsquo;ll win an I-Told-You-So badge of honor, but not much more.\nIn the meantime, SRE needed to shoulder the incidents and the consequent manual work to keep things running anyway.\nThis state can be temporary but could also be somewhat permanent. Operational work shoots up and all projects that require dev collaboration are at the back of their queue. Justifying proactive action is hard!\nYou might think: \u0026ldquo;But I know how to solve this. Give back the pager!\u0026rdquo; If that\u0026rsquo;s your thought, please continue below.\nPower dynamics Oftentimes the SRE organization is at the short end in power dynamics. This comes from multiple factors, but the most common causes are:\nThe business requires SRE to operate critical services. The dev organization is effectively funding SRE. The first one is easy to understand. You can\u0026rsquo;t give back the pager if the business explicitly forbids you to do so. This seems like an anti-pattern, but think about it. The CEO wants to protect their own crown jewels. What\u0026rsquo;s the best way to do it? Give the pager to whomever\u0026rsquo;s best to resolve incidents. The business doesn\u0026rsquo;t care whether you\u0026rsquo;re a happy on-caller or not. It just cares that things keep running.\nThe second is more insidious. Most tech companies are organized so that development organizations are effectively the ones that hold budgets. They may decide that part of the budget is used to fund SRE. Even though SRE is a parallel organization (i.e. mostly independent from a management perspective), it is in effect controlled by the developer org, through funding.\nThis control doesn\u0026rsquo;t have to be explicit. But it\u0026rsquo;s enough to skew the incentives on the SRE management side. For example, in thinking that to grow their SRE team they need to onboard new services. Even if nobody forces them, it\u0026rsquo;s still a powerful argument to get funding: \u0026ldquo;I need more people if you want me to support more services\u0026rdquo;.\nGiving back the pager removes the lever and goes in the opposite direction. It will prevent new funding or even trigger a team dismantling. This is bad for promo. No SRE director will be promoted to \u0026ldquo;senior director\u0026rdquo; by managing fewer teams, so they don\u0026rsquo;t do it.\nThese forces make sure that SRE holds the pager, no matter what.\nObviously there are exceptions to this. I have seen successful pagers handover, but they are exceptionally rare.\nPlatforms eating SRE Brief history of how production platforms are born.\nIn the beginning there was Chaos. Dev teams maintained their own infrastructure, making the same mistakes over and over. To reign the chaos, SRE was born. They brought a unified perspective to multiple teams, providing guidance by virtue of experiencing what was or wasn\u0026rsquo;t working in production (the \u0026ldquo;Wisdom of Production\u0026rdquo;).\nThen the company grows bigger and SRE teams multiply. They all want to automate themselves out of their job and so develop automation. Many Different Versions Of It. And the same observation applies: we need to provide unified production to the whole company. And a platform team is born.\nThe SREs job now shifts from \u0026ldquo;automate your toil away\u0026rdquo; to \u0026ldquo;bring service X to The Platform\u0026rdquo;. All is well. But what happens next?\nThe SRE book says:\nA production platform with a common service structure, conventions, and software infrastructure made it possible for an SRE team to provide support for the \u0026ldquo;platform\u0026rdquo; infrastructure, while the development teams provide on-call support for functional issues with the service—that is, for bugs in the application code. Under this model, SREs assume responsibility for the development and maintenance of large parts of service software infrastructure, particularly control systems such as load shedding, overload, automation, traffic management, logging, and monitoring.\n\u0026ndash; SRE Book [Chapter 32]\nAll is well? Not quite, as platforms are developed and maintained by a product team, not SRE!\nThe implications are interesting:\nSRE doesn\u0026rsquo;t own the platform, so they can\u0026rsquo;t directly change it to suit their needs. Supporting the platform itself is not a job for 1000 SREs, so only a few get to work on it. Incentives in the platform team are on minimizing maintenance cost, which means a lot of feature requests get shoved under the rug. Platform teams don\u0026rsquo;t need to please their customers so much if they get them through company mandates. This last point is the real problem. The business wants to minimize cost by de-duplicating work. They do so by mandating the use of the Blessed Platform. This makes sense, but creates perverse incentives. Guess who loses in this? SRE, because they are \u0026ldquo;such a small customer\u0026rdquo; compared to the rest of the company. Remember, there are many more devs than SREs. The platform will always try to prioritize problems for the majority of their customers.\nIn addition, platforms ate up a chunk of interesting work from SRE (automation). As fun as it sounds, SREs haven\u0026rsquo;t automated themselves out of their job, but only out of its most interesting part.\nMismatch in expectations SREs are first and foremost engineers. In many cases they come from pure software development and even have PhDs. They come to SRE expecting to have a software engineering job (i.e. building systems, researching cutting edge technologies) and just apply their skills to the \u0026ldquo;reliability domain\u0026rdquo;.\nWell, it often doesn\u0026rsquo;t work that way. Yes, there might be a 50% cap on toil, but what do SREs need to do with the rest of their time? From migrating one technology to the next to changing obscure configuration files, the reality for most of them is not so interesting.\n\u0026ldquo;What about SLOs or monitoring?\u0026rdquo; I hear you saying. PMs should own SLOs because they own the user experience. Platforms should own the SLO and alerting implementation, because there\u0026rsquo;s no need to reinvent the wheel in every team. What\u0026rsquo;s left for SRE? Well, they can put the little number in that config file there.\nTo be fair, not all of it is easy work. Migrations are especially delicate and need planning. Stakes are high and mistakes expensive. But is it interesting work? Changing one number from X to Y and waiting a week for a rollout? This feels like walking on ice for miles and miles. Challenging but just tedious.\nIt\u0026rsquo;s important work. You just don\u0026rsquo;t need a PhD or 15 years of experience to do it.\nThis mismatch in expectations is a very common experience for SRE new hires. Ultimately, most of the frustration comes from a disconnect between what an SRE is paid for (i.e. what the business wants from them) and what they want to be doing. The hard question is what value is SRE adding to the business?\nSuccessful engineers in platforms don\u0026rsquo;t automatically make good incident responders and vice-versa. Insisting on bundling together the two roles has several negative repercussions on teams.\nUnclear business value Signs that an SRE team has unclear business value:\nThe dev organization is too small. This happens when e.g. SREs are on-call for most services of the org and it\u0026rsquo;s hard to negotiate what services to focus on. Services are not critical to the business. Did anyone notice when one of your services was down for two hours? SLOs are always red, paging or ticketing, but no actual user complains about it. These are signs that you are being on-call not to serve the end users (and so the business), but to \u0026ldquo;serve the devs\u0026rdquo; and ease their operational burden.\nThis can also happen for just a part of the services the SREs are on-call for. It\u0026rsquo;s often not a straightforward picture.\nPartial solutions What if we take Chapter 32 of the SRE book to the letter? If the production platform can automate most ops tasks, SRE could give back all the services to the devs and just be on-call for the platform. This would have two immediate effects:\nDrastically reduce the number of SRE teams necessary. Many more devs start to be on-call 24/7. The company saves a bunch of money with the first point, but loses a bunch more with the second. There are way more dev than SRE teams and, contrary to SREs, a dev team is often in a single location. This means that a lot of people will be on-call during the night (because there\u0026rsquo;s no team across the ocean to hand off the pager to), which is very expensive and bad for retention (stressed out people tend to leave).\nMoney is one thing, effectiveness is the other. Can devs actually manage incidents effectively? They can definitely do it when the cause is within the service itself (e.g. a bug). But many outages happen in the cracks between services. The land of nobody. It often happens that an outage is caused by a service dependency, but the people managing the dependency can\u0026rsquo;t see anything wrong with their service.\nIncidents involving multiple teams require coordination. However there\u0026rsquo;s no SRE team to escalate to. It would be wrong to use platform SREs to do that, unless the platform itself is at fault. There won\u0026rsquo;t be a single team in charge of the overall incident, nor any team to escalate this to.\nThis setup overlooks also a third problem, which is operational expertise for critical services. There\u0026rsquo;s always a need for incident response experts for services that are both complicated and critical.1 The developers of those services may not have the skills to be good at that. And for this type of service, incidents must be resolved quickly (i.e. the business requires it).\nWe then have three unresolved problems:\nEveryone being on-call is expensive. Missing expertise for incident response in critical user facing services. Missing incident response on large (i.e. multi-service) outages. Let\u0026rsquo;s fix that?\nIncident Response Teams to the rescue Problem #2 seems to suggest a specialized on-caller role. Yes, critical services could still be managed by their dev teams. BUT, given the criticality, there\u0026rsquo;s still room for incident specialized responders. This is \u0026ndash; surprise surprise \u0026ndash; an ops role.\nProblem #3 is also for an ops role. This clearly requires specialization, but not necessarily SRE. This team needs strong systems understanding and incident response skills, but it doesn\u0026rsquo;t have to go beyond that. There\u0026rsquo;s no need for them to automate anything. In fact, incidents in this category are oftentimes black swan-like events, and as such, difficult to predict and unlikely to reoccur the same way. Their focus is on ops during incidents. Spare time should focus on post-mortems and consultation with the platform team and the business deciders. They are the best people that can answer the question: what are the biggest reliability risks right now?\nMinimizing cost The problem of cost (problem #1) is a bit more complicated to solve. There\u0026rsquo;s no way around having literally everyone building a service also being on-call and responsible for it. Shit happens, but these are the people having the best hand at fixing short and long term problems. If the dev team is paged every night for a month, they\u0026rsquo;ll fucking see at fixing the problem, trust me. The wisdom of production doesn\u0026rsquo;t really apply to SRE only. Wisdom arrives to anyone exposed to how systems behave. SRE shouldn\u0026rsquo;t rob developers this learning opportunity.\nThis, and constant improvements coming from the platform, should drive the need for routine ops work down. Emergencies outside business hours become more painful, so teams will strive to minimize them:\nMinimize critical dependencies. Stronger incentives to not build fucking Rube Goldberg machines, but better systems. No releases on public holidays. … and so on. I think you get the gist. Run your own shit. You\u0026rsquo;ll get the wisdom of production.\nThis will make it so that besides critical services (which should be minimized anyway), there will not be much need for highly reactive work. This will drive the number of teams that need to be on-call with guaranteed response time down (mitigating problem #1). This will not eliminate on-call in dev teams. It will be much less stressful and cheaper.\nIf a service considered not a critical dependency suddenly becomes important in an outage, the global incident response team should have the permissions to fix whatever they need, or page the shit out of anybody that can help. And this should be very rare.\nPutting everything together This is how things would roughly look like:\nIncident Response Teams (IRT) are responsible for both:\nemergency response for critical (i.e. revenue impacting) services, and providing an escalation path during large incidents. Is this the same as SRE by another name? No! Note how:\nRoutine operations are always a responsibility of the devs. Emergency operations are also mostly on the dev teams, except for a small number of critical services. Building a reliable system is still a dev responsibility. IR teams are only responsible for incident management and coordination. The teams can be quite a lot smaller than SRE, thanks to the reduced responsibilities and a pervasive platform that makes operations (e.g. drain a cluster, roll back a release, etc) look the same across the board.\nThis avoids most of the problems presented above: there\u0026rsquo;s no real need to influence the devs to make the system more reliable. It\u0026rsquo;s mostly on them anyway. Expectations are clear: incident response and ops. No engineering work required nor expected. The business value is 100% clear. They only work on incidents that impact revenue, which are important by definition.\nCareer I see one problem with this role: career and prestige. After years of Internet People bashing on ops (because it\u0026rsquo;s manual, inefficient, etc), the profession has now become unattractive. This is a marketing problem. I\u0026rsquo;m firmly convinced that ops will not go away anytime soon. Never mind AIs, bots, automation. Incidents happen and the more automation, the nastier they become. Yes, the profession needs to evolve from turning machines on and off to carefully operating complex tools. But it\u0026rsquo;s still operating things, just at a higher level of abstraction.\nI also see clear career paths where the better one becomes, the higher the influence over the business. From e.g. ops for a single (critical) service to coordinating large scale incidents across multiple teams. There\u0026rsquo;s a clear career progression from service IRT to company-wide IRT. Bigger scope, bigger responsibilities, more expertise required. It is only applicable to companies of a certain size, true. But this is true for most career paths in tech.\nI see a clear evolution (and career) path for Sysadmins here. From ssh-ing and rebooting machines to operating higher-level tools and influencing the business.\nConclusion In the software operations space, SRE and DevOps movements were fundamental innovations. They brought to attention important principles2. There\u0026rsquo;s however a common misconception: you need SRE to apply SRE principles.\nThere\u0026rsquo;s a perfectly valid alternative, which is\u0026hellip; just apply the principles? Developers themselves can run most services. In a world where platforms apply best practices, are mostly automated and devs know how to develop scalable services, the potential benefit of SRE teams is vastly reduced.\nCan the picture be as black for SRE as I paint it? As always, the answer depends. Depends on the maturity of the company and on how much SRE leadership is invested in running things As They Always Did. Some teams see the effects more than others. Some are shielded because they are closer to infrastructure.\nI think most of this really comes from a fact of ownership. SRE doesn\u0026rsquo;t really own systems end-to-end. Results are better when teams are empowered and responsible for the full lifecycle and outcome of their product.\nI also want to stress that this is not specific to Google only, even if that\u0026rsquo;s the place I know best. The pressures are the result of power dynamics that happen in many tech companies. Perhaps not with the same speed, but they are happening. If you are in a small company, you\u0026rsquo;re probably thinking I\u0026rsquo;m talking nonsense. And I am to some extent. This only applies to companies a certain size.\nThe goal of this little piece of mine was not to blame anyone or Rage Against The Machine. This is one of those cases where everyone\u0026rsquo;s best intentions cause a bad situation. I mostly wanted to bring to light some negative dynamics of “day 2 in SRE\u0026quot;.\nPart of the unfortunate situation is also the amount of gaslighting, because so many people have spent lots of time promoting SRE. Talking about the problems is much harder. If you are an SRE leader, you also don\u0026rsquo;t want your teams to shrink to a fraction of what they were, so you fight against the current. And deny the evidence.\nConversely, I don\u0026rsquo;t have a vested interest in any of these dynamics. Or at least, not anymore.\nBy critical I mean either directly impacting revenue (e.g. Ads serving in Google) or be a mandatory dependency of a revenue-critical service (e.g. a load balancer service).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nLike \u0026ldquo;running production is a shared responsibility\u0026rdquo;, or that \u0026ldquo;you need SLOs with consequences\u0026rdquo;.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://blog.mbrt.dev/posts/no-need-sre/","summary":"Running a Site Reliability Engineering (SRE) organization correctly is difficult and expensive. Spare the frustration, perhaps what you need is Sysadmins.\nSince the term was coined by Ben Treynor in 2003 at Google, lots of ink was spent on praising SRE practices. Not enough on when it is appropriate to have SREs. This post is a take on that angle.\nDisclaimer: I was an SRE at Google and this piece represents only my own views.","title":"Maybe you don't need SRE"},{"content":"This is the story of me trying to win a game of chess against my brother. A single freaking game. What\u0026rsquo;s so special about it? Am I good at chess? Not at all. Did I improve at my game in the process? Also no. Is it a story about \u0026ldquo;the journey rather than the destination\u0026rdquo;? Not really. Did I at least have fun in the process? Not so sure. This is the story of me trying to be unconventional at probably the most studied game in existence and using my software engineering background for something that probably doesn\u0026rsquo;t need it.\nAlthough I\u0026rsquo;m a total disaster at chess and this post is useless to whoever seriously wants to improve at their game, I still think it was worth sharing how it\u0026rsquo;s possible to apply software engineering principles to a problem. Was I successful? You\u0026rsquo;ll see at the end.\nIntro: why I got into chess During the 2020 COVID19 pandemic, my brother, along with many other people, took a passion for playing online chess. After playing for a couple of months he started speaking very enthusiastically about it and challenging other family members, but where my father would budge (and get digitally butchered) I wouldn\u0026rsquo;t. For one thing, I refrained myself from delving into a potentially very time consuming hobby. I knew enough about chess to understand that to become even a mediocre amateur club player you\u0026rsquo;d still need to sink hundreds if not thousands of hours in the game. I admit I also didn\u0026rsquo;t like the thought of losing against my brother, which was certain at the time, since he already had hundreds of games under his belt and I had none.\nOne day I finally gave in and accepted a challenge from him. Needless to say that I lost completely. I knew the rules and the rudimentary of the game, having played a little bit as a kid, but my brother was obviously no match. Looking at the post-game analysis in chess.com I saw that my disadvantage only grew, move by move, until reaching a +9 evaluation (equivalent to having lost one rook, a bishop and a pawn to zero), which is beyond any hope for a comeback, where I resigned. This blueprint was followed during another couple of matches, where I understood I had to do something to avoid being demolished every time.\nThis was when I decided I wanted to study the game a bit more.\nFirst attempt: learn My first attempt in trying to improve at the game was to do the obvious: head to Reddit and YouTube to see what other learners recommended. Between a tutorial from GM Naroditsky, some reading and puzzle solving on Lichess, I also played a few games with random people on the Internet. My rating stayed pretty low nevertheless (1300 - 1400 Rapid on Lichess).\nAfter another couple of lost matches against my brother, it dawned on me that I had no hope of beating him. I was following his same steps to improve (playing, studying the game, watching videos), but I was dedicating much less time. At that point he was playing hundreds of games a month and I was willing to play maybe 10. At that pace I was only going to get further and further behind.\nIt was at that point that I had my second realization: I didn\u0026rsquo;t really care about the game, I didn\u0026rsquo;t want to really improve at chess in general, I only cared about beating a single person: my brother.\nSecond attempt: study the opponent A chess match can be generally divided into three phases: the opening, the middlegame and the endgame. Converting a significant advantage into a victory during an endgame is usually \u0026ldquo;easy\u0026rdquo;, after studying some basic checkmate patterns, so the question for me was how to get that advantage in the first place. Gaining advantage during a middlegame is usually achieved by long-term strategy and tactics. The first can be improved by reading and studying the game principles (something I can enjoy) while the second is only possible by doing puzzles (which I don\u0026rsquo;t particularly like doing). I knew that I would be at a disadvantage here, given that my brother used to do about 20 puzzles a day on chess.com, something I would never be able to catch up to. This only left one possibility: gaining advantage during the opening.\nChess opening theory is humongous and involves memorizing long sequences and variations of moves, along with possible replies from the opponent. Beginners don\u0026rsquo;t really need to memorize much, but some familiarity with the most common openings can go a long way (or so I was told). What I tried then was to look at some random games that my brother played and try to study the openings he was using. I looked at the Italian opening and Sicilian defense on Lichess and tried to memorize the basic ideas behind them. I also watched a bunch of videos on YouTube.\nObviously my brother had already done all of this before me (and better) and so I understandably lost again. Not to mention that memorizing meaningless (at least to me) opening moves is boring and laborious. I didn\u0026rsquo;t really have fun doing any of that. Another issue was that after my opponent deviated from the known book moves, I had no idea about how to react, because I didn\u0026rsquo;t really understand the positions.\nIt was time to step back and think again. I realized then I wasn\u0026rsquo;t really trying to beat my brother, but I was trying to improve my game against opponents that played his same openings perfectly. Could I be more specific? Could I prepare against my brother\u0026rsquo;s weaknesses instead? Obviously this wouldn\u0026rsquo;t work against any player other than him, but that still satisfied my goal.\nThird attempt: engineering The problem to solve became: find positions out of the opening that my brother (PlayerX from now on, for simplicity) would likely reach and be at a disadvantage. Remember that neither of us is an expert of the game and at our level players don\u0026rsquo;t play very accurately. The only way to play against a good player would be to follow book moves accurately, because you at least know in advance there\u0026rsquo;s no move they can make to gain an advantage. The story is different when you play against a club player. You can take risks (i.e. be temporarily at a disadvantage) if you know that your opponent is unlikely to be able to find the correct response and so get into trouble.\nI also had a list of over 500 games my brother played on chess.com. Being a software engineer, it came natural to me to approach this as any other engineering problem.\nI started by downloading the games he played by using the chess.com APIs and split them between black and white games. I focused on the games he played as black, given that I felt I had better chances at guiding the game where I wanted if I played as white.\nimport json import requests def get_month_games(player, yyyy_mm): url = \u0026#39;https://api.chess.com/pub/player/{}/games/{}\u0026#39; r = requests.get(url.format(player, yyyy_mm)) if not r.ok: raise Exception(\u0026#39;get_month_games failed\u0026#39;) games = json.loads(r.content) # Format: {games: [{url, pgn}, ...]} return games[\u0026#39;games\u0026#39;] # ... import chess.pgn import io import json with open(\u0026#39;games.json\u0026#39;) as f: data = json.load(f) games = [] for game in data: pgn = io.StringIO(game) games.append(chess.pgn.read_game(pgn)) black_games = [g for g in games if g.headers[\u0026#34;Black\u0026#34;] == \u0026#34;playerx\u0026#34;] Then I formulated the problem in this way: \u0026ldquo;Given all the positions PlayerX has seen, what are the ones that he\u0026rsquo;s likely to reach out of the opening where he is at a substantial disadvantage?\u0026rdquo;.\nNow I had a well formulated problem and I was finally playing in a domain I was familiar with. I decided to do my analysis in Python and in particular to use a Jupyter notebook, because I didn\u0026rsquo;t really want to create a reusable tool, but only to explore the available data and find one solution. It turns out Python already has pretty amazing libraries to manipulate chess games: python-chess (moves generation, validation, visualization) and python stockfish (bindings to evaluate a chess position by using the famous Stockfish chess engine).\nI translated the problem into a graph problem in this way: A node is a particular chess position (described in FEN notation). An edge links two nodes where the destination is reachable from the source position by doing a valid move. There\u0026rsquo;s a special initial node that is common to all games: the initial position.\nI then constructed the graph of all games played by PlayerX as black, additionally annotating every edge with the number of times the particular move was played.\nclass GamesGraph(): def __init__(self): self.graph = igraph.Graph(directed=True) def add_move(self, start_fen, end_fen, uci): vs = self._ensure_vertex(start_fen) vt = self._ensure_vertex(end_fen) try: e = self.graph.es.find(_source=vs.index, _target=vt.index) e[\u0026#34;count\u0026#34;] += 1 except: e = self.graph.add_edge(vs, vt) e[\u0026#34;uci\u0026#34;] = uci e[\u0026#34;count\u0026#34;] = 1 @property def start_node(self): return self.graph.vs.find(chess.STARTING_FEN) def _ensure_vertex(self, fen): try: return self.graph.vs.find(fen) except: v = self.graph.add_vertex(name=fen) v[\u0026#34;fen\u0026#34;] = fen v[\u0026#34;turn\u0026#34;] = chess.Board(fen).turn return v What results is weighted directed graph (not a tree because a position can be reached with different sequences of moves) similar to this one (a synthetic one because the real one would be too big to fit here):\nHere the initial position is the squared node, the color indicates whether from that position it\u0026rsquo;s black or white\u0026rsquo;s turn.\nI also wanted an evaluation of each position in terms of advantage for white and to do so I used Stockfish. Given that the process of evaluating thousands of positions is somewhat time consuming, I decided to do that separately and create a JSON object mapping each unique FEN position to its Stockfish evaluation.\nfrom stockfish import Stockfish stock = Stockfish(parameters={\u0026#34;Threads\u0026#34;: 8}) stock.set_depth(20) stock.set_skill_level(20) def eval_pos(fen): stock.set_fen_position(fen) return stock.get_evaluation() # fens is a map between a FEN string and a node of the graph. for fen, node in graph.fens.items(): node.eva = eval_pos(fen) The evaluation is returned in centipawn advantage or \u0026ldquo;mate-in X moves\u0026rdquo;, where a positive number means advantage for white and negative is an advantage for black:\n{\u0026#34;type\u0026#34;:\u0026#34;cp\u0026#34;, \u0026#34;value\u0026#34;:12} # 12 centipawns advantage for white. {\u0026#34;type\u0026#34;:\u0026#34;mate\u0026#34;, \u0026#34;value\u0026#34;:-3} # Black has mate in three. 100 centipawns represent the advantage of having one more pawn than your opponent and 300 is a minor piece like a bishop. Note however that Stockfish assigns a value to pieces depending on their position, so it\u0026rsquo;s entirely possible to have an advantage of 1000 even if the pieces on the board are equal.\nI needed to map this evaluation into something more manageable, like a number between 0 and 1. To do so, I decided arbitrarily that an advantage of 300+ is mapped to 1.0 and a disadvantage of 300+ is mapped to 0. Additionally, any mate in X (even if X is 20) is 1 or 0.\n# Returns [-1;1] def rating(ev, fen): val = ev[\u0026#34;value\u0026#34;] if ev[\u0026#34;type\u0026#34;] == \u0026#34;cp\u0026#34;: # Clamp to -300, +300. Winning a piece is enough. val = max(-300, min(300, val)) return val / 300.0 # Mate in X: also max rating. if val \u0026gt; 0: return 1.0 if val \u0026lt; 0: return -1.0 # This is already mate, but is it for white or black? b = chess.Board(fen) return 1.0 if b.turn == chess.WHITE else -1.0 # Returns [0;1], where 0 is min, 1 is max advantage for black. def rating_black(ev, fen): return -rating(ev, fen) * 0.5 + 0.5 The information was then all there, I just needed to find nodes in the graph (i.e. positions) where black was at a disadvantage, along with the sequence of moves that was most likely to reach it. I needed to weigh the edges in such a way that it was possible to easily compute the probability to reach a certain position. My reasoning was as follow:\nAt every position, we can evaluate the probability of doing a certain move by dividing the number of times the corresponding edge was taken by the total number of moves done from that position. Every edge will now have a weight between 0 and 1, where the higher the number, the higher the probability the edge will be taken from that position. The probability of a certain path is then the product of the probability of all the traversed edges. To solve the problem with standard graph algorithms I needed to transform the weights at the edges in such a way that:\nThey represent a distance instead of a probability (i.e. the longer the distance, the lower the probability of the path). The distance between two nodes is the sum of the weights of the traversed edges (as opposed to the product of probabilities). This is actually easier to do than to explain. The actual formula is very simple:\ndistance(e) = -log(prob(e)) Or, in Python:\ndef compute_edges_weight(vertex): all_count = sum(map(lambda x: x[\u0026#34;count\u0026#34;], vertex.out_edges())) for edge in vertex.out_edges(): prob = edge[\u0026#34;count\u0026#34;] / all_count edge[\u0026#34;prob\u0026#34;] = prob edge[\u0026#34;weight\u0026#34;] = -math.log(prob) Taking the logarithm of the probability of an edge will give a negative number, because the probability is between 0 and 1. We don\u0026rsquo;t have to worry about the case of probability zero (which would shoot the logarithm to minus infinity), as every edge of the graph has been taken at least once. The lower the probability, the more negative the logarithm will be, so inverting its sign will make it satisfy our requirements, because:\nThe sum of logarithms is the same as the logarithm of the product of their arguments: log(a) + log(b) = log(a*b). The bigger the result, the lower the underlying probability. Equipped with this data, we can compute the shortest path between the initial node and all other nodes by using Dijkstra\u0026rsquo;s algorithm. The result is a mapping between every node and the shortest path to the initial position, which represents the sequence of moves most likely to land in that position.\nAt that point I arbitrarily chose a minimum advantage and sorted the paths by probability. The first few paths represented my best chances to gain an advantage out of the opening against PlayerX.\nTweaks What did I find? This was a position returned by the algorithm above (white to move):\nAs you can see the situation for black is pretty bad (+8.9 according to Stockfish), because g6, the last move for black, was a mistake. White will go on, take the e5 pawn and the rook. The game for black is pretty much over, as they scramble to save the knight, the h7 pawn and the bishop. Another result was this one (white to move):\nWhich is mate in one move (Scholar\u0026rsquo;s mate).\nThe problem here is that these were mistakes done several times by PlayerX only during his first matches and never repeated again. Early queen attacks are usually carried out by very inexperienced players and they are effective only against players at that level. PlayerX hasn\u0026rsquo;t fallen for that trap for a long time afterwards, because better opponents don\u0026rsquo;t play that kind of move! I knew that I couldn\u0026rsquo;t really use this opening, because PlayerX knew how to defend against it now and would not fall for it anymore.\nAnother problem was related to sequences of moves that happened only once, but coming from common positions. The probability of the final position was the same as the probability of the last common position, because every edge had a probability of 1.0 (given that no other possibilities have been played). In the example below (edges marked with their probabilities), you can follow the edges with 7 and 6 (the most common position at move 2), but then follow one of the edges with a 1. From that point on, all the subsequent moves will have been played only once (because only a single match reached that position) and so every step will have a probability of 1.0.\nAnd this is how the probabilities look like:\nThis is intuitively incorrect, as it\u0026rsquo;s improbable that the same exact sequence of moves will be played with absolute certainty. We don\u0026rsquo;t have enough games being played from those positions to know that.\nThe famous quote (from Brewster?) \u0026ldquo;In theory there is no difference between theory and practice, while in practice there is\u0026rdquo;, was true in this case as well, so I needed a few tweaks and manual inspection to find better candidate positions.\nTo correct the second problem I decided to put an upper bound to the probability of an edge, so long sequences of moves played only once will gradually lose probability.\ndef compute_edges_weight(vertex, prob_ceiling=0.9): all_count = sum(map(lambda x: x[\u0026#34;count\u0026#34;], vertex.out_edges())) for edge in vertex.out_edges(): # Certainty doesn\u0026#39;t exist... Let\u0026#39;s put a probability ceiling (default 90%). prob = min(edge[\u0026#34;count\u0026#34;] / all_count, prob_ceiling) edge[\u0026#34;prob\u0026#34;] = prob edge[\u0026#34;weight\u0026#34;] = -math.log(prob) For the first problem I just manually screened out bad suggestions. At the end of the day I only needed one or two good positions to work on.\nOne more tweak was related to the fact that I didn\u0026rsquo;t want white\u0026rsquo;s probabilities to affect the probability of the paths, because I was playing white and could decide which path to take. For that reason I set all whites probabilities to 1.0 (a zero weight). The end result was a graph like this one:\nPreparation The position I settled on studying was this one:\nAccording to Lichess this is an Alekhine defense (two pawn attack). In this position there\u0026rsquo;s only one good move for black (Nb6) and black is still at a slight disadvantage (+0.6 according to Stockfish). However, from that position PlayerX often plays Nf4, which is bad (+2.3). I created a study in Lichess and started looking at several variations (good moves and moves played by PlayerX). The end result was a tree of possibilities that I tried to memorize and understand. For example I needed to know what a move like d5 was threatening, why the move Nf4 was bad and prepare the best responses.\nI didn\u0026rsquo;t spend much time doing this because I got bored pretty quickly, but I did prepare a bit for the upcoming match.\nThe match As if I were predicting the future, in my match against PlayerX, we got into an Alekhine defense. Put under pressure he did end up blundering his knight at move 5. Turns out even players much better than you end up making one mistake after another when they are at a disadvantage. It\u0026rsquo;s easy to play accurately when you\u0026rsquo;re winning, but can you keep your cool when you are losing? At move 10 I was at a +7.1 advantage, pretty much impossible to lose, but I was also out of my preparation. Look at how cramped black\u0026rsquo;s position is and how my pieces are all pointing towards the enemy\u0026rsquo;s king:\nI started making a bunch of mistakes from that point on, but I nevertheless was able to keep a non trivial advantage until move 27:\nUnfortunately I was very low on time (it was a rapid 10 minutes game) and so I had to move quickly. I ended up messing up completely move 32 and 33, giving my half-dead opponent mate in one :/.\nHere\u0026rsquo;s the full match (blunders and all):1\nConclusion What did I learn from this endeavour? A few things, most of which seem obvious in retrospect:\nPreparing for a specific opponent can give a considerable edge in the opening. Players at lower levels aren\u0026rsquo;t good at punishing dubious moves from the opponent. Getting into tricky positions where only one response is correct are easy ways to gain an advantage. The opening isn\u0026rsquo;t everything. If you are bad at time management and tactics, it\u0026rsquo;s possible to lose completely winning positions. Chess games can be decided by one bad move. Studying the game is important and there\u0026rsquo;s no silver bullet if your opponent is much better than you, but narrowing the skill gap is possible with specific preparation. Applying software engineering principles to chess is fun. Doing it to have a chance at beating your brother is even more fun! I hope I\u0026rsquo;ll be able to do it one day :) You can find the code I used in my GitHub repo. Note that I did not include the data and the code is quite messy, but I hope it can be of some inspiration, especially for folks that are considering whether computer science might be for them or not. Look, you can solve \u0026ldquo;real world\u0026rdquo; problems with it, it\u0026rsquo;s not just moving bits around!\nThat\u0026rsquo;s all folks, I hope I\u0026rsquo;ll be able to win a match against my brother some day, but until then, I\u0026rsquo;ll keep trying\u0026hellip; my own way.\nFeedback on Hacker News.\nOriginal usernames are edited out because I didn\u0026rsquo;t ask my brother\u0026rsquo;s permission to post the match. I also still hope to try this trick one more time on him before he finds out :)\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://blog.mbrt.dev/posts/chess-eng/","summary":"This is the story of me trying to win a game of chess against my brother. A single freaking game. What\u0026rsquo;s so special about it? Am I good at chess? Not at all. Did I improve at my game in the process? Also no. Is it a story about \u0026ldquo;the journey rather than the destination\u0026rdquo;? Not really. Did I at least have fun in the process? Not so sure. This is the story of me trying to be unconventional at probably the most studied game in existence and using my software engineering background for something that probably doesn\u0026rsquo;t need it.","title":"Engineering a chess match against my brother"},{"content":"Over the last year, at work I had multiple chances to debug how containers work. Recently we had to solve some networking problems a customer had with Kubernetes, and I decided I wanted to know more. Once the problem was solved, I spent more time on investigating what is actually going on under the hood. After seeing the wonderful Eric Chiang and Laurent Bernaille talks, and reading through the very informative posts by Lizzie Dixon and Julia Evans (that I really really recommend), I got enough information about how a container is created and managed. I\u0026rsquo;m going to rip off and mix some stuff from their awesome posts in the first part of mine.\nWhat I missed in those talks was the networking part. How do containers talk to each other? In Bernaille\u0026rsquo;s talk there is some information, but I after seeing the video I was still not convinced completely. I was especially interested about how Calico works, and for that I could find very little information.\nTo answer this kind of questions I will try to create containers from scratch, by using just standard Linux commands. I will also setup the networking to make them happily communicate, again from scratch. I like this approach because it gets low level enough to demystify things that look very complicated, while it\u0026rsquo;s just a matter of spending some time to understand the basics.\nThis post is an extended version of a talk I gave internally at my company, trying to shed some light on the subject.\nPrerequisites for a good understanding are some basic networking and Linux concepts:\nthe OSI model, and in particular level 2 and 3; IP networking and the CIDR notation; NAT (Network Address Translation). I will link the advanced topics as the post unfolds.\nContainers from scratch Rise your hand if you ever tried the magic of Docker at least once. You pull an image from the Internet, you run it and you are projected inside another OS, with different libraries and applications installed, and all of that in no time. But how magic is a container after all? Is it composed by very complicated tools? Is it a sort of virtual machine? In the first part of this post I\u0026rsquo;m going to create a container from scratch, by using only a Linux shell and standard Linux commands, to try to answer these questions.\nPrepare the image When you do a docker pull you are downloading a container image from the Internet. This image at its core is basically just a root filesystem. You can safely ignore the fact that it\u0026rsquo;s composed by multiple stacked layers, because the end result is just a root filesystem.\nSo we can try to make our own, and for this post I decided to go with Alpine Linux, because it\u0026rsquo;s small and it\u0026rsquo;s different from my distribution.1 Needless to say that for this to work you have to be running on Linux and with a fairly recent Kernel. I haven\u0026rsquo;t checked the specific requirements, but if you updated your system in the last 5 years,2 you\u0026rsquo;re probably good to go.\nBe powerful, be root. You\u0026rsquo;ll save yourself a lot of sudo invocations and annoying \u0026ldquo;permission denied\u0026rdquo; messages:\nsudo su Download the mini root filesystem from the Alpine website and put it somewhere. Then extract it:\nmkdir rootfs cd rootfs tar xf ../alpine-minirootfs-3.6.2-x86_64.tar.gz if you look there, you\u0026rsquo;ll see the root filesystem:\n[root@mike-dell rootfs]# ls -l total 64 drwxr-xr-x 2 root root 4096 Aug 13 16:22 bin drwxr-xr-x 4 root root 4096 Jun 17 11:46 dev drwxr-xr-x 15 root root 4096 Aug 13 16:26 etc drwxr-xr-x 2 root root 4096 Jun 17 11:46 home drwxr-xr-x 5 root root 4096 Aug 13 16:22 lib drwxr-xr-x 5 root root 4096 Jun 17 11:46 media drwxr-xr-x 2 root root 4096 Jun 17 11:46 mnt dr-xr-xr-x 2 root root 4096 Jun 17 11:46 proc drwx------ 2 root root 4096 Aug 13 16:08 root drwxr-xr-x 2 root root 4096 Jun 17 11:46 run drwxr-xr-x 2 root root 4096 Jun 17 11:46 sbin drwxr-xr-x 2 root root 4096 Jun 17 11:46 srv drwxr-xr-x 2 root root 4096 Jun 17 11:46 sys drwxrwxrwt 2 root root 4096 Jun 17 11:46 tmp drwxr-xr-x 7 root root 4096 Jun 17 11:46 usr drwxr-xr-x 13 root root 4096 Aug 13 16:22 var chroot Now let\u0026rsquo;s try to chroot there. In this way we create a process and change its root directory to the one we just created:\nchroot rootfs /bin/ash export PATH=/bin:/usr/bin:/sbin This will execute a shell inside the chroot environment. Side note: exporting a new $PATH (the second command) is wise, because otherwise you\u0026rsquo;d be carrying your host $PATH in the chroot, and this might not be correct there. So where are we exactly?\n/ # cat /etc/os-release NAME=\u0026#34;Alpine Linux\u0026#34; ID=alpine VERSION_ID=3.6.2 PRETTY_NAME=\u0026#34;Alpine Linux v3.6\u0026#34; HOME_URL=\u0026#34;http://alpinelinux.org\u0026#34; BUG_REPORT_URL=\u0026#34;http://bugs.alpinelinux.org\u0026#34; Yes, in Alpine Linux. And you can\u0026rsquo;t reach your host files anymore, because your root directory is now the one we just chroot-ed into.\nLet\u0026rsquo;s now install some useful packages. They\u0026rsquo;ll come in handy for later:\napk add --no-cache python findmnt curl libcap bind-tools Another thing we have to fix now is the /proc filesystem. If you look there you\u0026rsquo;ll see that it\u0026rsquo;s empty so any utility like ps won\u0026rsquo;t work:\nmount -t proc proc /proc Now a question for you: Is this actually a container?\nSort-of, but the isolation is pretty poor. Take a look at ps aux from the \u0026ldquo;container\u0026rdquo;:\n/ # ps aux PID USER TIME COMMAND 1 root 0:03 {systemd} /sbin/init 2 root 0:00 [kthreadd] 3 root 0:00 [kworker/0:0] 4 root 0:00 [kworker/0:0H] 6 root 0:00 [mm_percpu_wq] 7 root 0:00 [ksoftirqd/0] 8 root 0:01 [rcu_preempt] 9 root 0:00 [rcu_sched] 10 root 0:00 [rcu_bh] 11 root 0:00 [migration/0] 12 root 0:00 [watchdog/0] 13 root 0:00 [cpuhp/0] 14 root 0:00 [cpuhp/1] 15 root 0:00 [watchdog/1] 16 root 0:00 [migration/1] 17 root 0:00 [ksoftirqd/1] 19 root 0:00 [kworker/1:0H] ... 2816 1170 0:00 top oops\u0026hellip; I can see all the processes of my host from here. An I can actually kill them:\nkillall top Not only that. Look at the network:\n/ # ip link 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 3: wlan0: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc fq_codel state UP qlen 1000 link/ether 40:49:0f:fe:c3:05 brd ff:ff:ff:ff:ff:ff You can see my WiFi card for example. I could change the IP, take it down, etc. Not nice. The answer is then NO, this is not a container, because it\u0026rsquo;s not isolated enough. This is just a process in a different root filesystem.\nNamespaces Linux has namespaces to the rescue. As man 7 namespaces says:\nA namespace wraps a global system resource in an abstraction that makes it appear to the processes within the namespace that they have their own isolated instance of the global resource. Changes to the global resource are visible to other processes that are members of the namespace, but are invisible to other processes. One use of namespaces is to implement containers.\nor in other words: we take a resource like the list of processes in the machine, we make an isolated copy of it, give it to our process and make sure that any change there is not reflected to the root process list. This is the PID namespace. Is it hard to set up? Judge by yourself:\nunshare -p -f chroot rootfs /usr/bin/env -i \\ HOME=/root \\ PATH=/bin:/usr/bin:/sbin:/usr/sbin \\ /bin/ash -l With this command from the host, we create a new process (the chroot we used before) but we put it in a new PID namespace by prepending the unshare -p invocation. This command is nothing fancy, just a handy wrapper around the unshare Linux system call. The env command executed after the chroot makes sure that the environment is correctly filled, avoiding us to repeat the export command every time.\nLet\u0026rsquo;s take a look at the list of processes now, after we mount /proc again:\n/ # mount -t proc proc /proc / # ps PID USER TIME COMMAND 1 root 0:00 /bin/ash 5 root 0:00 ps Oh yes. Now our shell is actually PID 1. How weird is that? And yes, you won\u0026rsquo;t be able to kill any host process.\nFrom the host you can instead see the containerized process:\n[root@mike-dell micheleb]# ps aux |grep /ash root 8552 0.0 0.0 1540 952 pts/3 S+ 20:06 0:00 /bin/ash and kill it if you want to.\nThe PID is not the only namespace you can create, as you can imagine. The network for example is still the host one:\n/bin # ip link 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 3: wlan0: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc fq_codel state UP qlen 1000 link/ether 40:49:0f:fe:c3:05 brd ff:ff:ff:ff:ff:ff Let\u0026rsquo;s isolate it then. It\u0026rsquo;s just a matter of adding some flags to unshare:\nunshare -pmn -f chroot rootfs /usr/bin/env -i \\ HOME=/root \\ PATH=/bin:/usr/bin:/sbin:/usr/sbin \\ /bin/ash -l here we are isolating the PID, mount and network namespaces, all at once. And here is the result:\n# / ip addr 1: lo: \u0026lt;LOOPBACK\u0026gt; mtu 65536 qdisc noop state DOWN qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 # / ping -c1 8.8.8.8 PING 8.8.8.8 (8.8.8.8): 56 data bytes ping: sendto: Network unreachable Pretty isolated I would say. Topic of the next section will be how to open a little hole in this isolation and get some containers to communicate somehow.\nBefore to move on I\u0026rsquo;d like to put a little disclaimer here. Even though I\u0026rsquo;m done with this section, it doesn\u0026rsquo;t mean that with an unshare command you get a fully secure container. Don\u0026rsquo;t go to your boss and say that you want to toss Docker and use shell scripts because it\u0026rsquo;s the same thing.\nWhat our container is still missing is, for example, resource isolation. We could crash the machine by creating a lot of processes, or slow it down by allocating a lot of memory. For this you need to use cgroups.3 Then there\u0026rsquo;s the problem you are still root inside the container, You are limited but you are still pretty powerful. You could for example change the system clock, reboot the machine, and other scary things. To control them you\u0026rsquo;d need to drop some capabilities.4 I won\u0026rsquo;t dig into these concepts in this post, because they don\u0026rsquo;t affect the networking. All of that involves just simple Linux system calls and some magic in the /proc and /sys/fs/cgroup/ filesystems.\nI point you though to the excellent resources I linked at the beginning, especially Eric Chiang and Lizzie Dixon, if you are more curious. I could also write another post on that in the future.\nI hope I nevertheless convinced you that a container is nothing more than a highly configured Linux process. No virtualization and no crazy stuff is going on here. You could create a container today with just a plain Linux machine, by calling a bunch of Linux syscalls.\nNetworking from scratch Goal of this section will be to break the isolation we put our container in, and make it communicate with:\na container in the same host; a container in another host; the Internet. I\u0026rsquo;m running this experiment in a three nodes cluster. The nodes communicate through a private network under 10.141/16. The head node has two network interfaces, so it\u0026rsquo;s able to communicate with both the external and the internal network. The other two nodes have only one network interface and they can reach the external network by using the head node as gateway. The following schema should clarify the situation:\nCommunicate within the host Right now our container is completely isolated. Let\u0026rsquo;s try to at least ping the same host:\n/# ping 10.141.0.1 PING 10.141.0.1 (10.141.0.1): 56 data bytes ping: sendto: Network unreachable It\u0026rsquo;s not working, so the network is isolated. No matter what you do you won\u0026rsquo;t be able to reach the outside, because the only interface you have there is the loopback (and it\u0026rsquo;s also down).\n/# ip link 1: lo: \u0026lt;LOOPBACK\u0026gt; mtu 65536 qdisc noop state DOWN qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 If you create another container on the same host, you can imagine they\u0026rsquo;re not going to be able to communicate either.\nHow do we solve this problem? We use a veth pair, which stands for Virtual Ethernet pair. As the name suggests, a veth pair is a pair of virtual interfaces, that act as an Ethernet cable. Whatever comes into one end, goes to the other. Sounds useful? Yes, because we can move one end of the pair inside the container, and keep the other end in the host. So we are basically piercing a hole in the container to slide our little virtual wire in.\nIn another shell, same host, let\u0026rsquo;s setup a $CPID variable to help us remember what is the container PID:5\nCPID=$(ps -C ash -o pid= | tr -d \u0026#39; \u0026#39;) Let\u0026rsquo;s create the veth pair with iproute,6 move one end into the container and bring the host end up:\nip link add veth0 type veth peer name veth1 ip link set veth1 netns $CPID ip link set dev veth0 up If you take a look at the interfaces in the container now, you\u0026rsquo;ll see something like:\n/# ip l 1: lo: \u0026lt;LOOPBACK\u0026gt; mtu 65536 qdisc noop state DOWN qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 3: veth1@if4: \u0026lt;BROADCAST,MULTICAST,M-DOWN\u0026gt; mtu 1500 qdisc noop state DOWN qlen 1000 link/ether 8e:7f:62:52:76:71 brd ff:ff:ff:ff:ff:ff Cool! Everything is down, but we have a new interface. Let\u0026rsquo;s also rename it to something less scary, like eth0. You\u0026rsquo;ll feel more home in the container:\nip link set dev veth1 name eth0 address 8e:7f:62:52:76:71 where the address used is the MAC address shown by ip link, or ip addr show dev veth1.7\nNow let\u0026rsquo;s step back for a second. We have a container with this \u0026ldquo;cable\u0026rdquo; pointing out. What kind of IP should we give to the container? What kind of connectivity do we want to provide? The way we are going to set it up is the default Docker way: bridge networking. Containers on the same host live on the same network, but different than the host one. This means that we have to setup a virtual network where containers are able to talk to each other at level 2. This also means that we won\u0026rsquo;t consume any physical IP address from the host network.\nI choose the 172.19.35/24 subnet for the containers, since it doesn\u0026rsquo;t conflict with the cluster private network (10.141/16).8 This means that I have space for 2^8 - 2 = 30 containers in this machine.9\nNow let\u0026rsquo;s give the container an IP and bring it up, along with the loopback interface:\nip addr add dev eth0 172.19.35.2/24 ip link set eth0 up ip link set lo up And this is the current situation:\nNow we want do to the very same thing with another container. So let\u0026rsquo;s create it from the same root filesystem:\nunshare -pmn -f chroot rootfs /usr/bin/env -i \\ HOME=/root \\ PATH=/bin:/usr/bin:/sbin:/usr/sbin \\ /bin/ash -l mount -t proc proc /proc Then in the host we setup another $CPID2 variable with the PID of this new container,10 and then create another veth pair:\nip link add veth2 type veth peer name veth3 ip link set veth3 netns $CPID2 ip link set dev veth2 up Then rename the interface in the container, give it an IP and bring it up as before:\nip link set dev lo up MAC=$(ip addr show dev veth3 | grep \u0026#39;link/ether\u0026#39; | tr -s \u0026#39; \u0026#39; | cut -d\u0026#39; \u0026#39; -f3) ip link set dev veth3 name eth0 address $MAC ip addr add dev eth0 172.19.35.3/24 ip link set eth0 up Note that I\u0026rsquo;m using another IP address in the 172.19.35/24 subnet. This is the situation right now:\nWhat we need to do here is try to link those two veth pairs together, in a way that they can communicate at layer 2. Something like… a bridge! It will take care of linking together the two network segments. It works at level 2 like a switch (so it basically \u0026ldquo;talks Ethernet\u0026rdquo;), by \u0026ldquo;enslaving\u0026rdquo; existing interfaces. You add a bunch of interfaces into a bridge, and they will be communicating with each other thanks to the bridge.\nLet\u0026rsquo;s create the bridge and put the two veth interfaces in it:\nip link add br0 type bridge ip link set veth0 master br0 ip link set veth2 master br0 Now let\u0026rsquo;s give the bridge an IP and bring it up:\nip addr add dev br0 172.19.35.1/24 ip link set br0 up Now we have this topology in place:\nAs you can see, now the containers can ping each other:\n/ # ping 172.19.35.3 -c1 PING 172.19.35.3 (172.19.35.3): 56 data bytes 64 bytes from 172.19.35.3: seq=0 ttl=64 time=0.046 ms --- 172.19.35.3 ping statistics --- 1 packets transmitted, 1 packets received, 0% packet loss round-trip min/avg/max = 0.046/0.046/0.046 ms Let\u0026rsquo;s check the ARP table11 on the first container:\n/ # ip neigh 172.19.35.3 dev eth0 lladdr c6:b3:e3:1d:97:7b used 40/35/10 probes 1 STALE So this means that these two containers are on the same network, and can talk to each other at level 2. And here is indeed the ARP request going through:\n[root@node001 ~]# tcpdump -i any host 172.19.35.3 22:55:37.858611 ARP, Request who-has 172.19.35.3 tell 172.19.35.2, length 28 22:55:37.858639 ARP, Reply 172.19.35.3 is-at c6:b3:e3:1d:97:7b (oui Unknown), length 28 Reach the internet If you try to reach the external network, or even the host IP, you\u0026rsquo;ll see that it\u0026rsquo;s still not working. That\u0026rsquo;s because to reach a different network you need some kind of level 3 communication. The way Docker sets it up by default is with natting.12 In this way, the 172.19.35/24 network will be invisible outside the host and mapped automatically into the host IP address, that in my case is 10.141.0.1 (which by the way is still a private IP, and will be natted by the head node into the public IP).\nLet\u0026rsquo;s first enable IP forwarding, to allow the host to perform routing operations:\necho 1 \u0026gt; /proc/sys/net/ipv4/ip_forward Then insert a NAT rule (also called IP masquerade) in the external interface:\niptables -t nat -A POSTROUTING -o eth0 -j MASQUERADE Then you need to set the default route in the container:\nip route add default via 172.19.35.1 In this way any packet with a destination on a different network will be sent through the gateway, which is the bridge. From there it will be natted by eth0, our physical interface, and then sent through the cluster fabric by using the physical IP as source.\nThis is now the situation:\nIf I ping Google\u0026rsquo;s DNS from the container, I see this from the host:\n[root@node001 ~]# tcpdump -i any host 8.8.8.8 -n 23:27:51.234333 IP 172.19.35.2 \u0026gt; 8.8.8.8: ICMP echo request, id 13824, seq 0, length 64 23:27:51.234360 IP 10.141.0.1 \u0026gt; 8.8.8.8: ICMP echo request, id 13824, seq 0, length 64 23:27:51.242230 IP 8.8.8.8 \u0026gt; 10.141.0.1: ICMP echo reply, id 13824, seq 0, length 64 23:27:51.242251 IP 8.8.8.8 \u0026gt; 172.19.35.2: ICMP echo reply, id 13824, seq 0, length 64 As you can see the packet comes from the container, is translated into the host IP (10.141.0.1) and then when it comes back, the destination is replaced with the container IP (172.19.35.2).\nThis is what I see from the head node, instead:\n[root@head ~]# tcpdump -i any host 8.8.8.8 -n 23:25:20.209922 IP 10.141.0.1 \u0026gt; 8.8.8.8: ICMP echo request, id 13568, seq 0, length 64 23:25:20.209943 IP 192.168.200.172 \u0026gt; 8.8.8.8: ICMP echo request, id 13568, seq 0, length 64 23:25:20.217286 IP 8.8.8.8 \u0026gt; 192.168.200.172: ICMP echo reply, id 13568, seq 0, length 64 23:25:20.217310 IP 8.8.8.8 \u0026gt; 10.141.0.1: ICMP echo reply, id 13568, seq 0, length 64 As you can see the packet comes from the node, it\u0026rsquo;s forwarded through the head node public IP (192.168.200.172), and then comes back the other way around. NAT is also working here.\nReach a remote container Now from a container we are able to communicate with both another local container and with the externa network. The next step is to reach a container in another node, in the same physical private network (the 10.141/16 network the nodes sit in).\nThis is basically the plan:\nThe two nodes communicate through the physical private network 10.141/16. We want to assign a subnet to each node, so each will be able to host some containers. We have already assigned the 172.19.35/24 network to the first host. We can then assign another to the second, for example 172.19.36/24. I could have chosen any other IP range that doesn\u0026rsquo;t conflict with the existing networks, but this one is especially handy, because both of them are part of a bigger 172.19/16 network. We can think of it as the containers\u0026rsquo; network, in which every host gets a slice (a /24 subnet). This means that we can assign 24 - 16 = 8 bits to different hosts, so maximum 255 nodes. Of course you can use different network sizes to accomodate your needs, but that\u0026rsquo;s the way we are going to set it up here. NAT has been already setup in the first host, so we are going to do the same for the second one, and then add routing rules (layer 3) between the two hosts.\nLet\u0026rsquo;s go real quick over the second host, create a container, setup the networking there as we did for the first host:\nunshare -pmn -f chroot rootfs /usr/bin/env -i \\ HOME=/root \\ PATH=/bin:/usr/bin:/sbin:/usr/sbin \\ /bin/ash -l then in the host:\nCPID=$(ps -C ash -o pid= | tr -d \u0026#39; \u0026#39;) ip link add veth0 type veth peer name veth1 ip link set veth1 netns $CPID ip link set dev veth0 up ip link add br0 type bridge ip link set veth0 master br0 ip addr add dev br0 172.19.36.1/24 ip link set br0 up echo 1 \u0026gt; /proc/sys/net/ipv4/ip_forward iptables -t nat -A POSTROUTING -o eth0 -j MASQUERADE Note that I used the 172.19.36.1/24 IP for the bridge. Then in the container:\nip link set dev lo up MAC=$(ip addr show dev veth1 | grep \u0026#39;link/ether\u0026#39; | tr -s \u0026#39; \u0026#39; | cut -d\u0026#39; \u0026#39; -f3) ip link set dev veth1 name eth0 address $MAC ip addr add dev eth0 172.19.36.2/24 ip link set eth0 up ip route add default via 172.19.36.1 and again I use 172.19.36/24 here. Now the container is able to talk to the Internet, as the other one. But, is the first container able to reach this new container?\nTry to think about it.\nThen try to do it. No, it doesn\u0026rsquo;t work, but why? The answer is in the routing table of the first host:\n[root@node001 ~]# ip r default via 10.141.255.254 dev eth0 10.141.0.0/16 dev eth0 proto kernel scope link src 10.141.0.1 172.19.35.0/24 dev br0 proto kernel scope link src 172.19.35.1 There is a default gateway pointing to the head node, and two \u0026ldquo;scope link\u0026rdquo; ranges, for networks reachable at level 2 (unsurprisingly there are the 10.141/16 physical network, and the 172.19.35/24 network for the local containers). As you can see there\u0026rsquo;s no rule for 172.19.36/24. This means the packet will go through the default gateway, and from there it will try to go outside, because the head node doesn\u0026rsquo;t know anything about this IP either.\nWhat we should do is add a routing rule to the node table, telling that any packet for 172.19.36/24 should be forwarded to the second host, listening at 10.141.0.2:\nip route add 172.19.36.0/24 via 10.141.0.2 src 10.141.0.1 The same goes for the other host, but in reverse:\nip route add 172.19.35.0/24 via 10.141.0.1 src 10.141.0.2 And now, both containers are able to talk to each other. If you want to show something fancy, you could run NGINX in one container, and curl the beautiful default page from the other.\nHooray!\nBonus: Calico What I showed in the last section is basically how Docker sets up its bridge networking. The routing rules to make the containers see each other come from me. What Docker Swarm and other networking solutions for Docker use instead is usually overlay networking, like VXLAN. VXLAN encapsulate layer 2 Ethernet frames within layer 3 UDP packets. This provides layer 2 visibility to containers across hosts. I didn\u0026rsquo;t show this approach because the routing rules were simpler, and also because I prefer the Calico approach, that I will present in this section.\nSome of you may already know Kubernetes. It\u0026rsquo;s the most popular (any my favorite) container orchestrator. What it basically does is providing declarative APIs to manage containers. Restarts upon failures, replicas' scaling, upgrading, ingress, and many other things can be managed automatically by Kubernetes. For all this magic to happen, Kubernetes imposes some restrictions on the underlying infrastructure. Here is the section about the networking model:\nall containers can communicate with all other containers without NAT all nodes can communicate with all containers (and vice-versa) without NAT the IP that a container sees itself as is the same IP that others see it as. As the documentation says:\nCoordinating ports across multiple developers is very difficult to do at scale and exposes users to cluster-level issues outside of their control. Dynamic port allocation brings a lot of complications to the system - every application has to take ports as flags, the API servers have to know how to insert dynamic port numbers into configuration blocks, services have to know how to find each other, etc. Rather than deal with this, Kubernetes takes a different approach.\nThe solution we used in the previous section does not satisfy these requirements. In our case the source IP is rewritten by NAT, so the destination container sees only the host IP.\nThere are a number of projects that satisfy the Kubernetes requirements, and among them I really like Project Calico, so I\u0026rsquo;m going to reproduce its setup here, again the hard way, just Linux commands.\nThe Calico\u0026rsquo;s solution is to use layer 3 networking all the way up to the containers. No Docker bridges, no NAT, just pure routing rules and iptables. Interestingly enough, the way Calico distributes the routing rules is through BGP,13 which is the same way the Internet works.\nThe end result we\u0026rsquo;re going to aim at is this:\nLooks familiar? Yes, it\u0026rsquo;s almost the same as the one I used in the previous section. We\u0026rsquo;re going to use the same IP ranges: the host networking under 10.141/16, and we\u0026rsquo;re going to setup a 172.19/16 network for the containers. As before, every host gets a /24 subnet. The difference is in the way the packets are routed. With Calico everything goes at layer 3, so on the wire you\u0026rsquo;ll see packets coming from a 172.19/16 address and going to a 172.19/16 address because, as I said before, no natting or overlays are used.\nSetup the host network Without further ado, let\u0026rsquo;s create our container on the first host:\nunshare -pmn -f chroot rootfs /usr/bin/env -i \\ HOME=/root \\ PATH=/bin:/usr/bin:/sbin:/usr/sbin \\ /bin/ash -l Then, let\u0026rsquo;s create our veth pair, and move one end into the container:\nCPID=$(ps -C ash -o pid= | tr -d \u0026#39; \u0026#39;) ip link add veth0 type veth peer name veth1 ip link set veth1 netns $CPID ip link set dev veth0 up Let\u0026rsquo;s now give the container an IP address:\nip link set dev lo up MAC=$(ip addr show dev veth1 | grep \u0026#39;link/ether\u0026#39; | tr -s \u0026#39; \u0026#39; | cut -d\u0026#39; \u0026#39; -f3) ip link set dev veth1 name eth0 address $MAC ip addr add dev eth0 172.19.35.2/32 ip link set eth0 up Have you noted anything strange? I\u0026rsquo;m using a /32 address for the container IP. This means that whenever I send a packet, even for a container living on the same host, it will need to go through level 3. This allows to get rid of the bridge, and also makes sure that the container doesn\u0026rsquo;t try (and fail) to reach another at level 2, by sending useless ARP requests.\nNow on the host we need to enable ARP proxy for the veth interface.\necho 1 \u0026gt; /proc/sys/net/ipv4/conf/veth0/rp_filter echo 1 \u0026gt; /proc/sys/net/ipv4/conf/veth0/route_localnet echo 1 \u0026gt;/proc/sys/net/ipv4/conf/veth0/proxy_arp echo 0 \u0026gt;/proc/sys/net/ipv4/neigh/veth0/proxy_delay echo 1 \u0026gt;/proc/sys/net/ipv4/conf/veth0/forwarding What this does is basically replying to ARP requests with its own MAC address. In this way, when the container looks for the link local address, veth0 will say: \u0026ldquo;it\u0026rsquo;s me!\u0026rdquo;, replying with it\u0026rsquo;s own MAC address, and the packet will be sent there at layer 2.14\nWe also need to enable IP forwarding on the host\u0026rsquo;s physical interface, to allow routing:\necho 1 \u0026gt;/proc/sys/net/ipv4/conf/eth0/forwarding And inside the container we have to add a couple of routing rules:\nip r add 169.254.1.1 dev eth0 scope link ip r add default via 169.254.1.1 dev eth0 Here we use a local link address, so we don\u0026rsquo;t have to manage the IP of the other pair of the veth. We can assign the same address to all the veths, since the address is valid only within the link, so no routing will be performed by the kernel. We\u0026rsquo;ve also added a default route, that says to use that IP for any address outside of the local range. But since our local range is a /32, no IP is local. So, what we are saying to the kernel in the end is: \u0026ldquo;any time we want to reach something outside the container, just put it on the eth0 link\u0026rdquo;. It seems convoluted, but the idea behind it is quite simple.\nLast bit missing on the host is the rule to reach the container from the host:\nip r add 172.19.35.2 dev veth0 scope link With this we\u0026rsquo;re saying that, to reach the container, the packet has to go through the veth0 interface.\nNow, from the container we\u0026rsquo;re able to ping the host:\nnode001:/# ping 10.141.0.1 -c1 PING 10.141.0.1 (10.141.0.1): 56 data bytes 64 bytes from 10.141.0.1: seq=0 ttl=64 time=0.077 ms --- 10.141.0.1 ping statistics --- 1 packets transmitted, 1 packets received, 0% packet loss round-trip min/avg/max = 0.077/0.077/0.077 ms And this is the traffic passing:\n[root@node001 ~]# tcpdump -i any host 172.19.35.2 -n 16:25:10.439980 IP 172.19.35.2 \u0026gt; 10.141.0.1: ICMP echo request, id 6144, seq 0, length 64 16:25:10.440014 IP 10.141.0.1 \u0026gt; 172.19.35.2: ICMP echo reply, id 6144, seq 0, length 64 ARP goes back and forth to determine the physical address of the local link IP:\n[root@node001 ~]# tcpdump -i any host 172.19.35.2 16:25:15.453847 ARP, Request who-has 169.254.1.1 tell 172.19.35.2, length 28 16:25:15.453882 ARP, Reply 169.254.1.1 is-at f6:5c:53:b4:f8:03 (oui Unknown), length 28 and if you look at the ARP table you\u0026rsquo;ll see the cached reply:\nnode001:/# ip neigh 169.254.1.1 dev eth0 lladdr f6:5c:53:b4:f8:03 ref 1 used 2/2/2 probes 4 REACHABLE The 169.254.1.1 IP is the only one reachable at level 2 from the container, as expected. The MAC address corresponds to the other end of the veth pair, as you can see from the host:\n[root@node001 ~]# ip l show dev veth0 5: veth0@if4: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc pfifo_fast state UP mode DEFAULT qlen 1000 link/ether f6:5c:53:b4:f8:03 brd ff:ff:ff:ff:ff:ff link-netnsid 0 And this is the current situation:\nAnother detail is the blackhole route, to drop packets coming for unexisting containers:\nip r add blackhole 172.19.35.0/24 In this way any packet sent to the host subnet to an IP not present in the host will be dropped. Packets for exising containers still work, because their routing rules are more specific, so they take precedence:\n[root@node001 ~]# ip r default via 10.141.255.254 dev eth0 10.141.0.0/16 dev eth0 proto kernel scope link src 10.141.0.1 169.254.0.0/16 dev eth0 scope link metric 1002 blackhole 172.19.35.0/24 172.19.35.2 dev veth0 scope link In this case, if you send a packet to 172.19.35.2, it will go to veth0. If you instead try to reach 172.19.35.3, it will go to the blackhole and dropped, instead of going to the default gateway.\nReach a remote container To reach a container running on another host, you have to replicate the setup done for this host. You have to assign to that node another /24 subnet from the container network, and use one IP from that subnet to create a container (I used the 172.19.36/24 subnet, the same as Part 2).15\nThen you need to add the routing rules to direct the traffic to the right host. From the first host:\nip route add 172.19.36.0/24 via 10.141.0.2 src 10.141.0.1 and similarly from the second host:\nip route add 172.19.35.0/24 via 10.141.0.1 src 10.141.0.2 Done. Now the containers can reach each other. If you look at the traffic, you\u0026rsquo;ll see that the source and destination IPs are preserved, and not NATted, satisfying the Kubernetes\u0026rsquo; requirements:\n[root@node001 ~]# tcpdump -i any host 172.19.35.2 20:08:02.154031 IP 172.19.35.2 \u0026gt; 172.19.36.2: ICMP echo request, id 17152, seq 0, length 64 20:08:02.154045 IP 172.19.35.2 \u0026gt; 172.19.36.2: ICMP echo request, id 17152, seq 0, length 64 20:08:02.155088 IP 172.19.36.2 \u0026gt; 172.19.35.2: ICMP echo reply, id 17152, seq 0, length 64 20:08:02.155098 IP 172.19.36.2 \u0026gt; 172.19.35.2: ICMP echo reply, id 17152, seq 0, length 64 Success!\nReach the Internet If you are lucky you are able to reach the external network already. This all depends on how NAT is setup in your cluster. A proper setup should allow only packets coming from the physical network to escape.\nFrom my head node (that is also the default gateway of the other nodes), I see:\n[root@mbrt-c-08-13-t-c7u2 ~]# iptables -L -t nat Chain PREROUTING (policy ACCEPT) target prot opt source destination Chain INPUT (policy ACCEPT) target prot opt source destination Chain OUTPUT (policy ACCEPT) target prot opt source destination Chain POSTROUTING (policy ACCEPT) target prot opt source destination MASQUERADE all -- 10.141.0.0/16 anywhere This is precisely my case. Only packets coming from the 10.141/16 network, will be natted. To perform NAT also for packets coming from the containers network, I have to add another rule:\niptables -t nat -A POSTROUTING -o eth1 -j MASQUERADE -s 172.19.0.0/16 Looking this way in the table:\nMASQUERADE all -- 172.19.0.0/16 anywhere Then we need a routing rule in the head node, telling it where it can find the 172.19.35/24 subnet:\nip route add 172.19.35.0/24 via 10.141.0.1 src 10.141.255.254 And now, you can finally ping the outside network from the container!\nMissing pieces Among the feature that I haven\u0026rsquo;t discussed, Calico has a really nice distributed firewall, applied through iptables, but I left it out of scope from this post.\nBonus: Debug container networking In this section I would like to digress a bit and talk about debugging. I hope it\u0026rsquo;s clear at this point that containers aren\u0026rsquo;t magical, and networking isn\u0026rsquo;t magical either. This means that for debugging you can use all the regular tools Linux provides. You don\u0026rsquo;t need to rely on Docker or Calico to provide anything on their end, and even if they would, how do you debug them when they are broken? In the previous section I used ping, iproute and tcpdump, but what happens if your Docker image does not contain these tools?\nnode001:/# ip r /bin/ash: ip: not found This happens many times, and even worse if your Docker image looks like this:\nFROM scratch ADD main / CMD [\u0026#34;/main\u0026#34;] You don\u0026rsquo;t even have a console there. What do you do?\nEnter the nsenter magical world There is a very simple trick you should probably remember: nsenter. This command enters one or more namespaces from the host. You can enter all of them and in that case you would have another console open on the container (similar to the docker exec command):\nnsenter --pid=/proc/$CPID/ns/pid \\ --net=/proc/$CPID/ns/net \\ --mount=/proc/$CPID/ns/mnt \\ /bin/bash and look, we see the same processes as the container do:\n[root@node001 rootfs]# mount -t proc proc /proc [root@node001 rootfs]# ps aux USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND root 1 0.0 0.0 1540 548 pts/0 S+ 16:19 0:00 /bin/ash -l root 97 0.0 0.2 116144 2908 pts/1 S 20:25 0:00 /bin/bash root 127 0.0 0.1 139492 1620 pts/1 R+ 20:28 0:00 ps aux What\u0026rsquo;s most important for our purposes is accessing the network namespace though:\nnsenter --net=/proc/$CPID/ns/net /bin/bash this way you have the same network as the container, but no other restrictions. In particular you have access to the host filesystem:\n[root@node001 ~]# cat /etc/os-release NAME=\u0026#34;CentOS Linux\u0026#34; VERSION=\u0026#34;7 (Core)\u0026#34; ID=\u0026#34;centos\u0026#34; ID_LIKE=\u0026#34;rhel fedora\u0026#34; VERSION_ID=\u0026#34;7\u0026#34; PRETTY_NAME=\u0026#34;CentOS Linux 7 (Core)\u0026#34; ... and all your favorite tools available. But the network you see is the container one:\n[root@node001 ~]# ip r default via 169.254.1.1 dev eth0 169.254.1.1 dev eth0 scope link This, of course works with Docker too. Once you have the PID of your container, you can nsenter it:\n[root@node001 ~]# docker inspect --format \u0026#39;{{.State.Pid}}\u0026#39; my-awesome-container 24028 [root@node001 ~]# nsenter --net=/proc/24028/ns/net /bin/bash So, please, don\u0026rsquo;t install debugging tools in your Docker images anymore. It\u0026rsquo;s not really necessary.\nConcluding remarks With this long post I tried to reproduce two different solutions for container networking, with nothing more than Linux commands. Docker, Calico, Flannel and the others are all nice tools, but they aren\u0026rsquo;t magical. They build on top of standard Linux functionality, and trying to reproduce their behavior helped me (and I hope you too) to understand them better.\nKeep in mind that this is not a complete guide. There are many more interesting topics, like network policies and security in general, then a universe of different solutions, like overlay networks, Ipvlan, macvlan, MacVTap, IPsec, and I don\u0026rsquo;t know how many others. For containers in general there are many other things you want to isolate, like physical resources and capabilities, as I mentioned during the first part of this post. The overwhelming amount of technical terms shouldn\u0026rsquo;t discourage you to explore and expand your knowledge. You might find, like me, that it\u0026rsquo;s not as hard as it seems.\nThat\u0026rsquo;s all folks. Happy debugging!\nFootnotes I run my laptop with Arch Linux and I used CentOS 7 for my demo cluster.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nToo bad CentOS 6 users!\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nAgain, man 7 cgroups is your friend.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nI might be boring: man 7 capabilities.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThis snippet assumes your machine is running only one ash command.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nman ip. If you\u0026rsquo;re not familiar with it, today you have a good change to get started , because ifconfig has been long deprecated.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nHandy if you want to get it from a script, as a quick hack:\nMAC=$(ip addr show dev veth1 | grep \u0026#39;link/ether\u0026#39; | tr -s \u0026#39; \u0026#39; | cut -d\u0026#39; \u0026#39; -f3) \u0026#160;\u0026#x21a9;\u0026#xfe0e; Note that I\u0026rsquo;m using private IPv4 address spaces.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n24 bits are fixed by the network mask so I have only 8 bits to assign to hosts, but 172.19.35.0 is the network address, and 172.19.35.255 is the broadcast, so they aren\u0026rsquo;t usable.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA possibility would be to find it with ps aux, or if you\u0026rsquo;re lazy you could temporarily run a recognizable process and query it\u0026rsquo;s parent process from the host. I\u0026rsquo;m using top here:\nCPID2=$(ps -C ash -o ppid= | tr -d \u0026#39; \u0026#39;) \u0026#160;\u0026#x21a9;\u0026#xfe0e; The Address Resolution Protocol is responsible for translating IP addresses into MAC addresses. Every time a network device wants to communicate with an IP in the same subnet, the ARP protocol kicks in. It basically sends a broadcast packet asking to everybody: \u0026ldquo;how has this IP?\u0026rdquo;, and it saves the answer (IP address, MAC address) into a table. This way every time you need to reach that IP, you know already which MAC address to contact.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nNetwork Address Translation. This is the same mechanism your home router uses to connect you to the Internet. It basically maps all the internal network IPs into the only one that is externally available, and assigned to you by your ISP. Externally, only the router IP will be visible. So, when a packet is sent outside, the source address is rewritten to match the router external IP. When the reply comes back, the natting does the reverse, and replaces the destination address with the original source of the packet.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nSee also the Calico data path for some details.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nSome nice comments are present in the Calico source code about it. See intdataplane/endpoint_mgr.go:\n// Enable strict reverse-path filtering. This prevents a workload from spoofing its // IP address. Non-privileged containers have additional anti-spoofing protection // but VM workloads, for example, can easily spoof their IP. err := m.writeProcSys(fmt.Sprintf(\u0026#34;/proc/sys/net/ipv4/conf/%s/rp_filter\u0026#34;, name), \u0026#34;1\u0026#34;) if err != nil { return err } // Enable routing to localhost. This is required to allow for NAT to the local // host. err = m.writeProcSys(fmt.Sprintf(\u0026#34;/proc/sys/net/ipv4/conf/%s/route_localnet\u0026#34;, name), \u0026#34;1\u0026#34;) if err != nil { return err } // Enable proxy ARP, this makes the host respond to all ARP requests with its own // MAC. This has a couple of advantages: // // - In OpenStack, we\u0026#39;re forced to configure the guest\u0026#39;s networking using DHCP. // Since DHCP requires a subnet and gateway, representing the Calico network // in the natural way would lose a lot of IP addresses. For IPv4, we\u0026#39;d have to // advertise a distinct /30 to each guest, which would use up 4 IPs per guest. // Using proxy ARP, we can advertise the whole pool to each guest as its subnet // but have the host respond to all ARP requests and route all the traffic whether // it is on or off subnet. // // - For containers, we install explicit routes into the containers network // namespace and we use a link-local address for the gateway. Turing on proxy ARP // means that we don\u0026#39;t need to assign the link local address explicitly to each // host side of the veth, which is one fewer thing to maintain and one fewer // thing we may clash over. err = m.writeProcSys(fmt.Sprintf(\u0026#34;/proc/sys/net/ipv4/conf/%s/proxy_arp\u0026#34;, name), \u0026#34;1\u0026#34;) if err != nil { return err } // Normally, the kernel has a delay before responding to proxy ARP but we know // that\u0026#39;s not needed in a Calico network so we disable it. err = m.writeProcSys(fmt.Sprintf(\u0026#34;/proc/sys/net/ipv4/neigh/%s/proxy_delay\u0026#34;, name), \u0026#34;0\u0026#34;) if err != nil { return err } // Enable IP forwarding of packets coming _from_ this interface. For packets to // be forwarded in both directions we need this flag to be set on the fabric-facing // interface too (or for the global default to be set). err = m.writeProcSys(fmt.Sprintf(\u0026#34;/proc/sys/net/ipv4/conf/%s/forwarding\u0026#34;, name), \u0026#34;1\u0026#34;) if err != nil { return err } \u0026#160;\u0026#x21a9;\u0026#xfe0e; For the lazy reader I reported the whole sequence here. Create the container:\nunshare -pmn -f chroot rootfs /usr/bin/env -i \\ HOME=/root \\ PATH=/bin:/usr/bin:/sbin:/usr/sbin \\ /bin/ash -l Then from the host:\nCPID=$(ps -C ash -o pid= | tr -d \u0026#39; \u0026#39;) ip link add veth0 type veth peer name veth1 ip link set veth1 netns $CPID ip link set dev veth0 up echo 1 \u0026gt; /proc/sys/net/ipv4/conf/veth0/rp_filter echo 1 \u0026gt; /proc/sys/net/ipv4/conf/veth0/route_localnet echo 1 \u0026gt;/proc/sys/net/ipv4/conf/veth0/proxy_arp echo 0 \u0026gt;/proc/sys/net/ipv4/neigh/veth0/proxy_delay echo 1 \u0026gt;/proc/sys/net/ipv4/conf/veth0/forwarding echo 1 \u0026gt;/proc/sys/net/ipv4/conf/eth0/forwarding ip r add 172.19.36.2 dev veth0 scope link ip r add blackhole 172.19.36.0/24 and from the container:\nip link set dev lo up MAC=$(ip addr show dev veth1 | grep \u0026#39;link/ether\u0026#39; | tr -s \u0026#39; \u0026#39; | cut -d\u0026#39; \u0026#39; -f3) ip link set dev veth1 name eth0 address $MAC ip addr add dev eth0 172.19.36.2/32 ip link set eth0 up ip r add 169.254.1.1 dev eth0 scope link ip r add default via 169.254.1.1 dev eth0 \u0026#160;\u0026#x21a9;\u0026#xfe0e; ","permalink":"https://blog.mbrt.dev/posts/container-network/","summary":"Over the last year, at work I had multiple chances to debug how containers work. Recently we had to solve some networking problems a customer had with Kubernetes, and I decided I wanted to know more. Once the problem was solved, I spent more time on investigating what is actually going on under the hood. After seeing the wonderful Eric Chiang and Laurent Bernaille talks, and reading through the very informative posts by Lizzie Dixon and Julia Evans (that I really really recommend), I got enough information about how a container is created and managed.","title":"Demystifying container networking"},{"content":"Introduction In February 2014 I had a very unpleasant surprise after coming back home from a weekend in Berlin. Me and my girlfriend found out our apartment upside down because of burglars. We suffered the loss not only of many of our belongings, but worse, of our safety. We didn\u0026rsquo;t feel safe in our home and we didn\u0026rsquo;t feel safe to leave it - not even for half an hour - for the fear to come back and find somebody inside. It was with this mood that I started this project: totally motivated to do something about it.\nMy goal was to install in the apartment a security alarm that was cheap, safe and easy to use: something that I could trust. Nowadays there are plenty of alternatives that satisfy all the requirements, but at that moment I couldn\u0026rsquo;t find anything like that. As I am a software engineer, I decided to jump into the project and spend most of my free time on it (my girlfriend was only partially happy with this resolution).\nThe end result was a Raspberry Pi with a camera module and a bunch of software, in part taken from the open-source and in part written by me. As of the time of writing, it\u0026rsquo;s been running in production (namely my apartment) untouched for more than two years now. Ironically my security camera lasted way more than my apartment, because since then I already moved twice. So, quite a success for me.\nAnd, if you\u0026rsquo;re asking yourself if I had any burglars again since then, the answer is no, luckily. But I had the opportunity to test it multiple times when my parents have intruded my apartment without permission\u0026hellip;\nWhy? At this point you can probably see my motivation behind the project, and also the target audience: I needed something cheap that I, and my girlfriend could use. This set of users was important for the design. It meant that the normal usage should be simple, because my girlfriend (who is not a developer) had to use it. The configuration and maintenance was however on me, so I didn\u0026rsquo;t focus on make that simple.\nToday I\u0026rsquo;m open-sourcing the project. You can find it on GitHub under mbrt/antifurto. The name comes from an Italian word that means \u0026ldquo;security alarm\u0026rdquo;. It was a codename at the beginning, waiting to be changed in something better, but in the end I didn\u0026rsquo;t bother. So antifurto is still the name today.\nWhy am I open-sourcing that now, and not before? Why not keeping it closed source? The project started to meet my needs, so I had no reason at the beginning to open source something that was completely tied to my use case. Nobody would have gotten any particular benefit from it. After some time however it grown up into a more featured product. So, since at the time I wasn\u0026rsquo;t happy with my job, I considered starting a business and commercialize it. Long story short, getting funds for startups in Italy is quite hard, so I needed money from myself or my family. I then had to improve many parts of the project because I needed to scale out for multiple customers. It was not only about my apartment anymore. In the meantime competitors started to jump out from nowhere before I even got started. I had an opportunity for a new job, so my motivations kinda vaporized.\nAt the end of the day, there is not a commercial product, but a working prototype, and an interesting experience to share. I decided to open-source it now, because I needed time to put together this writeup and cleanup some documentation. All things that I find boring, so it took me a while.\nWhat? In this writeup I\u0026rsquo;m going to present the interesting bits of the project. I will try to not focus too much on the details, but rather to present clearly the high-level architecture, some design decisions and some interesting implementation bits.\nFeatures The antifurto project is essentially a security camera that allows to monitor what happens through the lenses of a single camera. When the camera detects motion above a certain threshold, it sends notifications through WhatsApp and emails and starts to record pictures. These are in turn saved to the local disk and uploaded to a Dropbox folder. There is also a web portal (optimized for desktop and mobile browsers) from which you can start and stop the monitoring, a live view from which you can see images in real time and an archive page for the past recordings. It\u0026rsquo;s not possible to combine multiple cameras together: single camera, single website.\nArchitecture The project architecture is simple. Everything lives inside a Raspberry Pi, period. There isn\u0026rsquo;t a server component communicating with the camera, or anything else. This is not an ideal architecture from the security point of view, because all the keys, including SSL, Dropbox API secrets, email passwords, etc are inside that box. It was the perfect solution for me, because instead of a device and a server to manage, I had only one device. Moreover, the development time was reduced because the architecture was simpler.\nThis unfortunately cannot work out if you want to provide the project to your mother (assuming she\u0026rsquo;s not a software developer). The Raspberry needs a non-zero amount of maintenance, to provide a minimum amount of security. This includes for example installing OS updates and rebooting the device periodically. The keys need to be safe guarded inside the Raspberry itself, and re-generated in case of leak. Again, this is not good if you want to do it properly, but I preferred to do something quick and get it working as fast as I could.\nNow for the details. The project is divided into three main parts:\nthe main antifurto executable (written in C++), which is responsible for the monitoring and notifications; a web server (Apache + PHP), that serves the website from which you can see the live stream, turn the monitoring on and off and view the archive; a FastCGI component that serves as a bridge between the webserver and the main executable. There are also other small satellite components and scripts, such as:\na bash script to send emails with the mail command; python scripts to send WhatsApp notifications and upload pictures to Dropbox. You can see below a diagram of the high level architecture:\nAs you can see, the pictures come from the camera module and are processed by the antifurto main executable. This decides whether to store the pictures on the local hard drive (an SD card), and upload them on Dropbox or not. It also decides, when to send notifications via email or WhatsApp messages. Whenever the user decides to start a live view from the web interface, or start/stop the monitoring, the backend sends a POSIX signal to the main process. If the desired action was to start the live view, the main executable will start to send the pictures over the zmq channel to the antifurto.fcgi component. Its only task is to forward them to the webserver via an FCGI socket.\nThe design is heavily based on observers, type erasure, composite reuse principle and SOLID principles, to minimize dependencies among components. Well, at least I tried to keep those in mind.\nIn the diagram you can see the architecture of the main executable. Each box represents a class. I didn\u0026rsquo;t represent all of them, but only the most important. For example I left out the utility classes like schedulers, queues, observer lists. The dark boxes represent controller classes, which are responsible for managing specific parts of the application. Controller classes manage all the boxes connected via a \u0026ldquo;tilted square\u0026rdquo; arrow. This means that they both own those classes (so they are responsible for their lifetime) and they know how to operate them. Red boxes don\u0026rsquo;t manage anything, but they provide a functionality either for other classes or talk to external services.\nOne important thing to notice is that each class is owned by one and only one controller. The architecture and the lifetime of the resources are very simple and clear in this way. A consequence is that classes can be tested individually much more easily, since there are no cyclical dependencies, and children don\u0026rsquo;t know anything about their parents.\nIn the diagram you can also see what are the inputs and outputs of each class. Red arrows are inputs, and dark arrows are outputs. You can see that I didn\u0026rsquo;t connect explicitly those arrows. Why? Because they are loose connections. Outputs are provided in the form of observers,1 and classes interested in pictures don\u0026rsquo;t know anything about the Camera class. So, controllers are responsible to \u0026ldquo;wire\u0026rdquo; those connections, by registering themselves to the inputs they need and forward them to the classes they manage. For example the RecordingController class register itself to both alarm notifications (provided by the MotionDetector) and the picture stream (provided by the Camera). It is managed by the MonitorController, so whenever the monitoring functionality is stopped, the recording classes can be safely deleted. The RecordingController then listens to alarm events and whenever one occurs, it forwards the pictures stream directly to the PictureArchive and the DropboxUploader.\nMain executable In this long section I\u0026rsquo;m going to talk about the internal details of the main executable, called antifurto for a very lack of fantasy.\nMain class The main class is called Antifurto, what a surprise! It is responsible to start and stop the monitoring and the live view, by orchestrating the resources involved. It uses a Config structure for the configuration, that comes from the command line and the configuration file. It can be used as an external library, as most of the components in this project, since it is self contained.\nIt contains all the controllers, that are described in the Main controllers section, and the implementation details are hidden from the header file behind a Pimpl.\nThe interface is very simple: it takes a configuration and the user can control when to start and stop monitoring and live view from four public methods:\nclass Antifurto { public: Antifurto(const Configuration\u0026amp; c, bool maintenanceNeeded = true); void startMonitoring(); void stopMonitoring(); void startLiveView(); void stopLiveView(); private: meta::ErasedUniquePtr\u0026lt;AntifurtoImpl\u0026gt; pimpl_; }; So, this class is all about the very high level use cases of configuring, starting and stopping the main functionalities.2\nThese functions are a bit less simple than one can at first imagine. For example the startMonitoring is anynchronous and starts the monitoring only after a configurable timeout. This is because after the start, the person may need to get out the way before the monitoring effectively starts. The default I\u0026rsquo;m using for myself is one minute. At the same time, the function needs to check if the user cancels the start request before the timer goes off. I needed to put some attention in the interaction between start, stop and the destructor. The CameraController lifetime depends on whether one between the monitoring and the live view functionalities are on:\nvoid handleCameraControllerNeed() { if ((liveViewActive_ || monitorActive_) \u0026amp;\u0026amp; !camera_) camera_.reset(new CameraController()); else if (!liveViewActive_ \u0026amp;\u0026amp; !monitorActive_) camera_.reset(); } This method is called by all the four external methods, to factor out this common part.\nMain controllers In this section I\u0026rsquo;m going to describe the three controllers that manage the monitoring, live view and the camera sub-components.\nMonitorController This class controls the monitoring functionality life cycle. It delegates to its sub-components tasks such as motion detection, and notifications. The most important part of its public interface is the examinePicture function:\nvoid examinePicture(const Picture\u0026amp; picture); The Main class calls this function whenever a new picture comes out of the camera.\nAnother interesting bit is the way this class asks for the upper level controller to change the picture capture interval, or to stop the recording altogether. To break cyclical dependencies, the upper level class has to instantiate the MonitorController by passing a couple of callbacks. One of them is the SetPicturesInterval:\nusing SetPicturesInterval = std::function\u0026lt;void(std::chrono::milliseconds)\u0026gt;; that is used whenever some motion is detected. In that case, the MonitorController asks for an increase of the capture frequency. It\u0026rsquo;s also useful whenever nothing is going on, to decrease the capture frequency and so save energy:\nvoid MonitorController::onAlarmStateChanged(MotionDetector::State state) { using State = MotionDetector::State; switch (state) { case State::NO_ALARM: setPicturesInterval_(config::monitorCycleDuration()); break; case State::PRE_ALARM: setPicturesInterval_(config::monitorCycleDurationOnAlarm()); break; default: break; } log::info() \u0026lt;\u0026lt; \u0026#34;Alarm state: \u0026#34; \u0026lt;\u0026lt; state; } CameraController This class is responsible to take pictures from a camera at a given rate. A user of this class can register an observer and specify the rate at which the pictures have to be taken:\nclass CameraController { public: using Subject = meta::Subject\u0026lt;const Picture\u0026amp;\u0026gt;; using Observer = Subject::Observer; using Registration = Subject::Registration; using Period = std::chrono::milliseconds; /// Set the pictures capture rate void setDesiredPeriod(Registration const\u0026amp; r, Period period); /// Add an observer to the pictures flow Registration addObserver(Observer observer, Period desiredPeriod); // ... }; This uses the observer pattern, implemented as an utility in the meta namespace.\nEvery time a picture is taken, the observer callback is called. If multiple observers are interested in different capture rates, the maximum rate is used. This means that an observer specifies the minimum speed, but it could get pictures at a higher speed, if it\u0026rsquo;s necessary for other observers.\nTo implement this functionality, in a separate thread a Metronome class sleeps the required time, and then the Camera class takes a picture. Every time an observer is registered or de-registered, the sleep time is updated.\nLiveViewController This class starts and stops the live view functionality. It doesn\u0026rsquo;t implement the functionality itself; it just controls the lifetime of a LiveView object. From the outside it takes pictures and the start and stop commands.\nWhenever a picture comes, it is forwarded to the internal LiveView object. To detect when the user is not interested in the live view anymore, there is a primitive control flow, which is basically a fixed queue of pictures sent to the browser. When the client doesn\u0026rsquo;t request them, the queue fills up. After a certain timeout with a full queue, the LiveViewController simply stops the live view:\nif (liveView_-\u0026gt;addPicture(p)) lastPictureWrittenTime_ = system_clock::now(); else if (system_clock::now() - lastPictureWrittenTime_ \u0026gt; timeout_) stop(); To do this, the internal LiveView object simply informs whether it has been able to process the image or not, and if not, the timeout is checked.\nThe stop function invokes a callback, that asks to be de-registered from the stream of pictures.\nPicture\u0026rsquo;s capture MotionDetector This class uses the OpenCV library to examine the pictures flow and determine if something is moving. It implements the observer pattern to notify the observers for the current state. The motion detection code is pretty simple:\ncv::absdiff(curr_, p, currDiff_); cv::bitwise_and(prevDiff_, currDiff_, motion_); if (motionHappened()) onMotionDetected(); else onNoMotion(); // save std::swap(prevDiff_, currDiff_); curr_ = p; The code works with three pictures: the current one and the last two. Two images are computed out of them by making a difference (i.e. subtracting the gray values of the pixels one by one) between the first with the second and the second with the third. Then a \u0026ldquo;bitwise and\u0026rdquo; is computed between them. Random noise will be filtered out, since it\u0026rsquo;s unlikely to stay still for three frames, and the image will be almost completely black. Whenever something moves however, certain areas of the pictures will differ among the three frames, and so the difference will produce white pixels. These pixels are then counted in motionHappened(), and if they exceed a certain threshold, then motion is detected.\nThere is an additional layer of protection against errors, and it\u0026rsquo;s a state machine that counts how many consecutive moving frames have been detected. These states are used to better control energy saving, picture capture and alarm notifications.\nEvery time a transition occurs in this state machine, all the observers are notified. It will be up to them to take the right action.\nEverything starts from the IDLE state. Whenever some motion is detected, the state becomes PRE_ALARM. If no more motion frames are detected, the state goes back to IDLE. If the motion continues however, the state machine transitions to ALARM. It stays there while the motion continues. When it stops, the state goes to the STILL state. This means that even though nothing is moving, for some time, the alert level is still on alarm. Indeed, if some motion happens again, the state turns immediately to ALARM again. If instead nothing happens for some time, the state goes back to IDLE.\nIn this way we have decoupled the abstract states in which the system may be with the actions the various components have to take to respond.\nCamera The camera type is statically determined in StaticConfig.hpp. In the Raspberry-Pi case, there is a homegrown version implemented by PiCamera that uses a slightly modified version of the picam library, that I found here. This library is a simple interface on top of the Raspberry userland library I forked just to ease the build. To capture images outside the Raspberry world I instead opted for the OpenCV library and implemented CvCamera. Now, I have to admit that the CvCaptureRAII class might look a bit weird, but it was an attempt to implement the camera resource through RAII. I took inspiration from Martinho Fernandez rule of zero blog post and the concern about the rule of zero by Scott Meyers. To discuss this in detail I would need an entire blog post in itself, so I\u0026rsquo;ll just point you to these valuable resources. To be honest I\u0026rsquo;m not very satisfied by its look and feel now.\nWith the same spirit I implemented the capture resource for PiCamera, which is just a one liner:\nstd::unique_ptr\u0026lt;CCamera, void(*)(CCamera*)\u0026gt; capture_; It uses the non-so-well-known custom deleter feature of std::unique_ptr. Again, look at the Fernandez\u0026rsquo;s post for an explanation on why I didn\u0026rsquo;t just implemented a stupid destructor for PiCamera. Everything is handled automatically, since in the constructor I pass the resource, and the deleter function to be called in destruction (namely picam_stop_camera):\nPiCamera::PiCamera(int width, int height) : width_(width), height_(height) , capture_(::picam_start_camera(width, height, 10, 1, false), \u0026amp;::picam_stop_camera) { // ... } These two different implementations of the camera resource were not intended to be used at the same time: one was only for the Raspberry Pi hardware, and the other for PC\u0026rsquo;s with USB cameras. For this reason I didn\u0026rsquo;t introduce any common interface, and just used a compile time define and a typedef to switch between them:\nnamespace antifurto { namespace config { #if defined(ANTIFURTO_RASPBERRY) using Camera = antifurto::PiCamera; #else using Camera = antifurto::CvCamera; #endif }} The code will simply refer to the antifurto::config::Camera type to get a capture resource. I just needed to make sure their public interface (i.e. the public methods) are the same, so the two classes could be used interchangeably.\nThis trick is quite handy if you don\u0026rsquo;t need runtime polymorphism, but honestly it\u0026rsquo;s a bit overkill for this project.\nLiveView This class is managed by the LiveViewController and is responsible to forward pictures to a ZeroMQ socket. It has a single producer / single consumer queue (see the concurrency section) and a worker thread to offload the communication.\nThe interesting part about this class is the use of a non-blocking lock-free queue, that allows minimum interruption for the producer. Whenever the queue is full, the images are discarded, and the caller is notified, in order to make some control flow, without interrupting the images flow.\nFor the communication to the webserver we use the request-reply pattern in ZeroMQ. It\u0026rsquo;s a simple protocol where at very request corresponds one reply. Reconnections are implemented in the FastCGI backend, with the ZmqLazyPirateClient class.\nPicture recording RecordingController This class is responsible for managing the registration of the pictures while an alarm is active. It accepts pictures with the void addPicture(Picture p) method and registers itself to the MotionDetector to know when to start and stop the recording. This is done by saving Jpeg pictures on the local file system (by using PictureArchive) and uploading them to Dropbox (by using DropboxUploader).\nThe state machine is quite simple:\nvoid RecordingController::onAlarmStateChanged(MotionDetector::State state) { using State = MotionDetector::State; switch (state) { case State::NO_MOTION: archive_.stopSaving(); break; case State::NO_ALARM: enqueueOlderPictures(); break; case State::ALARM: archive_.startSaving(); break; case State::PRE_ALARM: default: break; } } Whenever the motion detector notifies this class about an alarm, it starts to save the pictures. When there is no motion involved (even if the alarm is still active), the recording is stopped.\nSaving pictures in real time is important, both on disk and online. If there is a slow upload for any reason, the queue between the producer (the Camera) and the consumer (the uploader), grows. This would mean that by looking at the pictures online, the delay between capture and upload will grow more and more over time during alarms. To avoid this behavior, the queue size is limited, and whenever it\u0026rsquo;s full, the coming pictures are queued in a secondary one:\nvoid RecordingController::onPictureSaved(const std::string\u0026amp; fileName) { if (!uploadWorker_.enqueue(fileName)) { log::info() \u0026lt;\u0026lt; \u0026#34;Failed to upload picture to Dropbox: queue is full\u0026#34;; std::unique_lock\u0026lt;std::mutex\u0026gt; lock(toUploadAfterQueueMutex_); toUploadAfterQueue_.emplace(fileName); } } This ensures a fixed maximum delay between capture and upload, just by skipping pictures now and then, when the queue is full. All the missing pictures are instead uploaded when the alarm is not active anymore (the case State::NO_ALARM: above):\nwhile (!toUploadAfterQueue_.empty()) { if (uploadWorker_.enqueue(toUploadAfterQueue_.front())) toUploadAfterQueue_.pop(); else break; } // if the queue is not empty, we need to schedule another upload cycle if (!toUploadAfterQueue_.empty()) { log::info() \u0026lt;\u0026lt; \u0026#34;Cannot empty the upload queue. Schedule a new upload\u0026#34;; scheduler_.scheduleAfter(std::chrono::minutes(10), [this] { enqueueOlderPictures(); }); } The logic is a bit brutal but it works. While there is still something to upload, it adds the pictures to the upload queue. If the queue gets full again, a new procedure is scheduled after 10 minutes.\nThere is another maintenance procedure, to avoid a full hard drive. Every 24 hours, older pictures are removed. Depending on the configuration, only a certain amount of days are kept:\n// schedule maintenance at every midnight using namespace std::chrono; auto maintenanceWork = [this] { performMaintenance(); }; scheduler_.scheduleAt(concurrency::tomorrow() + minutes(1), [=] { performMaintenance(); scheduler_.scheduleEvery(hours(24), maintenanceWork); }); PictureArchive This class saves pictures in Jpeg format to a given folder. It takes a stream of pictures and two commands: startSaving and stopSaving. When the recording is started, not only the next picture is saved, but also some of the previous. This object has indeed a fixed sized circular buffer that allows to retroactively save the images right before an alarm popped up. It also allows observers to register for when a picture is saved to disk, getting the file name.\nvoid PictureArchive::save(Picture\u0026amp; p, Clock t) { std::string filename{ fs::concatPaths(currentFolder_, text::toString(t, text::ToStringFormat::FULL, \u0026#39;-\u0026#39;, \u0026#39;_\u0026#39;) + \u0026#34;.jpg\u0026#34;)}; cv::putText(p, text::toString(t, text::ToStringFormat::SHORT, \u0026#39;/\u0026#39;, \u0026#39; \u0026#39;), cv::Point(30,30), CV_FONT_HERSHEY_COMPLEX_SMALL, 0.8, cv::Scalar(200,200,250), 1, CV_AA); cv::imwrite(filename, p, {CV_IMWRITE_JPEG_QUALITY, 90}); notifyObservers(filename); } The picture gets a timestamp text overlay on the top left corner and then is saved on disk.\nOn the bad side there is the ring buffer, which is actually not a ring buffer at all. Pictures are pushed to the end of a vector. The beginning is then deleted by moving all the other elements at the previous index. Not pretty, not fast, but all in all it works. Moving to a proper circular buffer should not be very hard.\nDropboxUploader This class is responsible for uploading files to a Dropbox account, by using an external dropbox_uploader.sh script. It just generates a configure file for it, starting from the Antifurto\u0026rsquo;s configuration, and uploads a file when requested, by launching an external process. Nothing fancy here, I just forked andreafabrizi/Dropbox-Uploader.\nNotifications Two types of notifications are supported: WhatsApp and emails. WhatsApp have been historically fighting against bots. For this reason the phone numbers I used as source for notifications have been banned. I don\u0026rsquo;t recommend using it for this reason. A much more sane approach would have been to implement a Telegram bot instead, but at that time they didn\u0026rsquo;t exist. Email notifications are instead much more safe and reliable to use. For those two functionalities we have two very similar controllers: WhatsappNotificationController and MailNotificationController, that register themselves to the MotionDetector and whenever there is an alarm, they try to use their counterpart (WhatsappNotifier and MailNotifier) to send the notifications asynchronously. They also take care of retrials in case of errors, and avoid sending too many of them in a short period of time, to avoid flooding the receivers.\nWhatsappNotifier This class manages WhatsApp notifications. Whenever send(std::string const\u0026amp; dest, std::string const\u0026amp; msg) is called, it sends a message with yowsup-cli by spawning an external process. This class just generates the configuration file needed by Yowsup from the main process configuration and takes care of its execution.\nMailNotifier This class is responsible for sending emails.\nvoid send(ContactList const\u0026amp; dest, std::string const\u0026amp; sender, std::string const\u0026amp; subject, std::string const\u0026amp; body); It calls an external bash script that uses the Unix mail utility, to send the mail.\nUtility libraries Here I present some random notes on the utility namespaces that help with design patterns, concurrency, filesystem and logging. Some of them are a bit over-engineered but in hobby projects you also need to have some fun, don\u0026rsquo;t you? :)\nmeta namespace This namespace contains some generic patterns and algorithms that do not depend on the specific details of the project itself. In Observer.hpp you can find a generic implementation of the observer pattern. A Subject wants to provide observers the possibility to register for events. The class takes a variadic number of type parameters, that will be used in the notification. For example:\nSubject\u0026lt;int, float\u0026gt; s; auto reg = s.registerObserver([](int a, float b) { print(a, b); }); s.notify(3, 3.14); in this example we want to notify our observer with an integer and a float. To do that we just need to declare Subject with the right parameters. This will in turn be able to accept observers that respect the std::function\u0026lt;void(int, float)\u0026gt; signature.\nInteresting:\nthe registration returns a token that when goes out of scope unregisters the observer automatically; it is possible to register and unregister observers within notification callbacks (re-entrant calls are supported). Other small utilities are also present, like ErasedUniquePtr, which provides a unique pointer with an erased deleter. This is an useful workaround to a subtle problem when you want to forward declare a class and use it in an unique pointer. For more details see the type erasure post of Andrzej\u0026rsquo;s blog.\nfs This namespace contains simple path manipulation utilities to concatenate multiple paths with a single call:\nstd::string p = fs::concatPaths(\u0026#34;/var/log\u0026#34;, bar, \u0026#34;file.txt\u0026#34;); This is similar to what boost::filesystem does, but in a more functional way.\nlog This namespace contains logging utilities. The focus of this library was to provide a fast and simple logging without using macro shenanigans.\nYou can use it with a call to a free function, that will return the proper logger:\nlog::debug() \u0026lt;\u0026lt; \u0026#34;my log here \u0026#34; \u0026lt;\u0026lt; 15; There is also a reload function. When a log rotation occurs it will simply close the old file (that has been rotated) and open a new file in the same place. Ignored log levels are implemented by returning a logger that writes to a NullSink, which simply does nothing. Interestingly cryptic is the implementation of an std::outstream that does nothing. You can find it in log/NullStream.hpp.\nconcurrency This namespace contains some classes that deal with concurrency. An interesting one is SpScQueue, that wraps a worker thread and allows to enqueue work items for it. The type of the work item is templated, to maximize reusability. The queue is a lock-free implementation that can be chosen at compile time among a fixed-size and a dynamically allocated one. The former is preferred in case the maximum queue size is known at compile time.\nAs a side note I would like to add here that since the project deals with real-time data, avoiding dynamic allocations can be critical. We used fixed bound queues in all places for this reason.\nAnother interesting class in this namespace is the TaskScheduler. It provides the possibility to schedule tasks at certain time points, either one-shot or periodic:\nvoid scheduleAt(Clock::time_point t, Task w); void scheduleAfter(Clock::duration d, Task w); void scheduleEvery(Clock::duration d, Task w); The work items are processed one after the other in a worker thread, so delays added by one task impact on the next ones. It is for this reason used only for short tasks.\nipc This namespace contains classes related to child processes and inter-process communication. There is a forkAndCall function, that forks the process, calls a the given function and returns the function result by using the child process exit code:\n/// This function fork the process, calls the function in the child process, /// wait for completion and returns the function return value. ChildProcess forkAndCall(std::function\u0026lt;int()\u0026gt; f); The child process itself can be killed or waited. In the latter case, the function return code will be returned.\nIn this namespace there is also a NamedPipe class that provides Linux named pipes. The constructor creates a FIFO with the given file name, and the destructor removes it.\nThere is also an interesting PosixSignalHandler class, that handles POSIX signals safely. You need to use it carefully though: initialize it at the beginning of the main function, before any thread creation, and register all the signal handlers as soon as possible, by using:\nvoid setSignalHandler(int signal, Handler h); where an handler is a callback that takes the signal that just happened:\nusing Handler = std::function\u0026lt;void(int)\u0026gt;; The POSIX standard says that a lot of functions are not safe to be used within signal handlers. For example it\u0026rsquo;s not possible to allocate heap memory and call many standard library functions. We need however to support arbitrary code execution in the handlers, so to workaround this we use a vector of atomic booleans, one for each possible signal. Whenever a signal is sent to the process, the handler flips the corresponding boolean to true. A separate thread polls that vector, and executes the registered handlers, if any were given. This allows the signal handler to return immediately and in a safe way:\nstd::vector\u0026lt;std::atomic\u0026lt;bool\u0026gt;\u0026gt; signalsToBeHandled(SIGRTMAX); void sigactionHandler(int sig, siginfo_t* , void* ) { signalsToBeHandled[sig].store(true, std::memory_order_release); } and the user-defined handler is called asynchronously in a separate thread. This allows to execute arbitrary code.\ntext In this namespace we have some string manipulation utilities, like toString. This free function converts any list of printable objects in an std::string, e.g.\nstd::string s = text::toString(\u0026#34;my \u0026#34;, std::string(\u0026#34;s\u0026#34;), 15, true); Allowing to both covert objects into strings and concatenate them, without the need of odd std::ostringstream objects all around the codebase.\nA TextReplace class allows to do replace variable occurrences in a text with user specified values. For example:\nstd::ifstream f(\u0026#34;file.txt\u0026#34;); std::ostringstream out; text::TextReplace r; r.addVariable(\u0026#34;var\u0026#34;, \u0026#34;X\u0026#34;); r.addVariable(\u0026#34;foo\u0026#34;, \u0026#34;BAR\u0026#34;); r.replaceVariables(f, out); and suppose file.txt contains:\nreplace ${var} variables with ${foo} their values ${p}. the result of the replacement will be:\nreplace X variables with BAR their values ${p}. Note that unknown variables are left untouched.\nWebsite I am not so proud of the website code, and I don\u0026rsquo;t recommend looking at it in detail. I did not have much experience in web development at that time, but I am still quite happy with the result. Year ago it was not so obvious that a website was mobile ready:\nThe website is just a bunch HTML + JavaScript pages. For the styling and the responsive design I went with the immortals Bootstrap and JQuery, while for the server side part I used the now infamous PHP.\nCommands like start and stop monitoring and live view are issued by the frontend by doing GET requests to pages under the controller/ path. The PHP backend listening that endpoint sends POSIX signals to the main executable. The communication is not more complicated than that, because this first implementation worked fine. I didn\u0026rsquo;t bother changing it in something more complicated.\nThe funniest part of the frontend is the live view though. Also in this case the first implementation was good enough :). Basically the frontend uses an infinite loop of Ajax requests3 to a special live.jpg picture, which is served by a custom FastCGI backend, written in C++. This is the one described in the FastCGI backend section.\nfunction loadImage(url, imageObj, target) { imageObj.onload = function() { target.setAttribute(\u0026#39;src\u0026#39;, this.src); loadImage(url, imageObj, target); }; imageObj.src = url + \u0026#39;?_=\u0026#39; + new Date().getTime(); } $.ajax({ url: \u0026#39;../controller/live.php\u0026#39;, dataType: \u0026#39;json\u0026#39;, cache: false }) .done(function(data) { if (data.result == 0) { $(\u0026#39;.live-container\u0026#39;).html( \u0026#39;\u0026lt;img id=\u0026#34;liveimg\u0026#34; class=\u0026#34;img-responsive\u0026#34;\u0026gt;\u0026lt;/img\u0026gt;\u0026#39; ); var img = document.getElementById(\u0026#39;liveimg\u0026#39;); loadImage(\u0026#39;live.jpg\u0026#39;, img, new Image); } else displayMessage(\u0026#39;.live-container\u0026#39;, \u0026#39;\u0026lt;h4\u0026gt;Ooops...\u0026lt;/h4\u0026gt;\u0026#39; + \u0026#39;\u0026lt;p\u0026gt;\u0026#39; + data.log + \u0026#39;\u0026lt;/p\u0026gt;\u0026#39;, \u0026#39;alert-danger\u0026#39;); }) .fail(function(jqxhr, textStatus, errorThrown) { displayMessage(\u0026#39;.live-container\u0026#39;, \u0026#39;\u0026lt;h4\u0026gt;Ooops...\u0026lt;/h4\u0026gt;\u0026#39; + \u0026#39;\u0026lt;p\u0026gt;\u0026#39; + errorThrown + \u0026#39;\u0026lt;/p\u0026gt;\u0026#39;, \u0026#39;alert-danger\u0026#39;); }); Yes, that\u0026rsquo;s it. I didn\u0026rsquo;t even have to shorten the code. Something that I couldn\u0026rsquo;t explain myself here was that in loadImage I couldn\u0026rsquo;t use JQuery, because it was much slower than the old style setAttribute and image.src = url. So I decided to live with that.\nThe archive page shows pictures from previous alarms. Just don\u0026rsquo;t look at the PHP code behind that, it\u0026rsquo;s really horrible crap. It can give you nightmares for days.\nThis is the carusel view:\nThis is the mobile version:\nAnd this is the date selector for the alarm, in the mobile version:\nFastCGI backend One of the website backend components is ironically in a folder called frontend. The name is unfortunate but it was meant to suggest an interface to the main executable. It communicates with it via a ZeroMQ socket, and with the web server through FastCGI.\nInterestingly enough, the first implementation was in Python, but it was too slow. I had to re-implement it in C++, and now it\u0026rsquo;s about three orders of magnitude faster (yes, I really mean 1000X).\nThe main.cpp file contains all the logic:\nA webserver request is directed to the executable through the standard input (which is ignored); a picture is requested to the main antifurto executable through a ZeroMQ request; as soon as a reply arrives, it is immediately written to the standard output, that is read by the webserver. There are a bunch of utility classes that have been used to make the code cleaner, described in the following sections.\nZmqLazyPirateClient This class implements the Lazy pirate pattern in ZeroMQ, which is a request-reply transition supporting socket reconnections. This allows to start and stop the main executable and the webserver independently; the connection between them will catch up automatically. When a request-reply transaction is needed, this class will send the request and wait until the reply comes, or a timeout expires. On timeout, the request is sent again, until the maximum number of retrials is reached. At that point the transaction is considered failed.\nStream utilities The StreamRedirector class is responsible for redirecting the standard input and output to FastCGI stream buffers, while StreamReader allows to buffer reads from a stream (in this case the standard input). I actually don\u0026rsquo;t remember because it\u0026rsquo;s a class instead of a simple function. Probably it\u0026rsquo;s a non-sense.\nConclusion In this post we had a look at the pet project I worked on for a while some years ago. By skimming through this post again I realized that it is mostly a random collection of impressions, design decisions and code snippets, so I don\u0026rsquo;t know how effective that is for a reader. However, for me it was important to wrap up, because after all the time and effort spent, I didn\u0026rsquo;t want to forget it, and I also wanted to share my insights with the community.4\nMy takeaways are that with this project I learned some stuff and I did something useful for myself. I would definitely recommend working on things you really need, as opposed to experimenting with technologies purposelessly. It really helps to get them done (to a certain extent at least). Or at least that\u0026rsquo;s the only way I found preventing me to give up projects too early.5\nI hope this post gave you some interesting insights and maybe inspire you some extensions, related projects or ideas. The code is open source on GitHub, under mbrt/antifurto, as I wrote earlier. I encourage you to take a look yourself to some of the classes. You can also build it and use it as is for your own security alarm. The deployment is kind of a pain right now, because there are many dependencies and configuring the external services is not exactly easy to do (Dropbox, mails, WhatsApp, etc). The documentation is also somehow lacking; apologies for that.\nThat\u0026rsquo;s all folks!\nTake a look at the meta namespace for the implementation.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nIf you are curious, the ErasedUniquePtr class is briefly described in the meta namespace section.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nYes, I know WebSocket existed already years ago, but really, at that time my phone didn\u0026rsquo;t support them, and I didn\u0026rsquo;t feel like developing two different protocols.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThe license is GPL.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nSee do finish your stuff by Martin Sústrik.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://blog.mbrt.dev/posts/antifurto/","summary":"Introduction In February 2014 I had a very unpleasant surprise after coming back home from a weekend in Berlin. Me and my girlfriend found out our apartment upside down because of burglars. We suffered the loss not only of many of our belongings, but worse, of our safety. We didn\u0026rsquo;t feel safe in our home and we didn\u0026rsquo;t feel safe to leave it - not even for half an hour - for the fear to come back and find somebody inside.","title":"Antifurto: home made security camera"},{"content":"I\u0026rsquo;ve been playing around with Rust for a year and a half, and the best part of it, like many others say, has been the very helpful community. There are a lot of online resources that help you to get started: the Rust book, the Rustonomicon and many blog posts and stack overflow questions. After I learned the basics I felt a bit lost though. I couldn\u0026rsquo;t find enough resources for intermediate-level-Rustaceans. I\u0026rsquo;m a C++ developer in my daily job, and so I\u0026rsquo;m used with books like Effective C++ from Scott Meyers, the Herb Sutter\u0026rsquo;s blog and a lot of online resources that always helped me with advanced C++ topics (that are a lot\u0026hellip; :sigh:). Those resources teach you how to get the best from the language, how to use it properly, and how to structure your code to be more clear and effective. Those resources are not completely absent in the Rust community, but neither common.\nHow do you learn those things then? Well, there are two ways in my opinion: you spend a lot of time and learn by doing, or you look at some good code. I think code reviews are incredibly useful; you can see how other people reason about problems you also struggled with, and how they have solved them. This post attempts to target those intermediate-level-Rustaceans (like me), by looking at the ripgrep crate by Andrew Gallant, a great example of good Rust.\nIntroduction I\u0026rsquo;m not going to explain everything about the crate, since there is already a very good blog post by Andrew himself, explaining how the application works from a functional perspective, and some used algorithms. We are going instead to walk through the crate architecture. I\u0026rsquo;m going to take for granted some of the basics, so if you need a refresher you can take a look at the resources I mentioned above.\nWe are going to look at this specific version of the crate:\n$ git describe 0.2.5-4-gf728708 which is the last one at the time of writing. By the time you are reading this, however, the crate might have evolved, so if you want to look at the code by yourself while reading, you should checkout this specific version:\n$ git clone https://github.com/BurntSushi/ripgrep.git $ cd ripgrep $ git checkout f728708 and without further ado, let\u0026rsquo;s get started.\nThe big picture ripgrep is a command line tool for searching file contents using regular expressions, similarly to GNU grep. The tool is split across four crates: the main one (ripgrep), ignore, grep and globset.\nThe grep crate provides line-by-line regex searching from a buffer and it is used only by the main crate. The globset crate uses regex to perform glob matching over paths. It is used by the main and the ignore crates. The ignore crate implements directory walking, ignore and include patterns. It uses the glob crate for that. Finally, the main crate, that glues everything together, implements command line argument parsing, output handling and multi-threading.\nOne clear advantage of splitting an application in multiple crates is that this forces you to keep your code scoped. It\u0026rsquo;s easy to create a mess of dependencies among the components if everything is in the same crate (or, even worse, in the same module). If you instead take a part of your application and try to give it a meaning by itself, you\u0026rsquo;ll end up with a more generic, usable and clearer interface. Embrace the Single responsibility principle and let it be your guide, like ripgrep clearly does.\nMain Everything starts from the ripgrep main function:\nfn main() { match Args::parse().and_then(run) { Ok(count) if count == 0 =\u0026gt; process::exit(1), Ok(_) =\u0026gt; process::exit(0), Err(err) =\u0026gt; { eprintln!(\u0026#34;{}\u0026#34;, err); process::exit(1); } } } It is very concise: it parses the command line arguments and then passes them to the run function. In between, there is the Result::and_then combinator, so the match statement gets to the Ok branch only if both operations succeed. If not, it selects the Err branch, handling errors for both the first and the second operation. Then the exit code depends on whether the count for matches is not zero.\nfn run(args: Args) -\u0026gt; Result\u0026lt;u64\u0026gt; { // ... } The run function at first decides if it\u0026rsquo;s worth to spawn threads or not, and if so, this is the way it setups the things:\nThe main thread, controlled by the run function digs files from the file system, and pushes them into a deque. This is a Single-producer / Multiple-consumers queue, from which multiple worker threads can pull at the same time. They will in turn perform the search operations. Here is the workers initialization in the run function:\nlet workq = { let (workq, stealer) = deque::new(); for _ in 0..threads { let worker = MultiWorker { chan_work: stealer.clone(), // initialize other fields... }; workers.push(thread::spawn(move || worker.run())); } workq }; As you can see, the deque::new() returns two objects. The queue is indeed composed by two ends: one is the workq from which the main thread can push, and the other end is the stealer, from which all the workers can pull. The loop creates a bunch of workers and move them to new threads, along with a stealer. Note that the stealer is cloneable, but this doesn\u0026rsquo;t mean that the queue itself is cloned. Internally indeed the stealer contains an Arc to the queue:\npub struct Stealer\u0026lt;T: Send\u0026gt; { deque: Arc\u0026lt;Deque\u0026lt;T\u0026gt;\u0026gt;, } To note here is the beauty of the deque interface. To express the fact that the producer is only one, but the consumers can be multiple, the type is split in two: the producer is then Send but not Sync, nor Clone. There is no way to use it from multiple threads, since you can move it to another thread, but in that case you lose your reference to it. The Stealer, which is the other end, is instead both Send and Clone. You can then pass it around by cloning and sending the copies off to other threads; they all refer to the same queue. There is no way to use this interface incorrectly.\nAnother thing to note here is that the workq variable is initialized by a block that returns just the producer part of a new deque. Inside the block, the workers along with their stealers are moved into new worker threads and those are in turn pushed into a vector. Using a block that just returns what it\u0026rsquo;s needed for the rest of the function is a good practice. In this way the run function is not polluted with variables that are not usable anymore because their values have been moved.\nThis is the MultiWorker struct, that runs in a separate thread:\nstruct MultiWorker { chan_work: Stealer\u0026lt;Work\u0026gt;, quiet_matched: QuietMatched, out: Arc\u0026lt;Mutex\u0026lt;Out\u0026gt;\u0026gt;, #[cfg(not(windows))] outbuf: Option\u0026lt;ColoredTerminal\u0026lt;term::TerminfoTerminal\u0026lt;Vec\u0026lt;u8\u0026gt;\u0026gt;\u0026gt;\u0026gt;, #[cfg(windows)] outbuf: Option\u0026lt;ColoredTerminal\u0026lt;WindowsBuffer\u0026gt;\u0026gt;, worker: Worker, } the first field is the stealer. As you can see from its type, the stealer receives Work structs:\nenum Work { Stdin, File(DirEntry), Quit, } The main thread will push them from its workq variable:\nfor dent in args.walker() { if quiet_matched.has_match() { break; } paths_searched += 1; if dent.is_stdin() { workq.push(Work::Stdin); } else { workq.push(Work::File(dent)); } } The args.walker() is an iterator over the files to search, or the standard input stream, if the - argument is passed. In the former case it pushes a Work::File entry with the path, in the latter a Work::Stdin entry. The items are received in the MultiWorker::run function. It is a loop that pops items from the deque and process them one by one:\nloop { if self.quiet_matched.has_match() { break; } let work = match self.chan_work.steal() { Stolen::Empty | Stolen::Abort =\u0026gt; continue, Stolen::Data(Work::Quit) =\u0026gt; break, Stolen::Data(Work::Stdin) =\u0026gt; WorkReady::Stdin, Stolen::Data(Work::File(ent)) =\u0026gt; { match File::open(ent.path()) { Ok(file) =\u0026gt; WorkReady::DirFile(ent, file), Err(err) =\u0026gt; { eprintln!(\u0026#34;{}: {}\u0026#34;, ent.path().display(), err); continue; } } } }; // ... } The steal() method tries to pop from the deque and returns a Stolen instance:\npub enum Stolen\u0026lt;T\u0026gt; { /// The deque was empty at the time of stealing Empty, /// The stealer lost the race for stealing data, and a retry may return more /// data. Abort, /// The stealer has successfully stolen some data. Data(T), } The outcome is matched against the different possibilities, but only Stolen::Data contains a Work entry. Both Stdin and File entry types are then translated into a WorkReady instance. In the second case the file is then opened with an std::fs::File. The work variable is later consumed by a Worker instance:\nself.worker.do_work(\u0026amp;mut printer, work); We\u0026rsquo;ll get back to that in a moment, but let\u0026rsquo;s first backtrack to the MultiWorker::run loop. The Work::Quit case breaks it, so the thread terminates:\nlet work = match self.chan_work.steal() { // ... Stolen::Data(Work::Quit) =\u0026gt; break, // ... }; This value is pushed by the main thread after it walks through all the files.\nfor _ in 0..workers.len() { workq.push(Work::Quit); } let mut match_count = 0; for worker in workers { match_count += worker.join().unwrap(); } The threads are all guaranteed to terminate because the number of Quit messages pushed is the same as the number of workers. A worker can only consume one of them and then quit. This implies, since no messages can be lost, that all the workers will get the message at some point, and then terminate. All the workers threads are then joined, waiting for completion.\nTo recap, this is a the multi-threading pattern used:\na deque in between a producer (that provides the work items) and a bunch of consumers (that do the heavy lifting) in separate threads; the deque carries an enumeration of the things to do, and one of them is the Quit action; the producer will eventually push a bunch of Quit messages to terminate the worker threads (one per thread). In case you just have one type of job, it makes perfect sense to use an Option\u0026lt;Stuff\u0026gt; as work item, instead of an enumeration. The workers have then to terminate in case None is passed. The Option can be used also in the ripgrep case instead of the Quit message, but I\u0026rsquo;m not sure the code would be more readable:\nlet work = match self.chan_work.steal() { Stolen::Empty | Stolen::Abort =\u0026gt; continue, Stolen::Data(None) =\u0026gt; break, Stolen::Data(Some(Work::Stdin)) =\u0026gt; WorkReady::Stdin, Stolen::Data(Some(Work::File(ent)) =\u0026gt; { // ... } }; Mono thread ripgrep can also operate in a single thread, in case there is only one file to search or only one core to use, or the user says so. The run function checks that:\nlet threads = cmp::max(1, args.threads() - 1); let isone = paths.len() == 1 \u0026amp;\u0026amp; (paths[0] == Path::new(\u0026#34;-\u0026#34;) || paths[0].is_file()); // ... if threads == 1 || isone { return run_one_thread(args.clone()); } and calls the run_one_thread function (I have removed some uninteresting details from it):\nfn run_one_thread(args: Arc\u0026lt;Args\u0026gt;) -\u0026gt; Result\u0026lt;u64\u0026gt; { let mut worker = Worker { args: args.clone(), inpbuf: args.input_buffer(), grep: args.grep(), match_count: 0, }; // ... for dent in args.walker() { // ... if dent.is_stdin() { worker.do_work(\u0026amp;mut printer, WorkReady::Stdin); } else { let file = match File::open(dent.path()) { Ok(file) =\u0026gt; file, Err(err) =\u0026gt; { eprintln!(\u0026#34;{}: {}\u0026#34;, dent.path().display(), err); continue; } }; worker.do_work(\u0026amp;mut printer, WorkReady::DirFile(dent, file)); } } // ... } As you can see, the function uses a single Worker and if you remember, this struct is used by MultiWorker too. The files to search are iterated by args.walker() as before and each entry is passed to the worker, as before. The use of Worker in both cases allows code reuse to a great extent.\nThe file listing We are now going to look over the file listing functional block.\nThe default operation mode of ripgrep is to search recursively for non-binary, non-ignored files starting from the current directory (or from the user specified paths). To enumerate the files and feed the search engine, ripgrep uses the ignore crate.\nBut let\u0026rsquo;s start from the beginning: the walker function. It returns a Walk instance, it is constructed by Args and used by the run function in main:\npub fn walker(\u0026amp;self) -\u0026gt; Walk; Walk is just a simple wrapper around the ignore::Walk struct. A value of this struct can be created by using its new method:\npub fn new\u0026lt;P: AsRef\u0026lt;Path\u0026gt;\u0026gt;(path: P) -\u0026gt; Walk; or with a WalkBuilder, that implements the builder pattern. This allows to customize the behavior without annoying the users of the library, since it frees them from the burden to provide a lot of parameters to the constructor, when just the default values are needed:\nlet w = WalkBuilder::new(path).ignore(true).max_depth(Some(5)).build(); In this example we have created a WalkBuilder with default arguments and just override the ignore and max_depth options.\nThe implementation of the type is not very interesting from our point of view. It is basically an Iterator that walks through the file system by using the walkdir crate, but ignores the files and directories listed in .gitignore and .ignore files possibly present, with the help of the Ignore type. We\u0026rsquo;ll look at that type a bit later. Let\u0026rsquo;s look at the Error type first:\n/// Represents an error that can occur when parsing a gitignore file. #[derive(Debug)] pub enum Error { Partial(Vec\u0026lt;Error\u0026gt;), WithLineNumber { line: u64, err: Box\u0026lt;Error\u0026gt; }, WithPath { path: PathBuf, err: Box\u0026lt;Error\u0026gt; }, Io(io::Error), Glob(String), UnrecognizedFileType(String), InvalidDefinition, } This error type has an interesting recursive definition. The Partial case of the enumeration contains a vector of Error instances, for example. WithLineNumber adds line information to an Error.1 Then the error::Error, fmt::Display and Fromio::Error traits are implemented, to make it a proper error type and to easily construct it out an io::Error. Here, the necessary boilerplate to crank up the error type are handcrafted. Another possibility could have been to use the quick-error macro, which reduces the burden to implement error types to a minimum.2\nIgnore patterns Ignore patterns are handled within the ignore crate by the Ignore struct. This type connects directory traversal with ignore semantics. In practice it builds a tree-like data structure that mimics the directories tree, in which nodes are ignore contexts. The implementation is quite complicated, but let\u0026rsquo;s give it a brief look:3\n#[derive(Clone, Debug)] pub struct Ignore(Arc\u0026lt;IgnoreInner\u0026gt;); #[derive(Clone, Debug)] struct IgnoreInner { compiled: Arc\u0026lt;RwLock\u0026lt;HashMap\u0026lt;OsString, Ignore\u0026gt;\u0026gt;\u0026gt;, dir: PathBuf, overrides: Arc\u0026lt;Override\u0026gt;, types: Arc\u0026lt;Types\u0026gt;, parent: Option\u0026lt;Ignore\u0026gt;, is_absolute_parent: bool, absolute_base: Option\u0026lt;Arc\u0026lt;PathBuf\u0026gt;\u0026gt;, explicit_ignores: Arc\u0026lt;Vec\u0026lt;Gitignore\u0026gt;\u0026gt;, ignore_matcher: Gitignore, git_global_matcher: Arc\u0026lt;Gitignore\u0026gt;, git_ignore_matcher: Gitignore, git_exclude_matcher: Gitignore, has_git: bool, opts: IgnoreOptions, } The Ignore struct is a wrapper around an atomic reference counter to the actual data (namely, the IgnoreInner). A first interesting field inside that struct is parent, that is an Option\u0026lt;Ignore\u0026gt;. It points to a parent entry if present. So, this is where the tree structure comes from: the Arc can be shared, so multiple Ignore can share the same parent. But that\u0026rsquo;s not all; they can also be cached in the compiled field, that has a quite complex type:\nArc\u0026lt;RwLock\u0026lt;HashMap\u0026lt;OsString, Ignore\u0026gt;\u0026gt;\u0026gt; This is the cache of Ignore instances that is shared among all of them. Let\u0026rsquo;s try to break it down:\nthe HashMap maps paths to Ignore instances (as expected); the RwLock allows the map to be shared and modified across different threads, without causing data races; and finally the Arc allow the cache to be owned safely by different owners in different threads. Every time a new Ignore instance has to be built and added to a tree, the implementation first looks in the cache, trying to reuse the existing instances. The tree is built dynamically, while crawling the directories, looking for the specific ignore files (e.g. .gitignore, .ignore, or .rgignore). The tree gets also custom ignore patterns from the command line, and adds them to the tree too.\nAnother interesting bit here is the add_parents signature for Ignore:\npub fn add_parents\u0026lt;P: AsRef\u0026lt;Path\u0026gt;\u0026gt;(\u0026amp;self, path: P) -\u0026gt; (Ignore, Option\u0026lt;Error\u0026gt;); Instead of returning a Result\u0026lt;Ignore, Error\u0026gt;, it returns a pair, that contains always a result and optionally an error. In this way partial failures are allowed. If you remember, the error value can also be a vector of errors, so the function can collect them while working, but then it can also return a (maybe partial) result in the end. I found this approach very interesting.\nThe search process In this section we will look at how the regex search inside a file is implemented. This process involves some modules in ripgrep and also the grep crate.\nEverything starts from Worker::do_work in main.rs. Based on the type of the file passed in, it calls search or search_mmap. The first function is used to read the input one chunk at a time and then search, while the second is used to search into a memory mapped input. In this case there is no need to read the file into a buffer, because it is already available in memory, or more precisely, the kernel will take care of this illusion.\nThe search function just creates a new Searcher and calls run on it.\nimpl\u0026lt;\u0026#39;a, R: io::Read, W: Terminal + Send\u0026gt; Searcher\u0026lt;\u0026#39;a, R, W\u0026gt; { pub fn run(mut self) -\u0026gt; Result\u0026lt;u64, Error\u0026gt;; } The first interesting thing to note here is that the run function actually consumes self, so you can\u0026rsquo;t actually run the method twice. Why is that? Let\u0026rsquo;s have a look at the new method, that creates this struct:\nimpl\u0026lt;\u0026#39;a, R: io::Read, W: Terminal + Send\u0026gt; Searcher\u0026lt;\u0026#39;a, R, W\u0026gt; { pub fn new(inp: \u0026amp;\u0026#39;a mut InputBuffer, printer: \u0026amp;\u0026#39;a mut Printer\u0026lt;W\u0026gt;, grep: \u0026amp;\u0026#39;a Grep, path: \u0026amp;\u0026#39;a Path, haystack: R) -\u0026gt; Searcher\u0026lt;\u0026#39;a, R, W\u0026gt;; } It takes a bunch of arguments and stores them into a new Searcher instance. All the arguments to Searcher are passed as reference, except haystack which is the Read stream representing the file. This means that when this struct will be destroyed, the file will be gone too. Whenever you complete the search for a file, you don\u0026rsquo;t have to do it again, indeed. You can enforce this usage by consuming the input file in the run function, or take its ownership in the constructor and force the run function to consume self.\nSince we cannot run the search twice using the same Searcher instance, why don\u0026rsquo;t we just use a function then? The approach used here has several advantages:\nyou get the behavior that the search cannot be run twice with the same file (but that\u0026rsquo;s nothing that a free function could not do); you can split the function among different private functions, without passing around all the arguments; they will all take self by reference (maybe also \u0026amp;mut self) and just use the member variables. So, instead of:\nfn helper1(inp: \u0026amp;mut InputBuffer, printer: \u0026amp;mut Printer\u0026lt;W\u0026gt;, grep: \u0026amp;Grep, path: \u0026amp;Path, haystack: \u0026amp;mut R) { // do something with path, grep, etc } we have:\nfn helper1(\u0026amp;mut self) { // do something with self.path, self.grep, etc } The end result is much nicer.\nThe first variable that the Searcher takes is an InputBuffer. It is defined in the same search_stream module, and it provides buffering for the input file. It has the interesting feature to be able to keep part of the data across reads. This is needed, for example, when the user requests context lines, or when a single read is not enough to reach the next end of line.\nThe fill function in the InputBuffer, reads from the input and optionally rolls over the contents of the buffer starting from the keep_from index:\nfn fill\u0026lt;R: io::Read\u0026gt;(\u0026amp;mut self, rdr: \u0026amp;mut R, keep_from: usize) -\u0026gt; Result\u0026lt;bool, io::Error\u0026gt;; The interesting implementation bit here is that the buffer grows whenever it needs more room, but it never shrinks. This avoids some re-allocations, at the expense of some memory. This approach is perfectly fine in this case, since the application is intended to work in one shot and then terminate. In a long running application such as a webserver, this is probably not what you want to do.\nAfter the buffer has been filled, the Grep matcher runs, and in case of a match, it prints the results according to the options (context lines, line numbers, etc.).\nNote that Searcher takes the input buffer by mutable reference. This means that it can be reused for the next file, without allocating new memory for the buffer with a new Searcher instance.\nI\u0026rsquo;ll be skipping most of the implementation review here, even if the code may be interesting. Most of it however is not very relevant outside this specific case. If you are interesting you can skim through the search_stream module code.\nThe other case is covered by the search_mmap function, that creates a BufferSearcher, defined in the search_buffer module, and calls run on it, like in the Searcher case:\nimpl\u0026lt;\u0026#39;a, W: Send + Terminal\u0026gt; BufferSearcher\u0026lt;\u0026#39;a, W\u0026gt; { pub fn run(mut self) -\u0026gt; u64; } The same reasoning applies here: the struct is created and used only once for one file, because the run function takes self by value. The purpose of the search_buffer module is to search inside a file completely contained in a single buffer, instead of a stream. This buffer is provided by a memory mapped file, and it\u0026rsquo;s used only when a stream would be slower.4 This module reuses some types provided by the search_stream module:\nuse search_stream::{IterLines, Options, count_lines, is_binary}; Notably, it does not use the InputBuffer, since there is nothing to buffer here: everything is already available in the given array. The implementation is very basic, and it doesn\u0026rsquo;t support some of the features the other module does (like showing context lines).\nNo big surprises here. The only minor weak point for me is that this module depends on the search_stream one. It doesn\u0026rsquo;t actually build on top of it, but it just imports some functionality. I\u0026rsquo;d rather try to move the common implementation in another module from which they can both import. This makes sense, since the common stuff is indeed not specific to either of the modules.\nThe grep crate The grep crate provides all you need to regex search into a line. It builds on top of the Rust regex crate and adds some optimizations in the literal module. The result of a search is a Match instance, which is simply a position inside that buffer:\n#[derive(Clone, Debug, Default, Eq, PartialEq)] pub struct Match { start: usize, end: usize, } The Grep type is cloneable. This is important, since it can be built once (which is an expensive operation) and then cloned to all the worker threads:\n#[derive(Clone, Debug)] pub struct Grep { re: Regex, required: Option\u0026lt;Regex\u0026gt;, opts: Options, } I won\u0026rsquo;t dig into the implementation details, since they are already very well covered in the already mentioned Andrew\u0026rsquo;s blog post.\nOutput handling The last bit we are going to investigate now is the output handling. The challenge here is that ripgrep needs to write from multiple threads to a single console avoiding to interleave the results.\nHere is how the run function in our MultiWorker handles that:\nlet mut outbuf = self.outbuf.take().unwrap(); outbuf.clear(); let mut printer = self.worker.args.printer(outbuf); self.worker.do_work(\u0026amp;mut printer, work); // ... let outbuf = printer.into_inner(); if !outbuf.get_ref().is_empty() { let mut out = self.out.lock().unwrap(); out.write(\u0026amp;outbuf); } self.outbuf = Some(outbuf); An output buffer is taken from self and passed to a printer. The printer is then passed to the worker, that uses it to print the results. So far all the output went to the buffer, and not to the actual console. Then, if anything has been buffered, lock the output, that is shared across all the workers, and write everything. The output buffer is reused in this interesting way: it is kept as an Option field inside the MultiWorker itself. For every file, it is taken from the option, passed by value to a Printer, and then when the Printer is done, put it back in the Option. This allows to keep it mutable and pass it around by value without creating it every time.\nThe trick used here, to avoid to interleave the prints, is to buffer all the matches found in a file into a \u0026ldquo;virtual terminal\u0026rdquo; that doesn\u0026rsquo;t print to the console. After the search in that file is done, the output is written in one shot, by locking a shared Out object and write the buffer contents to the actual console.\nLet\u0026rsquo;s take a look at the various types involved. The MultiWorker keeps a ColoredTerminal instance in its self.outbuf field. Its type depends on the platform:\n#[cfg(not(windows))] outbuf: Option\u0026lt;ColoredTerminal\u0026lt;term::TerminfoTerminal\u0026lt;Vec\u0026lt;u8\u0026gt;\u0026gt;\u0026gt;\u0026gt;, #[cfg(windows)] outbuf: Option\u0026lt;ColoredTerminal\u0026lt;WindowsBuffer\u0026gt;\u0026gt;, The self.out is the same in all the platforms:\nlet out: Arc\u0026lt;Mutex\u0026lt;Out\u0026gt;\u0026gt;; As you can see, it can be shared and mutated by multiple threads, because it is wrapped in a Mutex and an Arc. Inside an Out instance, there is the terminal used to write directly to the console:\n#[cfg(not(windows))] let term: ColoredTerminal\u0026lt;term::TerminfoTerminal\u0026lt;io::BufWriter\u0026lt;io::Stdout\u0026gt;\u0026gt;\u0026gt;; #[cfg(windows)] let term: ColoredTerminal\u0026lt;WinConsole\u0026lt;io::Stdout\u0026gt;\u0026gt;; A ColoredTerminal that refers to a TerminfoTerminal on Linux, and to a WinConsole on Windows. They are both structs defined in the term crate.\nBut let\u0026rsquo;s step back a and describe all these types a little bit better. The Searcher uses a Printer whenever a match is found and the output is enabled. The Printer is defined in the printer module and it encapsulates the general output logic. It knows how to print a match, given some options, and forwards the writes to an inner Terminal type.\npub struct Printer\u0026lt;W\u0026gt; { wtr: W, has_printed: bool, column: bool, context_separator: Vec\u0026lt;u8\u0026gt;, eol: u8, file_separator: Option\u0026lt;Vec\u0026lt;u8\u0026gt;\u0026gt;, heading: bool, line_per_match: bool, null: bool, replace: Option\u0026lt;Vec\u0026lt;u8\u0026gt;\u0026gt;, with_filename: bool, color_choice: ColorChoice } Note that I took the comments out to make it shorter. As you can see, there is a generic writer W (that is taken by value) and a lot of other options. This generic parameter is expected to implement term::Terminal and Send, as you can see in the implementation:\nimpl\u0026lt;W: Terminal + Send\u0026gt; Printer\u0026lt;W\u0026gt; { // printer implementation } The struct uses the builder pattern again, but in a slightly different flavor. The new method takes only a Terminal and sets all the options with a default value. To change them, the user needs to call the various builder methods, directly on the Printer itself, not on another builder helper. For example:\npub fn heading(mut self, yes: bool) -\u0026gt; Printer\u0026lt;W\u0026gt; { self.heading = yes; self } takes self by mutable value and, after changing the heading option, returns self by value again.\nThe implementation is simple. The public interface provides some methods to print the various match components, like the path, the context separator and the line contents. The only thing that is still not clear to me is why the Send trait is also needed, since I don\u0026rsquo;t see any threading in the struct implementation, and all the print methods require a mutable self, e.g.:\npub fn context_separate(\u0026amp;mut self) { // N.B. We can\u0026#39;t use `write` here because of borrowing restrictions. if self.context_separator.is_empty() { return; } self.has_printed = true; let _ = self.wtr.write_all(\u0026amp;self.context_separator); let _ = self.wtr.write_all(\u0026amp;[self.eol]); } In any case, the implementation is more or less straight forward, and in the end all the writes are directed to the inner Terminal.\nIn the Linux case, the Terminal is the default one provided by the term crate itself: TerminfoTerminal. On Windows ripgrep provides a custom implementation, since the coloring needs a special treatment, to avoid performance hurt:\nThis particular implementation is a bit idiosyncratic, and the \u0026#34;in-memory\u0026#34; specification is to blame. In particular, on Windows, coloring requires communicating with the console synchronously as data is written to stdout. This is anathema to how ripgrep fundamentally works: by writing search results to intermediate thread local buffers in order to maximize parallelism. Eliminating parallelism on Windows isn\u0026#39;t an option, because that would negate a tremendous performance benefit just for coloring. We\u0026#39;ve worked around this by providing an implementation of `term::Terminal` that records precisely where a color or a reset should be invoked, according to a byte offset in the in memory buffer. When the buffer is actually printed, we copy the bytes from the buffer to stdout incrementally while invoking the corresponding console APIs for coloring at the right location. The implementation is provided by WindowsBuffer:\n/// An in-memory buffer that provides Windows console coloring. #[derive(Clone, Debug)] pub struct WindowsBuffer { buf: Vec\u0026lt;u8\u0026gt;, pos: usize, colors: Vec\u0026lt;WindowsColor\u0026gt;, } /// A color associated with a particular location in a buffer. #[derive(Clone, Debug)] struct WindowsColor { pos: usize, opt: WindowsOption, } /// A color or reset directive that can be translated into an instruction to /// the Windows console. #[derive(Clone, Debug)] enum WindowsOption { Foreground(Color), Background(Color), Reset, } This struct implements terminfo::Terminal as we said before, and it contains a buffer of characters to print, a position on the buffer itself, and a vector of colors and positions. Whenever the write is called, the output is buffered in self.buf:\nimpl io::Write for WindowsBuffer { fn write(\u0026amp;mut self, buf: \u0026amp;[u8]) -\u0026gt; io::Result\u0026lt;usize\u0026gt; { let n = try!(self.buf.write(buf)); self.pos += n; Ok(n) } fn flush(\u0026amp;mut self) -\u0026gt; io::Result\u0026lt;()\u0026gt; { Ok(()) } } and whenever a coloring option is passed, it is pushed into the colors vector, along with the current position:\nimpl Terminal for WindowsBuffer { type Output = Vec\u0026lt;u8\u0026gt;; fn fg(\u0026amp;mut self, fg: Color) -\u0026gt; term::Result\u0026lt;()\u0026gt; { self.push(WindowsOption::Foreground(fg)); Ok(()) } // ... } Then, when the higher level logic decides it\u0026rsquo;s time to print everything, the print_stdout is called, passing another terminal (the real one, linked with the console):\n/// Print the contents to the given terminal. pub fn print_stdout\u0026lt;T: Terminal + Send\u0026gt;(\u0026amp;self, tt: \u0026amp;mut T) { if !tt.supports_color() { let _ = tt.write_all(\u0026amp;self.buf); let _ = tt.flush(); return; } let mut last = 0; for col in \u0026amp;self.colors { let _ = tt.write_all(\u0026amp;self.buf[last..col.pos]); match col.opt { WindowsOption::Foreground(c) =\u0026gt; { let _ = tt.fg(c); } WindowsOption::Background(c) =\u0026gt; { let _ = tt.bg(c); } WindowsOption::Reset =\u0026gt; { let _ = tt.reset(); } } last = col.pos; } let _ = tt.write_all(\u0026amp;self.buf[last..]); let _ = tt.flush(); } Here, if the terminal does not support coloring, there is nothing special to do, and all the buffer contents are written. Otherwise, for every color option, it writes the buffer contents until the recorded position for that option, and than it applies the option. This is repeated until the end of the buffer.\nThe terminal is not used as is by the higher level logic, but wrapped inside a ColoredTerminal instance:\n#[derive(Clone, Debug)] pub enum ColoredTerminal\u0026lt;T: Terminal + Send\u0026gt; { Colored(T), NoColor(T::Output), } The purpose of this type is simple: determine if the current terminal supports coloring, and if so use it. If not, just drop the terminal and use its internal writer type. Determine color support is a costly operation, so it\u0026rsquo;s done only once, and the result is cached in a static variable, with the help of the lazy_static crate:\nlazy_static! { // Only pay for parsing the terminfo once. static ref TERMINFO: Option\u0026lt;TermInfo\u0026gt; = { match TermInfo::from_env() { Ok(info) =\u0026gt; Some(info), Err(err) =\u0026gt; { debug!(\u0026#34;error loading terminfo for coloring: {}\u0026#34;, err); None } } }; } The type then implements some specialized constructors for a bunch of types:\nWindowsBuffer; WinConsole\u0026lt;io::Stdout\u0026gt;; and the one for the generic writer W: io::Write + Send. If the terminal then supports coloring, it uses the Colored(T) enum value (where T is T: Terminal + Send). In this case the ColoredTerminal instance contains a Terminal. In the other case, the NoColor(T::Output) value is selected and a plain io::Write is used. ColoredTerminal then implements Terminal itself in this way:\nimpl\u0026lt;T: Terminal + Send\u0026gt; term::Terminal for ColoredTerminal\u0026lt;T\u0026gt; { type Output = T::Output; fn fg(\u0026amp;mut self, fg: term::color::Color) -\u0026gt; term::Result\u0026lt;()\u0026gt; { self.map_result(|w| w.fg(fg)) } // other very similar implementations... } The intended behavior here is to forward the function to the inner terminal, if present, or return an error. A possible solution would have been to match self in this way:\nmatch *self { ColoredTerminal::Colored(ref mut w) =\u0026gt; w.fg(fg), ColoredTerminal::NoColor(_) =\u0026gt; Err(term::Error::NotSupported), } for all the functions. The solution adopted here is more elegant, as it Implements a map_result that applies the given function to the inner Terminal if it\u0026rsquo;s present and returns an error otherwise:\nimpl\u0026lt;T: Terminal + Send\u0026gt; ColoredTerminal\u0026lt;T\u0026gt; { fn map_result\u0026lt;F\u0026gt;(\u0026amp;mut self, mut f: F) -\u0026gt; term::Result\u0026lt;()\u0026gt; where F: FnMut(\u0026amp;mut T) -\u0026gt; term::Result\u0026lt;()\u0026gt; { match *self { ColoredTerminal::Colored(ref mut w) =\u0026gt; f(w), ColoredTerminal::NoColor(_) =\u0026gt; Err(term::Error::NotSupported), } } } In this way the whole Terminal implementation is just a bunch of one-liners.\nThe missing piece of this puzzle is the Out struct. The comment on top of the struct speaks for itself:\n/// Out controls the actual output of all search results for a particular file /// to the end user. /// /// (The difference between Out and Printer is that a Printer works with /// individual search results where as Out works with search results for each /// file as a whole. For example, it knows when to print a file separator.) pub struct Out { #[cfg(not(windows))] term: ColoredTerminal\u0026lt;term::TerminfoTerminal\u0026lt;io::BufWriter\u0026lt;io::Stdout\u0026gt;\u0026gt;\u0026gt;, #[cfg(windows)] term: ColoredTerminal\u0026lt;WinConsole\u0026lt;io::Stdout\u0026gt;\u0026gt;, printed: bool, file_separator: Option\u0026lt;Vec\u0026lt;u8\u0026gt;\u0026gt;, } The implementation is straightforward: whenever write is called with a ColoredTerminal as a buffer, it prints a separator (except for the first file), then prints the buffer contents and then flushes the terminal. Here is the Unix version:\n#[cfg(not(windows))] pub fn write(\u0026amp;mut self, buf: \u0026amp;ColoredTerminal\u0026lt;term::TerminfoTerminal\u0026lt;Vec\u0026lt;u8\u0026gt;\u0026gt;\u0026gt;) { self.write_sep(); match *buf { ColoredTerminal::Colored(ref tt) =\u0026gt; { let _ = self.term.write_all(tt.get_ref()); } ColoredTerminal::NoColor(ref buf) =\u0026gt; { let _ = self.term.write_all(buf); } } self.write_done(); } A very similar but not exactly equal version is provided for Windows, so there is some code duplication. It would be better to abstract these details in ColoredTerminal, providing a write_all method there, or in alternative, to introduce a new trait used by ColoredTerminal itself that does the same and than make TerminfoTerminal, WindowsBuffer and WindowsConsole to implement it.\nConcluding remarks In this post we have done a ripgrep code review, with the main focus on the design decisions and the interesting implementation solutions. The review is far from being complete, but my goal was to look at the patterns and break them down, in hope that they can be used in similar contexts by other projects.\nIn general the code is very clean, a part certain functions that would benefit from some more comments. There is however an extensive usage of #[inline(always)] and #[inline(never)] directives in the code, that I could not explain. I wonder if they have been added after profiling and if so, why the compiler have failed to identify them correctly. A possible use case is intra-crate inlining, but compiling with rustc -C lto already allows to inline everything (by slowing down compilation).5\nIn any case, I found the ripgrep crate a beautiful piece of software, from which I could learn a lot. I hope I was able to convey this beauty with this post.\nFeedback Andrew posted his feedback on Twitter and on HN. I report his comment here, because it\u0026rsquo;s relevant for some of the remarks I made:\nripgrep author here! This is a great review, thanks for doing it! I\u0026rsquo;d like to respond to a few of the bad things pointed out. :P\nThe search code is indeed in a less than ideal state. I\u0026rsquo;ve mostly avoided refactoring it because I want to move it to its own separate crate. I\u0026rsquo;ve been steadily doing this for other things. Namely, ripgrep used to be a single main crate plus a small regex handling crate (grep), but now it\u0026rsquo;s several: globset, grep, ignore, termcolor and wincolor. I\u0026rsquo;d like to roll the search code into the grep crate so that others can use it. Once that\u0026rsquo;s done, ripgrep proper will be a pretty small, limited mostly to argv handling and output handling.\nI do sometimes get overzealous with inline(always) and inline(never). Both are almost always a result of trying things while profiling, and then forgetting to remove them. If you look closely, most of them are in the core searching code where performance is quite important!\nFinally, this code review was done while I was in the middle of moving more of ripgrep code out into the `ignore` and `termcolor` crates. The `ignore` crate does all the gitignore handling (which is quite tricky and is now being used by the tokei project) and provides a parallel recursive directory iterator, which made ripgrep even faster! The `termcolor` crate handles cross platform coloring shenanigans, including Windows consoles and mintty. It wasn\u0026rsquo;t fun: issue #94… — The author did a great job reviewing the previous solution I used for colors though, and was something I really wasn\u0026rsquo;t proud of!\n— burntsushi@ on HN\nI have corrected a typo, thanks @toquetos.\nThere is some discussion going on in the r/rust subreddit, and on HN. Thank you guys for the feedback and the kind words, and thanks to Andrew for his great work.\nThat\u0026rsquo;s all folks.\nIn this case Box\u0026lt;Error\u0026gt;, since a recursive type cannot embed itself, otherwise it would be impossible to compute the size of the type.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nYou can find a good reference on the error handling topic in the Rust book.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nPlease bear in mind that I have taken out the comments to make it shorter.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nGenerally this happens when searching into a single huge file.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nSee When should I use inline.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://blog.mbrt.dev/posts/ripgrep/","summary":"I\u0026rsquo;ve been playing around with Rust for a year and a half, and the best part of it, like many others say, has been the very helpful community. There are a lot of online resources that help you to get started: the Rust book, the Rustonomicon and many blog posts and stack overflow questions. After I learned the basics I felt a bit lost though. I couldn\u0026rsquo;t find enough resources for intermediate-level-Rustaceans.","title":"Ripgrep code review"},{"content":"This was published in the Diary of a reverse-engineer as a guest post:\nKeygenning with KLEE\n","permalink":"https://blog.mbrt.dev/posts/keygen/","summary":"This was published in the Diary of a reverse-engineer as a guest post:\nKeygenning with KLEE","title":"Keygenning with KLEE [doar-e]"}]