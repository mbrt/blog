[{"content":"I processed a large fraction of the Internet on a single machine, trying to find some valuable but forgotten domain names to buy (probably 25 years too late). Here\u0026rsquo;s the story of what I found and how I did it, and not the story of why I decided to try, because I don\u0026rsquo;t know myself.\nSpoiler alert: If you just want to know whether there is indeed something, and you should scramble to scoop it up to become a domain-flipper billionaire, hold your horses: I believe there isn\u0026rsquo;t much. The following sections are more focused on the engineering side, so if you only care about the conclusion, skip to Results.\nA valuable domain Domain names have a fixed registration period. They have to be periodically renewed, and sometimes they aren\u0026rsquo;t. The question was: is there some domain, among those forgotten, that is worth resurrecting?\nBut first, what makes a domain valuable? I asked a few people, and the first answer they came up with was this: a memorable or short name, or attached to a known brand, or something that rings cool.\nI believe this is too subjective. For example, food.com is arguably a good sounding domain, but what about food.de.com? Or asfood.com? Why is the latter estimated to cost $76000, while the former just $22/year? It\u0026rsquo;s not just the .com top level domain.\nI thought that a more quantitative (and therefore easier) way to derive the value of a domain would be to estimate its position in search results ranking.\nThe idea My idea was to compute Page Rank for every domain (assuming it\u0026rsquo;s a good proxy for their value), and use it to sort candidate expired domains. So, I started with one problem, and ended up with two:\nHow to calculate page ranks of the entire Internet. How to identify expired domains. There\u0026rsquo;s no direct way to get search scores from Google, so I thought to approximate that by computing Page Rank myself. Although not really in use anymore, it powered the early Google and made it extremely successful, before being replaced by the zoo of algorithms, heuristics, models and manual curation of today. I thought it should serve as a good base.\nAnd here\u0026rsquo;s where waiting 25 years helps, because thanks to Common Crawl, we don\u0026rsquo;t need to crawl anything. We have monthly datasets with most of the web, crawled and pre-processed for us, for free. Problem number one becomes: \u0026ldquo;how to calculate PageRank over that dataset\u0026rdquo;, which is a much easier problem to solve.\nFor the second problem (identifying expired domains), we could think of comparing several snapshots of Common Crawl, and look for domains that disappeared, but that\u0026rsquo;s expensive. There\u0026rsquo;s indeed an easier way: look at which domains are being referenced in links, but they themselves are not in the dataset. 1\nSingle machine? The Internet is big. Even though Common Crawl doesn\u0026rsquo;t have all of it, its latent snapshot is currently counting 421 TiB of content. Something of this size is not impossible to handle, but also not cheap. Processing it sequentially at 1 Gbit/s (which is not particularly slow) would take 42 days.\nFortunately, we can avoid most of this data and focus on the WAT subset, which is 76 TiB (16 TiB compressed) of JSON metadata about pages and their links.\nMore than answering the original question, then I became interested in the technical challenge: could I tackle the problem with a single machine in reasonable time? And it turns out the answer was yes.\nAfter keeping 32 cores 100% busy for about 13h, downloading 16 TiB of data, processing 7+ billion pages and 83+ billion links [logs, infrastructure], I got my answer in the form of less than 2 GiB of parquet files: a list of domain names, sorted by rank.\nBut I\u0026rsquo;m getting ahead of myself. How did I know that this had a chance to work? I tried to work it out from first principles by estimating resource usage and time: network bandwidth, memory, CPU and disk.\nNetwork The first problem was how to download 16 TiB of data. If this were to go through a slow link from the Internet, I would have been worried, but Common Crawl datasets are also in S3. A VM in the same region can download objects in parallel to the point of saturating its network bandwidth. Staying between 1 and 10 Gbit/s would take this process between 4h and 40h, which seemed acceptable.\nWith this first constraint, I got the first design decision down: it had to run in AWS from us-east-1 (where the crawls are located).\nDisk space The dataset contains about 39 million domains.2 Accounting for an average of 100 bytes per domain (to store the name), this would be about 3.5 GiB uncompressed (which I think is a generous upper bound).\nFor my purposes (domain reputation), I didn\u0026rsquo;t need to store, nor keep track of individual pages, but just domain names and their link counts. A list of triplets: source_domain, target_domain, count is the simplest way. And if each domain had links to an average of 50 other domains, this would result in 180 GiB of data. With compression, this was definitely going to be lower, and if the estimate was too low, I could store the names just once and have the list of edges as a table of integer indexes, making it at least one order of magnitude smaller.\nIf I processed the data in a streaming fashion from S3 to a table stored locally, it would result in some denormalization (because pages are processed in parallel and the dataset is not ordered), inflating the numbers above. Adding a 10x penalty results in 1.8 TiB of data, but since this is mostly ASCII text, we can account for a 70 - 80% compression, which brings it down to about 500 GiB.\nAll of this sounded manageable on a single disk / volume. Second decision down: stream the results to a local disk is good enough (no need for distributed storage).\nMemory The first part of the process (creating the edges table) can be done in streaming, so I had no memory usage concerns. Aggregating over the table can be memory intensive, but could be done off-core too (paying the penalty of spilling to disk). Page rank is parallelizable (as it runs in a distributed way), so it can also be turned into a sharded off-core computation.\nThis opened a sub-problem: how to compute aggregations over the dataset while spilling to disk?\nDisk speed Running Page Rank over multiple iterations would mean reading and writing the whole dataset several times, and 100 GiB of data written 10 times is about 1 TiB. EBS supports up to 1 GiB/s throughput, and even the default 125 MiB/s would make this possible in about 2h.\nSo, I wasn\u0026rsquo;t concerned about memory, nor disk.\nCPU This was the least concerning to me, as the problem didn\u0026rsquo;t seem to need any complex computation: decompress files, parse JSONL, turn URLs into domain names and count.\nWell, here I was wrong, as I underestimated the effort required by decompressing large gzip files (i.e. FLATE), which turned out to be the actual bottleneck.\nI found this out early while running a smaller scale experiment locally (with a handful of WAT files), so I had to adjust things slightly before the actual full run. See Mapper implementation below for more details.\nImplementation The informal feasibility check gave me the feeling that the bottleneck would be streaming the 16 TiB from S3, as the rest was orders of magnitude smaller. Optimizing for that would mean to stream process the data, one thread per core and store it on disk denormalized (to keep memory usage low). After that, compute PageRank one iteration at a time, while check pointing intermediate steps on disk.\nBased on that, I split the problem into two independent parts:\nA mapper, responsible for processing the Common Crawl WAT dataset and writing a set of parquet files with counters for links going from one domain to another (the edges). A reducer: which aggregates the edges (into nodes) and iteratively computes PageRank. Mapper The first implementation of the mapper was a Python script which used a ProcessPoolExecutor to process WAT files in parallel. Very simple and readable implementation, but while testing locally, to my surprise I discovered that it was terribly slow [see profile]. The code would take about a minute to process a single WAT object, which would translate to roughly 16000 CPU hours, given the 100k total files in the dataset.\nThe culprit was mostly domain name computation (eTLD+1), and after trying to replace it in many different ways (e.g. adding heuristics, or using fasttld and seeing a large drop in extracted domains) I decided to give up and rewrite the thing in Go.\nI thought this would be a good match for the language: strong standard library, especially for the web and networking domains, and fast concurrency. Unfortunately, Go doesn\u0026rsquo;t have an equivalent to the awesome fsspec library, so I had to roll my own, to abstract away how to read from a certain URL (local path, http or s3 URL). Another thing I wasn\u0026rsquo;t happy with was the support for good estimation of speed and completion time, so I rolled my own as well by using an exponentially weighted moving average (EWMA) and some prettier formatting.\nAlthough that was much faster (down from 60s to 9s per file), there was still low-hanging fruit. Looking at the CPU profile, I noticed a large amount of time spent in JSON parsing, and I suddenly remembered how slow that is in Go (it\u0026rsquo;s based on reflection and severely under-optimized). So I swapped it for bytedance/sonic, which is a drop-in replacement, and saw another 30% speed up. Down to 6s per file, I was projecting the map phase to finish in 160 CPU hours, which would be about 5h on a 32 cores machine.\nI was ready to start working on the reducer.\nReducer The reducer has a very simple job: take the denormalized input from the previous phase and aggregate it into unique rows of source_domain, target_domain, count on a set of parquet files. This is the type of thing that Polars should eat for breakfast [ref]:\nmap_df = pl.scan_parquet(tmp / \u0026#34;part_*.parquet\u0026#34;) edges = ( map_df.group_by(\u0026#34;src_dom\u0026#34;, \u0026#34;tgt_dom\u0026#34;) .agg(pl.col(\u0026#34;count\u0026#34;).sum().alias(\u0026#34;count\u0026#34;)) ) edges.sink_parquet(out / \u0026#34;edges.parquet\u0026#34;, compression=\u0026#34;snappy\u0026#34;) But it turned out that out-of-core operations (i.e. swapping to disk) were not yet supported in the new streaming engine (pola-rs/polars#20947). On a 250 GiB dataset (compressed), this was not going to fly, and indeed, on my first run, memory usage exploded and the whole thing crashed.\nAfter several experiments, I was able to simulate out-of-core aggregations by:\nProcessing one map partition at a time Splitting each one into a set of smaller shards, partitioned by src_dom. This made it so that all rows with the same source domain are in the same partition. Aggregate shards with the same partition key together, one by one. This allowed splitting up the larger computation into sequentially independent steps, where I control the size of each. A map reduce could process this in parallel faster, but I just decided to process it sequentially, so it could fit on a single machine.\nPage Rank The second phase of the reducer computes Page Rank. We\u0026rsquo;ll see in a minute how this turned out to be insufficient, but it\u0026rsquo;s good to understand the basic version before complicating it.\nThe original paper from Page and Brinn describes the intuition behind it quite well:\nA page has a high rank if the sum of the ranks of its backlinks is high. This covers both the case when a page has many backlinks and when a page has a few highly ranked backlinks.\nThis reduces the problem of establishing the importance of pages to that of analyzing links, rather than contents, of those pages. The paper gives one more intuition on how to compute it, based on the \u0026ldquo;random surfer\u0026rdquo; model. The surfer picks a random page to start with and keeps clicking on random links until it \u0026ldquo;gets bored\u0026rdquo;, where it starts over from (i.e. \u0026ldquo;teleports to\u0026rdquo;) another random page.\nMore technically, the Page Rank of page P is the steady state probability that the random surfer is at page P. The more a page is ranked high, the higher the probability that a \u0026ldquo;random surfer\u0026rdquo; lands on that page. The formulation is very short:\nWhere:\nα is the damping factor (usually set at 0.85), which determines the probability of picking a random page over following outlinks. B(u) is the set of pages linking to page u directly. N is the total number of pages. The formula is iterative and proven to converge.\nGoing back to the intuitive explanation, the rank of a page is the combination of two factors: the \u0026ldquo;mass\u0026rdquo; of people navigating to it because they follow links in pages that point there, and the ones that just picked a random page. The combination is modulated by the damping factor α.3\nI introduced a small variation on that, given that I didn\u0026rsquo;t need the rank of every page, but the aggregate rank of their domains. In this setup, domains with large amounts of outlinks end up distributing rank equally to domains linked only once and linked thousands of times. To account for this skew, I divide contributions of a node by the total number of its outlinks. In this way, each single link counts as a \u0026ldquo;vote\u0026rdquo; for the target domains, and the more links point there, the more \u0026ldquo;votes\u0026rdquo; it gets.\nThe basic implementation of that in Polars is straightforward:\n# Contributions contribs = ( edges .join(scores, left_on=\u0026#34;src_dom\u0026#34;, right_on=\u0026#34;node\u0026#34;) .with_columns( ( pl.col(\u0026#34;score\u0026#34;) * pl.col(\u0026#34;count\u0026#34;) / pl.col(\u0026#34;out_weight\u0026#34;) ).alias(\u0026#34;contrib\u0026#34;) ) .group_by(\u0026#34;tgt_dom\u0026#34;) .agg(pl.sum(\u0026#34;contrib\u0026#34;).alias(\u0026#34;contrib\u0026#34;)) .rename({\u0026#34;tgt_dom\u0026#34;: \u0026#34;node\u0026#34;}) .select([\u0026#34;node\u0026#34;, \u0026#34;contrib\u0026#34;]) ) # New scores new_scores = ( scores .join(contribs, on=\u0026#34;node\u0026#34;, how=\u0026#34;left\u0026#34;) .with_columns( (damping * pl.col(\u0026#34;contrib\u0026#34;).fill_null(0.0) + (1 - damping) / N).alias(\u0026#34;score\u0026#34;) ) .cast({\u0026#34;out_weight\u0026#34;: pl.Int64}) .select([\u0026#34;node\u0026#34;, \u0026#34;out_weight\u0026#34;, \u0026#34;score\u0026#34;]) ) Off-core As with the edges reduce phase, this simple approach doesn\u0026rsquo;t work right out-of-the-box in Polars, given the lack of off-core computation. Again, I had to simulate it by shuffling contributions to the right shards and compute each one separately:\nThe actual code is here. I was ready to give this a try, so I let this run overnight, and got some surprises in the morning.\nPage Rank take II (variants) I knew the Internet was full of crap. I just didn\u0026rsquo;t know the extent of it. It\u0026rsquo;s literally, full - of - crap. After the first few obviously good results at the top of the ranking (google.com, facebook.com, etc.) a lot of dubious or outright bad domains came up. This is the predictable result of running a 25+ years old algorithm on today\u0026rsquo;s Internet. I thought I could ignore the problem, because I just needed to look at top results, but spam can rank high too!\nSee, for example, this analysis, where I try to understand why versiti.co and balsic-fastive.io rank so high in my run. It comes down to the fact of having two highly ranked domains (wikipedia.org and zendesk.com) linking to focusvision.com, which was then considered very highly ranked. This site was only partially crawled, so it had just a few links in it, and one turned out to be towards versiti.co.\nI had to descend a deeper rabbit hole on how to combat spam, but I didn\u0026rsquo;t need to remove all of it. I just needed to push it down enough for me to find some decent expired domain higher up. This was a much smaller problem to solve than the general case Google has to deal with.\nTo start, I needed to find a list of spam and trusted domains, and this is one area where being 25 years late went to my advantage, because there\u0026rsquo;s plenty of choice.\nAs a first set of attempts, I tried to implement some PageRank variations straight from the literature:\nTrustRank [Gyongyi, Z., et. al, 2004.], biases the teleport vector towards a curated set of trusted pages. Anti-Trust Rank [Krishnan, V. and Raj, R., 2006], starts from the opposite direction and demotes pages by looking at whether they point to a curated set of spam pages. MaxRank [Fercoq, O., 2012], modifies the graph by removing links going to spam, while minimizing a cost function that pays some cost on both landing on spam, and on removing links. I tried the first two on the full dataset and found them to perform quite badly. After checking some results empirically, I discovered two major problems:\nA lot of \u0026ldquo;trusted\u0026rdquo; domains point to spam (as well as good targets). A favorite trick from spammers and SEO sites is to \u0026ldquo;land\u0026rdquo; a link from a reputable place to their site. There are many ways to do this, but an easy one is to add a link pointing to a spam site in the comment section of a reputable site. Or edit a half forgotten Wikipedia article. Spam sites might point to both spam and non-spam. Spammers do this to muddy the waters, so it\u0026rsquo;s hard to propagate distrust from a small set of spam domains. Both spam and non-spam looks similar in that regard: they both point to some good and bad pages. A problem specific to Anti-Trust rank is that it doesn\u0026rsquo;t really take into consideration the proportion by which a domain is pointing to spam. The extreme example was Wikipedia, classified as the #3 most spammy domain on the Internet, just because in its 11M outlinks there\u0026rsquo;s some amount of spam. But this is to be expected in such a massive crowdsourced site.\nAlthough the first two algorithms are quite intuitive (the changes are simple and the papers are well written), I cannot say the same for the third (MaxRank). For one thing, it requires dynamic programming and a globally sorted \u0026ldquo;cost\u0026rdquo; vector, both of which will impact runtime. The second, and most important part, is that I cannot really explain intuitively why this should work at all. Modeling the problem as a cost that balances between keeping and removing links towards spam seemed quite arbitrary to me. But perhaps it\u0026rsquo;s me not understanding what the paper is trying to say. It\u0026rsquo;s certainly densely fitted with proofs for properties that don\u0026rsquo;t really move the needle of my understanding.\nLong story short, I decided to implement my own version, by combining the literature above and fixing a few issues I saw in the experimental data.\nPage Rank take III (Spamicity Trust Rank) I don\u0026rsquo;t know what to call it, so allow me to throw some arbitrary words together: Spamicity Trust Rank. It may be nonsense, but it combines the concepts I used:\nA Page Rank variant that still uses hyperlinks to infer \u0026ldquo;importance and influence\u0026rdquo; of domains. Trust Rank, because it uses a trusted set of domains as seeds. Spamicity, because it also assigns spam scores, based on a modified Anti-TrustRank pass. The implementation is done in two steps:\nspamicity, and trust_rank. The goal of the first pass is to discover as much spam as possible, by starting from a dataset of known spam domains. Anti-Trust Rank was designed with this goal in mind, but massively suffers from the problem presented above of punishing popular domains, regardless of them being spam or not (just because some links towards spam creeped in).\nTo counter this problem, I started from the algorithm as described by the original paper:\nInverse page-rank is computed by reversing the in-links and out-links in the web graph. In other words, it merely involves running pagerank on the transpose of the web graph matrix.\nBut add a twist: scale the spam contribution by the factor in which the source domain points to the target domain. The intuition here is that if a domain points to spam, it should be classified as spam, but the score should be reduced by the proportion of how much it actually does point to spam. I also include a damping factor, to progressively attenuate the score over distance (if a links to b that links to c, we expect the score of c to influence b more than it would influence a).\nThe more precise formulation is as follows:\nAnd the code (using Polars):\ncontribs = ( edges .join(nodes, left_on=\u0026#34;src_dom\u0026#34;, right_on=\u0026#34;node\u0026#34;) .with_columns( ( pl.col(\u0026#34;count\u0026#34;) * pl.col(\u0026#34;score\u0026#34;) ).alias(\u0026#34;contrib\u0026#34;) ) .group_by(\u0026#34;tgt_dom\u0026#34;) .agg(pl.sum(\u0026#34;contrib\u0026#34;).alias(\u0026#34;contrib\u0026#34;)) .select([\u0026#34;tgt_dom\u0026#34;, \u0026#34;contrib\u0026#34;]) ) contribs = ( contribs.group_by(\u0026#34;tgt_dom\u0026#34;) .agg(pl.col(\u0026#34;contrib\u0026#34;).sum().alias(\u0026#34;contrib\u0026#34;)) ) updated = ( nodes .join(contribs, left_on=\u0026#34;node\u0026#34;, right_on=\u0026#34;tgt_dom\u0026#34;, how=\u0026#34;left\u0026#34;) .with_columns( ( ( damping * pl.col(\u0026#34;contrib\u0026#34;).fill_null(strategy=\u0026#34;zero\u0026#34;) / pl.col(\u0026#34;out_weight\u0026#34;) ).fill_nan(0) + (1 - damping) * pl.col(\u0026#34;bias\u0026#34;) ) .alias(\u0026#34;score\u0026#34;) ) .drop(\u0026#34;contrib\u0026#34;) ) To explain with an example: suppose that src1.com points to tgt.com and spam.com, with weights 3 and 1 (i.e. its number of out-links).\nTo calculate the spamicity score of src1.com, we add the scores of its target domains (0 and 0.1), scaled down by the damping factor (0.85), and do a weighted average based on the number of outlinks:\ntgt.com contributes with a score of 0 spam.com has a score of 0.1, dampened by 0.85 (constant factor), scaled by ¼, as there\u0026rsquo;s only one out-link from the source, over 4 total. Given those spam scores, we run a biased PageRank that takes spamicity into consideration. The first intuition is that it\u0026rsquo;s more likely that when a user goes to a random page, they land on a popular domain, so we can use domain popularity datasets as a basis. The second is about spam domains: you can trick a user to click to a link that points to spam, however when they land there, it\u0026rsquo;s unlikely that they will stay and follow any further links (therefore they will jump to somewhere else).\nThese intuitions are modeled by biasing the teleport vector with domain popularity, and by scaling down contributions of spam domains – as classified by the first step – so that their impact on other domains will be limited. In other words, the popularity of a spam domain doesn\u0026rsquo;t translate into popularity of its outgoing links [code]:\n# Compute contributions and penalize based on spamicity # up to a certain threshold contribs = ( edges .join(nodes, left_on=\u0026#34;src_dom\u0026#34;, right_on=\u0026#34;node\u0026#34;) .with_columns( ( pl.col(\u0026#34;score\u0026#34;) * pl.col(\u0026#34;count\u0026#34;) / pl.col(\u0026#34;out_weight\u0026#34;) * (1.0 - (max_spam_penalty * pl.col(\u0026#34;spamicity\u0026#34;))) ) .alias(\u0026#34;contrib\u0026#34;), ) .group_by(\u0026#34;tgt_dom\u0026#34;) .agg(pl.sum(\u0026#34;contrib\u0026#34;).alias(\u0026#34;contrib\u0026#34;)) .select([\u0026#34;tgt_dom\u0026#34;, \u0026#34;contrib\u0026#34;]) ) # Sum all contributions going to the same target contribs = ( contribs.group_by(\u0026#34;tgt_dom\u0026#34;) .agg(pl.col(\u0026#34;contrib\u0026#34;).sum().alias(\u0026#34;contrib\u0026#34;)) ) # Compute the updated scores by summing up the two factors: # Contributions from backlinks and initial popularity estimates. updated = ( nodes .join(contribs, left_on=\u0026#34;node\u0026#34;, right_on=\u0026#34;tgt_dom\u0026#34;, how=\u0026#34;left\u0026#34;) .with_columns( ( damping * pl.col(\u0026#34;contrib\u0026#34;).fill_null(strategy=\u0026#34;zero\u0026#34;) + (1 - damping) * pl.col(\u0026#34;bias\u0026#34;) ) .alias(\u0026#34;score\u0026#34;) ) .drop(\u0026#34;contrib\u0026#34;) ) See how contributions are penalized (up to a certain max) for domains considered spammy, and the teleport contribution is biased by the \u0026ldquo;bias\u0026rdquo; column, which is initialized with larger weight for popular domains and uniformly lower weights for the rest.\nDangling domains After getting a ranked list of domains, it was time to find the dangling ones. For that, I looked for those that simply had no outlinks. If they have many domains pointing to them, but none going out, there\u0026rsquo;s a chance the domain wasn\u0026rsquo;t crawled because it expired.\nWell, not quite. There are many reasons why domains have no outlinks in CommonCrawl: robots.txt didn\u0026rsquo;t allow the crawl, or the domain is not serving http content at all, or the site was down, but the domain is still registered to someone.\nA good way to filter through this is by combining the filter for no outlinks with a DNS resolution. If that returns NXDOMAIN, then there\u0026rsquo;s a much better chance that the domain actually expired. The code is quite simple:\nasync def resolve(domain: str, sem: asyncio.Semaphore) -\u0026gt; tuple[str, bool]: async with sem: try: _ = await resolver.resolve(domain) return domain, True except dns.NXDOMAIN: return domain, False except Exception: # Treat other errors as \u0026#34;something went wrong, but domain may exist\u0026#34; return domain, True This resulted into a list of candidates (I kept about a hundred) that I checked manually. And this is the part where I would have loved to say: \u0026ldquo;and this is how I made my first million\u0026rdquo;. However, the results mostly split into three categories:\nDomains that looked interesting, but someone was already sitting on them, likely waiting for a buyer. Domains that were boosted by much more clever spam sources (e.g. fake blogs and fake companies, likely machine / AI generated). Shady domains that I definitely didn\u0026rsquo;t want to touch. Perhaps the first category might become a future good buy (if the owner just loses hope of selling and lets it expire), but I assume that several drop-catchers are already watching and will be faster than regular people in securing them right then.\nResults? When I said that I was 25 years late to the problem, I meant: a lot of people thought about this very same problem for a very long time. So much that there is plenty of terminology, research papers and valuable companies built around understanding and exploiting it: this is the weird universe of SEO, Spamdexing, Google bombing, Domain name speculation and Domain drop catching.\nSo, what\u0026rsquo;s the probability that there indeed is some hidden gem, forgotten by everybody, waiting just for me to take? Probably close to zero. But I wanted to try anyway.\nAt this point, I could continue optimizing and improving the ranking algorithms, but decided not to. The first category of domains I found above (domains that are interesting but not available), made me think that whatever is mildly interesting, was already found by somebody.\nOK, so what was the point of this post? Perhaps it looks like a flex, or a pointless exercise of someone with too much time on their hands. Maybe there\u0026rsquo;s some of that, but I had a lot of fun working on this project and wanted to share it. I might be in a small niche, but I also love reading posts like this one, so I wanted to contribute to the genre with something of my own.\nSome takeaways from this experiment:\nI feel like out-of-core aggregations are generally easy to fake with streaming capabilities, even when they are not built-in in the data framework one is using. See Generalized out-of-core aggregations. One more empirical proof of the value of back-of-the-envelope design, combined with small scale experiments (see Single machine?). The Internet is unbelievably full of crap. Even with very low expectations, I was still surprised by how much this is true. If you feel like I got things horribly wrong, and there are ways to make easy money with this data, be my guest! You can run things from scratch, starting from my code (messy but includes all experiments as well) or save a bunch of time and money by starting with the final data:\nScored domains dataset Links dataset Both of which are based on the September Common Crawl dataset, and are massively smaller (down to 35 GiB from 76 TiB).\nI didn\u0026rsquo;t get rich in the process, but I learned a thing or two about optimizing data ingestion, polars and PageRank variants. The journey was more important than the destination, etc. etc.\nAppendix Or, an unordered list of things I didn\u0026rsquo;t know where to put but still thought interesting.\nGeneralized out-of-core aggregations I think we can extrapolate a way to generalize to out-of-core aggregations from the particular example of the reducer implementation:\nUse the columns in the group-by clause as sharding key Calculate the number of necessary shards to fit a single one in memory, based on statistics on the number of unique elements in each column. Stream the source table to a set of separate tables, partitioned by the sharding key. Process shards one by one, instead of in parallel. Step number 2 is perhaps the hardest to get correctly, as memory usage depends on many factors. It will be hard to guess a good number of shards without prior knowledge. A good approximation might be to abound on the number of shards and then compute some of them in parallel in step number 4. If memory usage grows too large, kill some of these shards and postpone their computation to subsequent sequential steps.\nRewrite the mapper in Rust? Before someone starts screaming: \u0026ldquo;bro, you should have started with Rust, it\u0026rsquo;s faster, safer, and blah blah\u0026rdquo;, I want to show my little benchmark, where I simply ran the most compute intensive part of the mapper (i.e. gzip decompress) by using the fastest available library in Rust, and couldn\u0026rsquo;t really see a speed improvement. It turns out, the Go version is very well optimized.\nIn summary, I don\u0026rsquo;t expect a Rust implementation to be significantly (i.e. orders of magnitude) faster than the one I came up with. Not to mention that I\u0026rsquo;m more familiar with Go and Python, and they ran fast enough for my purposes.\nInfrastructure Based on the back-of-the-envelope estimates above and a small scale run locally from my laptop (done by downloading WAT files by hand and processing them locally), I concluded that each file would take an average of 6s of CPU time to process. Given that the dataset has 100k files, this roughly means 160 CPU hours.\nSince compute was the bottleneck (and not network), I just needed a compute-optimized instance with enough disk to get the job done. This pointed me towards a c6id.8xlarge VM, with 32 cores and 1.9 TiB of SSD, which costs about $1.6/h. The whole process should have taken less than 24 hours, and therefore less than $38 in total.\nInfrastructure as code CloudFormation fell out of favor among platform and cloud engineers nowadays, in favor of Terraform or fancier solutions like Pulumi and Crossplane. I still find the ease of setting up and use of CloudFormation simply unbeatable, as you just need the AWS CLI and an AWS account. No need to deploy extra things, or manage the state somewhere. Given that I had to run within AWS, it felt like a very natural choice.\nAnd hey, GitHub Copilot and other AI assistants really like it, so they speed up the creation a lot by making good suggestions.\nSee config and deploy script.\nWe\u0026rsquo;ll see later that this is a good starting point, but not sufficient.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nInterestingly, this is one order of magnitude less than the reported number of domains from DNIB.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nI couldn\u0026rsquo;t find an explanation for why the damping factor is usually set to 0.85. I ran a small scale ablation on notable pages with different values and concluded empirically that this indeed looks reasonable. When it\u0026rsquo;s set to a lower value (e.g. 0.5), ranks seem to vanish very quickly, and for higher values, they propagate unreasonably further away from the source. For example, a single link from google.com would positively impact pages 3-4 degrees separated away, thereby boosting a lot of spam.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://blog.mbrt.dev/posts/domain-resurrect/","summary":"\u003cp\u003eI processed a large fraction of the Internet on a single machine, trying to find\nsome valuable but forgotten domain names to buy (probably 25 years too late).\nHere\u0026rsquo;s the story of what I found and how I did it, and \u003cstrong\u003enot\u003c/strong\u003e the story of\n\u003cstrong\u003ewhy\u003c/strong\u003e I decided to try, because I don\u0026rsquo;t know myself.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eSpoiler alert\u003c/strong\u003e: If you just want to know whether there is indeed something,\nand you should scramble to scoop it up to become a\n\u003ca href=\"https://en.wikipedia.org/wiki/Domain_name_speculation\"\u003edomain-flipper\u003c/a\u003e\nbillionaire, hold your horses: I believe there isn\u0026rsquo;t much. The following\nsections are more focused on the engineering side, so if you only care about the\nconclusion, skip to \u003ca href=\"/posts/domain-resurrect/#results\"\u003eResults\u003c/a\u003e.\u003c/p\u003e","title":"Resurrecting valuable expired domains"},{"content":"I was frustrated by the gap between stateless and stateful applications in the cloud. While I could easily spin up a stateless application as a \u0026ldquo;serverless\u0026rdquo; function in any major cloud provider and pretty much forget about it, persisting data between requests was a game of pick two among three: cheap, strongly consistent, portable.\nCould I solve portability and lack of transactions myself with a single client-side solution? I thought it would be possible through object storage (e.g. AWS S3), which is strongly consistent, ubiquitous and cheap.\nYes, I could rely on a \u0026ldquo;serverless database\u0026rdquo; and just build a thin conversion layer between DynamoDB (AWS), Cosmos DB (Azure) and Firestore (GCP). So, does it make sense to instead reinvent a database almost from scratch? Not in a practical sense, but think about it this way: I learned a lot about database design, so it was still worth it, and I think you will find my journey useful too.\nAnd before I forget, here\u0026rsquo;s the source code.\nThe beginning My thinking started a few years back, when I realized I could easily deploy serverless solutions – like AWS Lambda or Google Cloud Run – to deploy a function that handles HTTP requests, without worrying about host OS updates, Kubernetes clusters, VMs, and all the underlying infrastructure. And since the serverless billing is a pay-per-request (with a decent free tier), I also paid little to nothing for most of the services I was running for myself.\nThe deal is easy: you provide the code and the cloud provider runs it. Pay no money, do no maintenance, and get a functional web service in return. Sounds like a good deal?\nIt was until I realized that this worked only if I didn\u0026rsquo;t need to persist data across requests. All database options had downsides: Google SQL for Postgres costs money whether I use it or not, upgrades were manual and required downtime. Spanner was strongly consistent, but very expensive (shared instances didn\u0026rsquo;t exist) and not portable. Datastore (now surpassed by Firestore) was cheap, but had restrictions on transactions (reads before writes) and wasn\u0026rsquo;t portable outside Google Cloud either. AWS provided similar services (RDS portable but expensive and manual, DynamoDB hands-off and cheap, but not portable to other clouds and eventually consistent).\nThe idea I decided to give the wheel reinvention another round of my energies. With the excuse of learning something, I thought: I\u0026rsquo;m already using object storage (Google Cloud Storage and AWS S3) to store larger files. Can I abuse their strong consistency properties and turn them into transactional databases? And yes, GCS is slow, but it\u0026rsquo;s extremely scalable. And how slow is too slow? I ran some tests on reading and writing 100KiB objects, and the 90th percentile looked like this:\nRead: 63.1ms Write: 105ms Metadata: 41.3ms Which is far from what you get from a local SQLite (10x slower), but fine in many scenarios and also works with multiple writers. In fact, as many writers as I want.\nThe other question was: when does this become too expensive compared to a traditional managed database solution like Google Cloud SQL? The cheapest managed Postgres instance comes at $8.57 per month. To pay the same, we would need an average of 0.66 writes / s or 8.3 reads / s (at $0.004 per 10k read ops and $0.05 per 10k write ops). If we instead use the smallest dedicated instance as a reference, estimates would grow to 2.7 writes / s or 34 reads / s.\nFor infrequent or bursty traffic, this makes sense so far.\nThe simplest database After I got the cost component confirmed, the next step was to come up with the simplest working solution – disregarding all performance considerations. \u0026ldquo;First make it work, then make it pretty\u0026rdquo; was my working mode. I started with GCS, because Google Cloud was the one I knew better at the time.\nThe first design choice I made was the architecture. The simplest possible is the one with no server component. I wanted to see how far I could go with a client-side library and just object storage.\nIn the same line of thinking, the simplest possible database is SQLite. You can make it work by running each transaction locally in the client and returning OK only if both the transaction and the upload to object storage succeed. To avoid race conditions between two different clients, you can either use locks, or implement a form of OCC (Optimistic Concurrency Control), by checking that no concurrent updates happen between a transaction\u0026rsquo;s reads and commit.\nI decided to go with the second option, because it was much easier to implement.\nFor that, I needed two APIs: read and conditional write – both available in GCS. These primitives, although primitive, allowed me to build a database. I could serialize a SQLite database into an object and for every transaction do the following:\nDownload the DB locally from GCS, keep note of its metadata version, and open it with SQLite. Execute the transaction locally. If it succeeds, take this new database and try to write it conditionally to GCS (using the previously read metadata version). If it fails, repeat from #1, executing the transaction again. On success, return success to the user. Step number two is atomic and succeeds only if no other client modified the object in the meantime, which is what makes the algorithm work.\nWell, this is slow. The whole database must be downloaded and uploaded in full at every transaction – a bit of a problem if it\u0026rsquo;s gigabytes in size. Clients are also guaranteed to conflict at every transaction, adding retries on top. Even if they access different tables, they still need to update the same global object, resulting in no parallelism.\nEven worse, GCS officially allows one update per second per object, resulting in a maximum of one transaction per second, killing the performance of the database. In practice, because of the way rate limiting is implemented (tokens are checked and reset every minute), GCS allows short bursts of requests to go faster, with penalties coming afterward. This results in very inconsistent performance, with latencies going from tens of milliseconds to tens of seconds, the moment clients hit the same object consistently.\nDistributed transactions With the single object approach only going so far, I needed to look into distributing the load to multiple objects. The main problem I needed to solve became implementing ACID transactions across multiple objects. Even though updates to the same object are guaranteed to be strongly consistent – the contents of a write are available to all readers, no matter where they are, immediately after the write completes – this only applies to single objects. Operations involving multiple objects at once, such as copying or renaming multiple objects, are not atomic. Furthermore, the version number of an object has nothing to do with that of another, so there\u0026rsquo;s no global ordering for writes either.\nLuckily, there\u0026rsquo;s a whole literature of distributed transactions algorithms designed to solve this problem space with different tradeoffs. And the design space at this stage was:\nObject storage Provides strong consistency for operations on single objects. Latency can be high (50ms - 150ms). Does not allow frequent updates to the same objects (\u0026lt; 1s). No backend component, only a client library. No direct communication channel between clients. This suggested that I look into algorithms that minimize the number of write operations, while keeping \u0026ldquo;correctness\u0026rdquo;. We\u0026rsquo;ll get back to this in a moment, but as an aside, what do I mean by correctness?\nAside: Isolation and Consistency When thinking about correctness, my mind went to university lectures on databases about ACID properties, especially isolation. But I found it difficult to relate that (stale) knowledge with material in the literature and on the web. The main issue, I believe, is due to overlapping terminology and concepts between research on traditional relational databases and shared memory systems. These concepts, previously treated separately, became simultaneously useful to distributed systems, which borrowed terminology from both sides.\nThe crux of the issue is that about ACID properties (Atomicity, Consistency, Isolation, Durability), everybody agrees on atomicity (a transaction is applied in full or not at all) and durability (after a commit, the effects of a transaction are persistent even after system crashes). But not so much on the other two. Isolation levels in the SQL standard are not only imprecise, but also stuck in the nineties. This is a problem known since 1995, which opened the door to narrow and misleading interpretations by database vendors. Oracle famously claimed their implementation of snapshot isolation to be \u0026ldquo;serializable\u0026rdquo; – even though it wasn\u0026rsquo;t – through some legally anal interpretation.1 Furthermore, isolation levels apply to single host databases, but fail to adequately take replication into consideration, which introduce new anomalies and so require more expressiveness. In addition, consistency means different things between the SQL standard and in distributed systems\u0026rsquo; literature. In ACID, C has traditionally meant \u0026ldquo;respects constraints specified by the user\u0026rdquo; such as foreign keys and unique constraints. Whereas, the C in the CAP and PACELC theorems refers to the ability of a read to receive the value of the most recent write.\nOn top of all this, literature and blogs use \u0026ldquo;isolation levels\u0026rdquo; in a very liberal way, sometimes meaning to say \u0026ldquo;consistency level\u0026rdquo;, or a combination of an isolation and a consistency guarantee. If I wanted to design a database, I first needed to clarify things for myself.\nAnd in my quest for clarity, I bumped into Jepsen\u0026rsquo;s diagram over and over. Jepsen takes Bailis\u0026rsquo; approach of unifying everything in a single hierarchy of ever greater \u0026ldquo;consistency\u0026rdquo;. This is a nice simplification, but it forgets to mention that the unified hierarchy is actually two hierarchies fused together: one of consistency and one of isolation guarantees. Isolation provides rules for how the system behaves when transactions run concurrently, and consistency specifies when writes are available to reads after they complete. Thanks to Daniel Abadi I realized that a database can provide guarantees for both hierarchies at the same and this is what we sometimes call \u0026ldquo;isolation level\u0026rdquo; and sometimes \u0026ldquo;consistency model\u0026rdquo;, making the terminology very confusing.\nMy revised diagram makes this distinction more prominent. Arrows show guarantees of inclusion (e.g. Linearizable is Sequential plus additional guarantees). Dotted lines show how \u0026ldquo;isolation levels\u0026rdquo; are defined as a combination of isolation and consistency guarantees. Note how Strict Serializable is the strongest level, as it implies all the guarantees.\nIn essence, a guarantee is defined in terms of which anomalies it prevents. The more it prevents, the stronger it is. And since anomalies are unexpected results that happen when transactions run concurrently, the more anomalies are allowed in an isolation level, the more care a developer has to put to prevent them from popping up as bugs in the application logic. For example, dirty reads, allowed in the Read Uncommitted isolation guarantee, happen when a transaction is allowed to read a value modified by another transaction that isn\u0026rsquo;t committed yet. Because of this, transactions that abort may still influence other transactions.\nIsolation anomalies break the illusion of transactions running independently of each other, and consistency anomalies break the illusion of a system being in a single machine (i.e. replication lags become visible).\nNotable levels Isolation levels seemed quite abstract to me, until I looked at notable databases and their position in the hierarchy. Two things were clear: the first is that database vendors go to great lengths in explaining why their database is \u0026ldquo;consistent enough\u0026rdquo; and how \u0026ldquo;rare those anomalies are in practice\u0026rdquo;. The second thing is that there\u0026rsquo;s great variability in levels:\nFor example, Postgres provides Read Committed by default, which means that the same process can update a row in a transaction and then the next transaction could still read the outdated value. Yes, these are two separate transactions, but this is surprising to most developers, especially when the database is hosted as a single replica. But since Postgres is also Sequentially Consistent, at least that the transactions\u0026rsquo; ordering is the same for everyone, even though it doesn\u0026rsquo;t necessarily follow the real time in which transactions were applied.\nKing of the pack in terms of isolation is Spanner – which is Strict Serializable – meaning that transactions run as if they were serially executed, and they follow the real time order globally. This is the most correct model. FaunaDB is also Strict Serializable through a different mechanism.2\nUnsurprisingly, Cassandra, being a true NoSQL database, provides a weak, eventually consistent model, because the focus is on availability at all costs. An interesting middle ground is CockroachDB. Very close to Spanner, but occasionally allows for transactions to read stale data, in the name of more optimistic concurrency controls.3\nWhich isolation level to target? Weaker isolation levels allow for more optimizations in the database implementation, but also allow for more classes of anomalies. This means they are usually faster, but require application developers to implement more logic themselves to avoid bugs. The balance between performance and correctness is a pendulum that swung from the NoSQL years – faster is better – to the NewSQL era, after the Spanner paper in 2012, where correct is better. Practice and papers like ACID rain (from Bailis) showed that designing correct applications with weak consistency databases is a lot harder: they require more effort and time to develop and understand, so why bother, if stronger consistency is fast enough? After all, developer time is more expensive than machine time.\nFor me the answer was clear: the ideal isolation level is the highest one before things become too slow for the application at hand.\nNo isolation Given this background, my thinking went to this basic algorithm for a transaction:\n- Read what you need - Write what you need - On commit: do nothing - On abort: undo the writes one by one This clearly doesn\u0026rsquo;t provide Atomicity, nor basic isolation such as Read Committed, because transactions commit one key at a time and concurrent transactions can read new values before they are committed. This however provides Linearizable consistency, thanks to the strong consistency offered by S3 and GCS, which state that any read initiated after a successful write, return the contents of that latest write. Without effort, we have basically achieved strong consistency with no isolation.\nThe challenge then was to introduce stronger isolation guarantees while keeping acceptable performance.\nInterface Before going further down the rabbit hole of picking a transaction algorithm, I needed to take a step back and decide on which interface I wanted to provide. The nice part of my simplest database is that it provides a recognizable SQL interface through SQLite out of the box. But, by deciding to split the data among multiple objects in a bucket, I was making that a lot harder to achieve.\nI looked around for inspiration and found that many modern databases are transactional key value stores at their core. Some, like FoundationDB, stop there. They are in a way just key value stores. Others – like CockroachDB – take a step further and build a Postgres compatible SQL layer on top of it.\nOne more source of inspiration was Ben Johnson\u0026rsquo;s BoltDB, a very elegant transactional embedded key value store written in Go. You can see its influence in my GlassDB example:\nfunc compareAndSwap(ctx context.Context, key, oldval, newval []byte) (bool, error) { coll := db.Collection([]byte(\u0026#34;my-collection\u0026#34;)) swapped := false err := db.Tx(ctx, func(tx *glassdb.Tx) error { b, err := tx.Read(coll, key) if err != nil { return err } if bytes.Equal(b, oldval) { swapped = true return tx.Write(coll, key, newval) } return nil }) return swapped, err } This snippet implements a simple compare-and-swap for a key in a collection – which is just a set of keys with the same prefix. The db.Tx method wraps the operations into a single transaction, so that it can be retried by GlassDB transparently.\nI will not spend too much time discussing how to use it. If you are curious, please check out the repository and the relative reference documentation. The only thing I wanted to show is that you can put arbitrary code inside a transaction, and reads and writes to the database will be, well, transactional.\nChoosing the algorithm As I mentioned earlier, my goal was to choose an algorithm that guarantees the strongest possible isolation levels, while retaining a reasonable level of performance. The first thing that came to mind then was to use Strict Two Phase Locking (S2PL), similarly to Spanner. This alone would guarantee serializable isolation, and combining that with the linearizable consistency of GCS, I would get the top spot in isolation levels: strict serializability.\nThe algorithm itself is quite intuitive:\nA key must be locked as read-only or read-write, before you can read it. It must be locked as read-write, before you can modify it. You can release all the locks only after committing. My approach was to first make it work, and then make it fast.\nMake it work To make the algorithm above work, I needed locking and atomic commits.\nLocks GCS objects are made of two independent parts: content and metadata. Metadata contains user-controlled key values and a version that automatically changes at every write (or metadata change). Given this, it\u0026rsquo;s easy to update an object\u0026rsquo;s metadata to mark it as \u0026ldquo;locked\u0026rdquo;. The algorithm is basically a compare-and-swap:\nRead the object\u0026rsquo;s metadata with its version number. Is the \u0026ldquo;locked\u0026rdquo; tag set to \u0026ldquo;true\u0026rdquo;? If yes, wait a bit and repeat #1. If no, conditionally write the metadata, setting \u0026ldquo;locked\u0026rdquo; to \u0026ldquo;true\u0026rdquo;. The condition constrains the write to succeed only if the metadata version is the same as the one we read in step #1. If the write failed, go to #1. If it succeeded, we got the lock. Unlocks work similarly, but it does a compare-and-swap for \u0026ldquo;locked=false\u0026rdquo; instead.\nThis algorithm is quite simplistic: it doesn\u0026rsquo;t distinguish between read and write locks, and crashes in the application may result in forever-locks. We\u0026rsquo;ll talk about crashes in a bit, but before we need to add read locks:\nUse a lock-type and a lockers tag, containing respectively the type of lock (read, write, none) and the IDs of the transactions holding the lock. Locking means updating the lock-type and adding the ID of the transaction holding the lock to the lockers tag (we can modify multiple tags atomically). Unlocking is the opposite: remove the ID from lockers and if that was the last locker, set the lock-type as unlocked. Some additional intuitive ground rules as well:\nMultiple transactions can hold read locks to the same key at the same time. Read locks can be upgraded to write, but only if there was a single transaction holding it. Write locks are exclusive. Commits Commits must appear instantaneously: a transaction is either committed or not, and nothing in between. How do we do it with transactions updating multiple keys? How do we realize those updates at the same time? It\u0026rsquo;s clear that we can\u0026rsquo;t update one key at a time while releasing the locks, because if the process crashes in the middle, we would get an inconsistent state.\nOne way would be to use transaction logs. Transactions stash the updates they are going to do into a log, before actually doing them. This allows replays after crashes. When the log is committed, the transaction is considered committed and a process picking up the work after a crash will read it and continue committing the writes.\nThe way I implemented this was to include all the values written by a transaction into its transaction log upon commit. A transaction keeps the keys untouched until commit, and only after the log is written successfully it unlocks the keys while updating their values.\nDeadlocks and crashes Distributed locks require timeouts. If a process opens a transaction, locks a few keys and then crashes, those keys would become inaccessible. Even if the crash was temporary, we still would depend on all clients to come back online reasonably quickly after crashes, which might be unreasonable.\nThe way to solve this is by introducing TTLs on locks. While locking a key, a transaction also keeps track of the time it\u0026rsquo;s taking to commit. After a timeout, it will create (or update) its transaction log by stating that the transaction is still pending, along with a refresh timeout. Competing transactions that want to check whether a lock is still valid, just need to read the corresponding transaction log. If the timestamp is too far back in the past, or no log appears during the entire timeout period, the lock is considered \u0026ldquo;expired\u0026rdquo;.\nHowever, the naive algorithm of observing a lock and just taking it over after a period of inactivity, is incorrect. The reason is that if a transaction blocks only temporarily, after the verify stage and before writing the transaction log, there\u0026rsquo;s no way for this transaction to check whether it\u0026rsquo;s still allowed to commit or it has lost some of its locks.\nThis cannot be solved by introducing more checks before committing, because the delay could happen exactly during the write for the transaction log, which would be impossible to block.\nA way to solve this, is to use the transaction log itself as a synchronization point. A transaction (2) that wants to take over an expired lock held by (1), can try to conditionally write to the transaction log for (1), stating that it is aborted. If it succeeds (i.e. the log didn\u0026rsquo;t change in the meantime), then (1) is officially aborted and all its locks are invalid. If it failed (because e.g. the transaction successfully committed, or refreshed the locks), then (2) will have to wait longer.\nBy making sure changes to transaction logs happen atomically and are never overwritten incorrectly, we make sure that even in the presence of concurrent changes, we always communicate the intention and the state of each transaction.\nIn the example above, if (2) succeeds in writing the transaction log for (1), the write to the same log that (1) tries to do for its commit will fail, because its condition would be set for a past version. At that point, (1) will just abort.\nCollections, create and delete keys The way we handle creating and deleting keys in transactions is by using pseudo-keys. There\u0026rsquo;s one such key per collection, representing the \u0026ldquo;list of keys\u0026rdquo; present in that collection. To ensure consistency when iterating through keys or adding and removing keys, transactions just lock the corresponding pseudo-key for each affected collection:\nCreating a key: lock the collection (i.e. its pseudo-key) in write and create the key (locked in write). Deleting a key: lock the collection and the key in write. Iterating through a collection: lock the collection in read. This mechanism allows keeping the collection unlocked while keys are just modified, but not added or deleted (which should be most of the time).\nMake it fast Up until this point, I had a working algorithm with strong isolation and consistency, but was it fast? Given that object storage itself is not particularly fast, to maximize throughput I wanted to avoid transactions operating on different keys to interfere with each other. This excluded any kind of global lock, or single transaction log, because they would generate contention from all the transactions. I also wanted to maximize parallelism, given that object storage is designed for horizontal scalability in mind.\nIn the quest of making things faster, we still should not forget the larger context and constraints:\nNo server component No communication between different clients Transaction logs The easiest way to avoid transactions to interfere with each other is by having a different log object for each transaction, rather than a single object for all. The client assigns a random ID to each transaction and that is part of the path of the log object. Each client is then able to read a transaction log without the need to communicate with anyone, because given an ID, its path is deterministic.\nBut how do we map keys to transactions affecting them? The answer is in objects metadata. When locking, we keep track of the ID of the locker(s). A reader is then able to check whether the object is still locked or not, by which transaction and what is the state of that transaction, just by reading its metadata. We have three cases to consider while testing a key:\nThe lock-type is set to none: The key is unlocked. The lock-type is set to read or write, and: There\u0026rsquo;s one transaction listed in locked-by that is pending (by looking at the corresponding logs): the key is locked. Otherwise, the key is unlocked. Optimistic locking Fast is always relative and situational: faster compared to what? And under which conditions? Which use cases are we willing to sacrifice in order to improve on the most important ones? The key decision for me was to optimize for low data contention workloads. If writes to the same key at the same time are rare, I could optimize for transactions that don\u0026rsquo;t conflict with each other, to the expense of transactions that do. This idea points in the direction of optimistic locking, where transactions use data resources without acquiring locks. Transactions verify that no other transaction affected the same data only on commit. It also optimizes the case where object storage shines, which is not latency, but throughput.\nThe algorithm works like this:\nModify: Read keys and tentatively write (i.e. stage locally) changes. Verify: Check that other transactions did not modify read values after the transaction started. Commit: If no conflicts are detected: make the changes effective (commit), otherwise retry the transaction. Optimistic locking avoids locking, but implicitly requires transactions to validate and commit one at a time. This is because verify and commit must be executed atomically. If not, we would risk losing writes:\nTo avoid this problem and maintain the isolation level achieved by the base algorithm, I used a modified version of optimistic locking. One that, well – uses locks. The difference from the base algorithm is that transactions run without locks until the verify phase, where they acquire all the locks in parallel, verify for conflicts safely and only then, commit a transaction log (or retry):\nRead all the key values it needs without locks. Write values in a staging area (in memory). When committing: Lock all the keys that were read or written in parallel. Check that the read keys haven\u0026rsquo;t changed in the meantime. If yes, retry the transaction from #1. Write a transaction log that includes which keys were modified along with their new values. (async) Write the new values back to the modified keys. This avoids global synchronization points. If two transactions touch different keys, they will not compete for the same objects, so they are able to proceed in parallel and locks are kept for the shortest amount of time.\nNote that in #3b, when retrying, locks are not released. This has the side effect that transactions will be retried at most once, as the second run will happen while still holding the locks, turning it effectively into pessimistic locking. No other transaction will be able to commit those keys in the meantime.\nRead-only transactions We are already optimizing for high throughput, low contention workloads. In this scenario, read-only transactions are also going to run undisturbed most of the time. Can we take advantage of that and run the standard optimistic concurrency control algorithm without locking? Remember that verify steps of transactions must run sequentially, to prevent losing updates. In the general case of read-write transactions, both the current transaction can interfere with others, and others can interfere with the current. For read-only transactions, however, we don\u0026rsquo;t have to worry about the first problem. They don\u0026rsquo;t write anything, so they can\u0026rsquo;t interfere with other transactions\u0026rsquo; verify and commit steps. Therefore, the only thing to do is to make sure no transaction modified keys while our read-only transaction was reading them, i.e. between our first and last read.\nWe can achieve this in a relatively simple way:\nRead phase: read all required keys while keeping track of their current version. Verify phase: check the current version of all the keys read again: If the versions didn\u0026rsquo;t change, and the keys were not locked in read-write, we can return successfully. Otherwise, there was a concurrent write and we retry. Note that in (2) we not only check that the object version stayed the same, but that the metadata version did. The actual check is slightly more complicated, as we can avoid retrying in a few cases even when metadata did change, for example when the key was locked in read and unlocked, or when locked, but the transaction aborted.\nTo avoid retrying an unlucky transaction forever, after the first retry we fall back to the regular algorithm, which will acquire (read) locks to keep writers away. This approach guarantees progress eventually.\nIf the transaction succeeded the first time around, since we didn\u0026rsquo;t really make any change in values, nor in the state of locks, we can avoid writing a transaction log as well. The happy path requires no writes and just one value plus one metadata read per key.\nThis approach is different from the one taken by Spanner, which implements Multiversion concurrency control. MVCC allows reads to go much faster, by reading a consistent snapshot of the database. This is hard to do correctly, because the linearizable consistency of the database depends on picking a version compatible with the real time ordering of all transactions globally. Picking anything older than that would lead to weaker consistency models. Spanner achieves this the hard way, through atomic clocks and bounded time uncertainty.\nGiven that we don\u0026rsquo;t keep multiple versions of objects and we don\u0026rsquo;t have access to atomic clocks, nor we have transactions coordinators, I chose to revert to two-phase locking in case of any conflicts. This makes sure we stay in the strict serializability isolation level.\nSingle read-modify-write We can optimize transactions that read and write to a single key by using native object storage compare-and-swap (CAS) operations. The algorithm becomes:\nRead the key contents. During validation: read the key metadata If it\u0026rsquo;s locked: retry (go to #1) Otherwise, check that the version is the same it was during the read. If not, retry (go to #1) Do a conditional write on the key with the new value, and set the condition to be that content and metadata versions match what we read in #2. In case of conflicts, we can retry the transaction with the base algorithm, so that the transaction is not starved in case of high contention.\nLocal cache The last optimization I wanted to talk about is the local cache. The cache is useful to eliminate reads when a client is the only one updating a certain key. A set of transactions like these would require reading the same key 100 times, even in the case no other transaction touched it:\nfor i := 0; i \u0026lt; 100; i++ { db.Tx(ctx, func(tx *glassdb.Tx) error { b, _ := tx.Read(coll, key) return tx.Write(coll, key, string(b) + “a”) }) } For this reason, I introduced a local LRU cache that keeps in memory the values committed by the most recent transactions. The cache has a limited and configurable size, to make sure it won\u0026rsquo;t eat all the available memory.\nThe way this works is by checking the cache during every read in a transaction. If there\u0026rsquo;s a value, the transaction uses that. If, during validation, the value turned out to be outdated, the key is marked as stale and the transaction retried. During the retry, the cache would be invalid, and so the transaction will go and fetch the fresh value from object storage.\nBenchmarks How performant is this thing? Details are on the glassdb repo, but in summary, low conflicts workloads lead to a somewhat linear throughput with the number of transactions.\nThe graph above shows three different types of transactions, where the bold line is the median number of transactions per second and the error band includes the 10th and 90th percentiles. The dataset above was composed of 50k keys and each client is doing 10 transactions in parallel (10% read + write 2 keys, 60% read 2 keys, 30% weak reads4 of a single key).\nGiven that keys are selected randomly by transactions, you can see how higher percentiles are affected by the higher concurrency, because of the higher probability of conflicts.\nInstead, latency – which is not the strong point of the solution – is higher than most databases, but stays relatively flat even with higher concurrency. Exception being with higher levels of contention.\nIs that all? The article is pretty long, and even if it doesn\u0026rsquo;t cover everything – and I would have really liked to talk about the source code, testing, correctness, profiling and more optimizations – it covers the most important aspects of the design. I hope I was able to transmit some of the key lessons I learned along the way and demystified some of the opaque sounding, but actually not so hard to understand, terminology, used and abused in database design and classification.\nIf you want to explore some of the source code, I recommend starting from the transaction algorithm. There aren\u0026rsquo;t many comments, but hey it\u0026rsquo;s written in Go, it should be understandable :)\nOne more thing: did I forget about S3? At the beginning, I praised how easy it would be to have the same solution across cloud providers. Did this come to pass? Not really. Or, at least, I spent so much time making this work on GCS that I never had the time to port it to other cloud providers. It also turns out to be not so easy, because Google Cloud provides a superset of conditional updates with respect to AWS S3.\nWell, no problem. I learned something and solved something. The rest can wait for another release.\nFeedback on Hacker News.\nSee Introduction to Isolation Levels in the Fauna blog for more details on this.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nSee also Fauna architectural overview.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nSee CockroachDB\u0026rsquo;s blog post on consistency and the Jepsen report.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA weak read is a read that is allowed to get a stale value, bounded by a user specified limit. For example, a weak read with a 10s staleness bound, is allowed to read the latest value of a key, or any value written in the previous 10s from the cache, but no earlier.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://blog.mbrt.dev/posts/transactional-object-storage/","summary":"\u003cp\u003eI was frustrated by the gap between stateless and stateful applications in the\ncloud. While I could easily spin up a stateless application as a \u0026ldquo;serverless\u0026rdquo;\nfunction in any major cloud provider and pretty much forget about it, persisting\ndata between requests was a game of pick two among three: cheap, strongly\nconsistent, portable.\u003c/p\u003e\n\u003cp\u003eCould I solve portability and lack of transactions myself with \u003cem\u003ea single\nclient-side solution?\u003c/em\u003e I thought it would be possible through object storage\n(e.g. \u003ca href=\"https://aws.amazon.com/s3/\"\u003eAWS S3\u003c/a\u003e), which is strongly consistent,\nubiquitous and cheap.\u003c/p\u003e","title":"Glassdb: transactional object storage"},{"content":"What is a design document? When is it useful to have one? How to make it useful? These are the questions I\u0026rsquo;m going to address in this post. The goal is to provide some bite sized, easy to remember guidelines. My promise to you is that after this post, you\u0026rsquo;ll be faster and more effective in making and reviewing designs.\nThis is a written version of a talk I gave at my current employer. Since it was well received and helped a few folks, I decided to make it more available.\nWhy this? I\u0026rsquo;m not a professional technical writer. I\u0026rsquo;m not even the best writer among my peers. Why am I even trying to address this complex topic? Well, I happen to have seen lots of design docs (and written a few) over the years. Some excellent, some awful, and everything in between. I collected patterns and made my own perspective.\nThe first goal of this is to help people struggling to write or get attention on their designs. My secondary goal is definitely selfish, as I\u0026rsquo;m hoping to reduce the percentage of design docs reviews I need to make that are physically painful.\nTo be fair, I\u0026rsquo;ve written my fair share of painful-to-read docs. You can even scroll back in time in this blog to see some great examples of bad writing. The third goal is then to pay back some of this debt.\nWhat is the purpose of a design doc? The answer is, as always, it depends. But among the various things a design doc could be, I\u0026rsquo;m not going to focus on:\nImplementation manuals, Detailed requirement documents, Novels, Brain dumps. These may all be valid written documents in certain contexts. For example, a detailed specification document might be necessary when writing the actual code is offloaded to contractors, or when specifications are legally required (e.g. aerospace or safety critical domains). Brain dumps are useful while brainstorming. Novels are a pleasure to read, and so forth.\nNevertheless, the focus is here on organizations of software engineers that own software end-to-end. The goal of a design doc there is to solve a problem collaboratively, rather than fulfilling red tape or legal requirements. This is the attitude you commonly see in today\u0026rsquo;s tech companies (established and startups alike).\nDesign docs are also most definitely not perfect. We\u0026rsquo;re not trying to win the Pulitzer prize, but solve a concrete problem in a timely and effective manner.\nIn this context, the key purposes of a design doc are instead:\nImprove the understanding of a problem and its domain (of author and reviewers). Drive alignment, to get to a better solution faster than trial and error. Although these are the main goals that we\u0026rsquo;ll use to build our guiding principles, good design docs come with side benefits:\nDocumentation: they serve as organizational memory, ease knowledge transfer and onboarding. Easier stakeholder communication. Lower the cost of failure (a discarded design is cheaper than a discarded implementation). Serve as artifacts for engineers (in performance reviews). Enable asynchronous discussion. Does every decision need to be a design document? Absolutely not! Smaller scope or easily reversible decisions might need just a simple code review or an email.\nHow to make a good design doc? The idea is simple: we take the two goals above and follow the implications logically to build the principles below. Since there\u0026rsquo;s no perfect structure, and we don\u0026rsquo;t have infinite time, we\u0026rsquo;ll also need to be pragmatic in applying them. Sometimes not all the principles are necessary or useful, but it\u0026rsquo;s still good to keep in mind when we deviate and why.\n1. Know your audience Who needs to read the document? What\u0026rsquo;s their background? What are they worried about? The target audience of the document informs its main focus.\nFor example, if the audience is internal to the team, there needs to be less background information and perhaps more implementation detail. For cross-team reviews? The focus should be on cross-team interfaces and responsibilities. For executives or product managers? Success metrics and business impact should be the main focus.\nGetting the audience wrong will generate more questions during reviews and waste precious space on the doc with unwanted details. Using the same document for two vastly different audiences will make them both unhappy. Split it into two smaller documents instead.\nDo your audience a favor and tailor your document to their needs. This will improve their understanding of the problem and solution.\n2. Timing The document should arrive in a timely manner. There\u0026rsquo;s a time for designing, a time for experimentation and one for implementing. Outside a window of opportunity, design documents are no longer useful.\nIt\u0026rsquo;s common advice that designs should be shared early, but there\u0026rsquo;s a limit to it. Sharing a design before doing any experimentation might be a waste of time. Too many options unexplored and unclear direction. Sharing something too rough with a wide audience attracts brutal reviews, where the document is ripped apart line by line.\nAt the same time, sharing after implementation started is too late! Changing direction based on feedback at this point in time is expensive and might be unfeasible. What\u0026rsquo;s the point of the design, then? Ticking a box in your next performance review?\n3: Explain reasoning Explaining how a solution works is not enough. It\u0026rsquo;s even more important to explain why:\nWhy do we even have a problem (worth solving)? Why are we picking this solution? What are the alternatives we didn\u0026rsquo;t pick and why? Why is much more important than how. If you need to cut anything, cut implementation detail over reasoning and alternatives. This will improve understanding now, but also answer future questions of people digging and wondering \u0026ldquo;why didn\u0026rsquo;t we do that instead?\u0026rdquo;. Your future self and colleagues will thank you.\n4: Clear scope and abstraction level This tracks back to #1, but focuses on scoping. Given an audience, focus on one problem (or class of problems) to solve and one abstraction level. Don\u0026rsquo;t mix detailed requirements with implementation details. It will make the document too large and difficult to follow.\nIn addition, place the document in a context: what is related work? What is the area, project or overarching effort? How does this work fit into the greater context? Add pointers to these to help place the doc logically in people\u0026rsquo;s heads.\nIf a document is becoming too large, split it up! Keep a shorter high level overview and add pointers to separate documents focusing on sub-parts with additional details. This has the additional benefit of allowing multiple people to design sub-parts (delegation) and still keep efforts in harmony. It also keeps the abstraction levels and scoping clear, allowing reviews from the right set of people on each document, as they can be different.\n5: Simple Nobody\u0026rsquo;s got time to read your doc. Assume casual reading from busy people. Assume no in-depth knowledge of the topic (unless proven otherwise). Keep sentences short and simple. Keep the document short. Add pictures and diagrams whenever possible.\nBe kind to your reviewers and their time. You\u0026rsquo;ll get quicker and better feedback.\n6: Important information first Reader\u0026rsquo;s attention will drop after just a couple of pages. Keep the objective and main idea in the first page. Details, alternatives and future work should be at the bottom. This maximizes return over readers\u0026rsquo; attention.\nFor example, put meeting notes, alternatives considered and implementation details at the bottom of the document. Keep objective, TL;DR, metadata and diagrams at the top.\n7: Clear status Readers should have an easy time understanding whether to provide feedback and to whom. Document metadata serves this purpose. Who is the author? When did they write it? What\u0026rsquo;s the current state (under review, approved, abandoned or implemented)?\nAt a minimum, there should be:\nOwners Date of last update Review / implementation status Link to bugs (e.g. Jira issues) When you come across an old doc you own, take the time to update its metadata when necessary.\n8: Data and metrics If the problem is not self-evident, quantify it. How much of a problem is it? Define success metrics and predict the impact of the solution (e.g. x% faster, $$ saved per year, etc.).\nThis is not always relevant or feasible, but very important for more expensive and broader scope efforts (e.g. when multiple teams are involved). Also keep in mind that executives love numbers on things they review.\n9: Good title (bonus) Having a memorable title is a nice plus. It will help people remember and refer to it more easily. It can also prime the reader towards the proposed solution. You can think of the title as prime real estate on the document, so it should be used wisely.\nFor example, consider a document aimed to solve observability issues in a service. The idea is to get SRE involved in a subset of the service. Compare these two titles:\nAddressing noisy alerts in ServiceX ServiceX monitoring responsibilities: TeamX and SRE While the first version talks about a symptom, the second version already suggests what the proposal is. This will prime the reader to look into where the responsibilities are shifted, which is the most important part.\nEfficient creation process How do you go from a blank page to something good enough to guide an implementation? The mistake most people make here is to simply sit on the design and refine it for weeks or months before sharing. By that point, the idea and its implementation became too precious. Too much time was invested, and it\u0026rsquo;s too expensive and emotionally distressing for the author to part away from their child.\nDocuments designed for too long tend to result in charged reviews. On the one hand, authors are very invested in the idea, on the other, their isolation from reviewers will oftentimes lead to suboptimal proposals. This generates many questions and comments asking for changes, especially when the audience is broad. The discussion might become heated.\nWhat is the mistake here? The problem is considering that a design is \u0026ldquo;done once\u0026rdquo;, perfected and then presented to the target audience (waterfall model). Reviewers are treated as gatekeepers. They should approve or object to the design during that latter phase.\nSharing a document with everyone before it\u0026rsquo;s ready is also problematic, because you will likely still get too much feedback from many people, and it might be distracting and useless. This is the more true, the broader the target audience.\nThe better way to design something is to iterate on both document and target audience:\nDiscuss Dump knowledge, incorporate feedback Organize Share with a few (more) Iterate, incorporate comments (go to #2) Maintain until the implementation is finished, then archive Ideally, authors should not spend more than a few hours on a document before sharing. Terrible ideas will be weeded out during early iterations, while the document is not polished.\nQuestions from early reviewers really show where the document is missing details or is making weak assumptions. Make sure those are addressed in the document directly, rather than just replying to the questions during the review.\nIn case formal review and approval is necessary, this can be done during later iterations, when most of the target audience has already seen and discussed the contents. This ensures a smoother review.\nFinding the audience How do you know which reviewers to pick? The most critical decision to make is to distinguish between reviewers and people that should simply be informed about the change. The list of reviewers should be minimal, while still being able to collect quality feedback. Don’t make the mistake of e.g. asking for a review to the whole engineering org (assuming it’s a big one). Not all readers need to be reviewers. This extended audience can safely be a lot bigger. If possible, keep the document open for everyone to read. This improves knowledge sharing and contributes to a culture of openness.\nYes, but who to pick? This is highly context dependent (not only company, but team and project wise). In any case, your team is usually a good start. If there are other teams affected by the proposal, it’s also usually a good idea to involve them (unless they are too many). In case of doubts don’t hesitate contacting your tech lead, manager or other leads in the relevant areas. They should be able to direct you to the actual experts you need.\nHandling feedback Assuming you picked your audience correctly (i.e. includes experts in the relevant fields and it’s not too broad), you should expect to get relevant feedback. We can divide it in three major categories and try to make the best use of it:\nDeal-breakers or asks for major changes. Questions. Bike-shedding. You should treat the first category, major concerns, with major attention. If the proposal turns out to be infeasible, abandon it. If it can be reworked, pause the review and do so. Likewise, pay attention to the second category. When people ask questions and are unsure about what you’re saying, it’s often an indication that context is missing, or the prose is unclear. Make sure you fix the text, rework the structure or add glossary and pointers, so that the same questions don’t pop up again and again. On the contrary, feel free to disregard comments in the third category (insisting on irrelevant details, or not useful). You should own the review process. Don’t get dragged into irrelevant discussions.\nStructure There\u0026rsquo;s no perfect structure. You\u0026rsquo;ll see that different formats are used or even imposed in different companies. I personally don\u0026rsquo;t think imposing a structure is useful, except for more inexperienced teams and authors, or in companies where reviews are very formalized and structured.\nMy personal go-to starter structure for a technical design is the following:\nTitle + metadata (author, date, status, issue link) Objective Requirements Background Proposal Alternatives considered From this structure, I will add or remove sections depending on the domain and audience. I might, for example, put more emphasis in security considerations and therefore add a specific section for that.\nNote also that this structure will not work for retrospectives, nor project proposals, where the emphasis is on different points (the past in the former, the responsibilities and problem analysis in the latter).\nAlternatives Amazon\u0026rsquo;s 6-pager The famous Amazon\u0026rsquo;s 6-pager comes with a prescribed structure and length. The prose is basically a wall of text without links, nor pictures (except for the appendix). The review is a formalized meeting where leads read print-outs of the document. The focus is really on an in-person, synchronous review, which requires lots of preparation and can be painful. The differences stop there. The rest of the approach is the same as the one I described: focus on the audience, clarity and simplicity, reasoning.\nI personally prefer asynchronous and incremental reviews, as these tend to be cheaper and less emotionally charged. However, given how successful Amazon is, their approach is clearly working as well :)\nArchitecture Decision Records An alternative is Architecture Decision Records (ADRs). Their focus is on the decisions being made and their consequences. The proposed structure is: context, decision, status and consequences. I really like the approach because of its simplicity. What are you changing, how, and what is the expected result? What is becoming easier, and what harder as a consequence? These are the questions answered by ADRs, which help to make decisions but also serve as documentation for the future.\nThe only thing I\u0026rsquo;m missing from ADRs is the discussion about alternative options and why they weren\u0026rsquo;t picked. They can nevertheless serve as a very good starting point for a design document.\nDo nothing The most important alternative, I would always like to see in a design, is the default \u0026ldquo;do nothing\u0026rdquo; option. I\u0026rsquo;m going to add it here as well: what about not using design docs at all? For example, doing decisions through very small, incremental changes that can be discussed within a team, or having a lead making those decisions. One prominent proponent of this idea is 37 Signals, which suggests not using designs at all and if really forced, \u0026ldquo;this process shouldn\u0026rsquo;t take more than one day\u0026rdquo;. The focus is on mocks and coding instead.\nThere are areas where this can work just fine, especially with two-way doors decisions, or UI changes for an app. I didn\u0026rsquo;t find this working well enough when decisions impact multiple teams or are not easily reversible. For example, security or data protection changes cannot be made without deliberate discussions. Large architectural changes or new technologies also require scrutiny and discussion. Failing that, organizations will rely on decisions being made by the quickest person implementing them, or leaders in an untransparent way. Both of which result in suboptimal and sometimes disastrous results. A common example of that is introducing a new technology just because someone wants to learn it, rather than solving a real problem. We all know how this ends: people introducing this new technology leave and the remaining are stuck with an inconsistent architecture. This can happen multiple times, until the result is so unnecessarily complex that every successive change requires a lot of discussion or is plain impossible. If design docs are not used here either, this means many repetitive meetings and slow progress.\nConclusions Design documents should not be a pain to read and write. They should not slow people down, nor make them feel stupid. They should be used as a shift-left tool to iterate on ideas collaboratively, efficiently and transparently. Furthermore, they should serve as an accelerator for quality and knowledge sharing.\nTo facilitate moving from painful to useful, keep in mind the following key goals of a doc:\nFacilitate understanding. Drive alignment. Translate that into practice by paying attention to the following principles:\nKnow your audience Get the timing right Explain reasoning, discuss alternatives Keep scope and abstraction level consistent Keep it simple and short Put important information first Add metadata Use data and metrics to help quantify problems and solutions (when necessary) Come up with a memorable title Obviously, this is coming from my own experience. Your mileage may vary and so forth. I would be definitely curious to hear different opinions on the matter!\nReferences In writing this, I also took inspiration from the following references:\nScott Adams, The day you became a better writer. Design docs at Google. William Zinsser, On writing well. Kubernetes KEP process. Rust RFC process. ","permalink":"https://blog.mbrt.dev/posts/better-design-docs/","summary":"\u003cp\u003e\u003cstrong\u003eWhat\u003c/strong\u003e is a design document? \u003cstrong\u003eWhen\u003c/strong\u003e is it useful to have one? \u003cstrong\u003eHow\u003c/strong\u003e to\nmake it useful? These are the questions I\u0026rsquo;m going to address in this post. The\ngoal is to provide some bite sized, easy to remember guidelines. My promise to\nyou is that after this post, you\u0026rsquo;ll be faster and more effective in making and\nreviewing designs.\u003c/p\u003e\n\u003cp\u003eThis is a written version of a talk I gave at my current employer. Since it was\nwell received and helped a few folks, I decided to make it more available.\u003c/p\u003e","title":"Better design docs"},{"content":"Running a Site Reliability Engineering (SRE) organization correctly is difficult and expensive. Spare the frustration, perhaps what you need is Sysadmins.\nSince the term was coined by Ben Treynor in 2003 at Google, lots of ink was spent on praising SRE practices. Not enough on when it is appropriate to have SREs. This post is a take on that angle.\nDisclaimer: I was an SRE at Google and this piece represents only my own views.\nWhat is an SRE? Among the various definitions floating around, let\u0026rsquo;s use the most common from the Google SRE books. SRE is both a set of practices and a job title. Ultimately, the idea is to solve operational problems through automation and share responsibilities with developers. These are the most important principles of SRE:\nOperations is a software problem (i.e. you need Software Engineers). Manage by Service Level Objectives (SLOs) (i.e. measure to take decisions about reliability vs feature velocity). Work to minimize toil (i.e. manual work is bad). Automate this year\u0026rsquo;s job away (again, manual work is bad. Use automation). Move fast by reducing the cost of failure (i.e. reduced impact of faults increases dev velocity). Share ownership with developers (i.e. SRE is not a gatekeeper, devs co-own the outcomes). SREs can fulfill their mission from different angles (consultation for devs, on-call expertise, improvements in internal platform or migrating to newer infrastructure). But SREs are primarily software engineers. They are encouraged to improve the reliability of systems through software, as opposed to manual work.\nThe biggest contribution from the SRE, and more widely from the DevOps movement, is recognizing that reliability work can be specialized engineering work and at the same time should not be treated as a silo from development.\nWhat can go wrong This is all nice and more or less obvious nowadays. But what can go wrong?\nMany common problems and solutions are discussed in the canonical Google SRE books. The focus there is however on “day 1\u0026quot;. The goal of the books is to move from the “old world\u0026quot; of ops into a new one, where devs and ops are not siloed and reliability improvements are measurable.\nHere I want to focus only on day 2 problems: I have SREs implemented By The Book and it\u0026rsquo;s 2023. Is it all ponies and rainbows?\nUnfortunately Things can go wrong at all levels. They can go wrong for SRE teams in many more ways than developer teams. I think this comes from the fact that even though SRE is a young discipline, it also crystallized quickly. In its 20 years of life, many things changed in the software domain. The definition of SRE did not (and stayed quite narrow).\nMost of the problems I\u0026rsquo;ve encountered so far can be categorized as follows:\nPower dynamics where SRE is at the short end. Mismatch in expectations between SREs and business needs. Lack of influence in setting priorities. Failure to acknowledge the rise of platforms. What the hell am I talking about? Let\u0026rsquo;s go in order.\nThe vicious cycle of being Somewhat Irrelevant Here\u0026rsquo;s a handy picture:\nWhen a team starts, it never does with a lot of influence or trust from other parts of the organization. This happens despite all efforts and it\u0026rsquo;s normal. Everyone needs to prove themselves when they begin a new job and this applies to newly formed teams as well.\nThis lack of trust translates into lack of influence to the reference dev organization. Which leads to a lack of Big Enough Projects and therefore Impact. There will be a seniority ceiling nobody in the team can ever smash. The exact level depends on the specifics, but it\u0026rsquo;s usually quite lower with respect to the one in the dev organization.\nIf this stays true for long enough, senior people move to teams where they can more easily find impact. The remaining people cannot get promoted and the vicious cycle continues. Interestingly, this can\u0026rsquo;t be fixed by adding senior engineers to the team. You can try, but internal transfers will be joining begrudgingly (and so either refuse to go or leave at the first opportunity) and external hires will have a very hard time ramping up (and follow a similar destiny).\nSuccess stories do happen in these teams as well. They are usually single people who can swim upstream and find a niche for themselves. These stories are nice and give hope, but they are never reproducible. Try the exact same steps and you will fail. In these teams people are disconnected from each other (as projects are relatively small) so you can\u0026rsquo;t look up to more senior people and do the same to get promoted. It\u0026rsquo;s very hard to escape the vicious cycle of being Somewhat Irrelevant.\nJustifying proactive action is hard Much of what SRE does (and wants to do) is proactive. You work today to solve tomorrow\u0026rsquo;s problems. The challenge is that SRE does not operate in isolation. You always need to convince dev partners to be onboard with your plans. And this is hard because:\nSee above: you might lack influence. Adding features is always more shiny than fixing old crap. In economic downturns or when the company has to play catch-up with competitors, the focus is on today, not tomorrow. Before devs are convinced of doing major changes to accommodate reliability improvements, you will need multiple major incidents. At that point, the necessity of improvements will be clear, but did you really need SRE to realize it? Perhaps you\u0026rsquo;ll win an I-Told-You-So badge of honor, but not much more.\nIn the meantime, SRE needed to shoulder the incidents and the consequent manual work to keep things running anyway.\nThis state can be temporary but could also be somewhat permanent. Operational work shoots up and all projects that require dev collaboration are at the back of their queue. Justifying proactive action is hard!\nYou might think: \u0026ldquo;But I know how to solve this. Give back the pager!\u0026rdquo; If that\u0026rsquo;s your thought, please continue below.\nPower dynamics Oftentimes the SRE organization is at the short end in power dynamics. This comes from multiple factors, but the most common causes are:\nThe business requires SRE to operate critical services. The dev organization is effectively funding SRE. The first one is easy to understand. You can\u0026rsquo;t give back the pager if the business explicitly forbids you to do so. This seems like an anti-pattern, but think about it. The CEO wants to protect their own crown jewels. What\u0026rsquo;s the best way to do it? Give the pager to whomever\u0026rsquo;s best to resolve incidents. The business doesn\u0026rsquo;t care whether you\u0026rsquo;re a happy on-caller or not. It just cares that things keep running.\nThe second is more insidious. Most tech companies are organized so that development organizations are effectively the ones that hold budgets. They may decide that part of the budget is used to fund SRE. Even though SRE is a parallel organization (i.e. mostly independent from a management perspective), it is in effect controlled by the developer org, through funding.\nThis control doesn\u0026rsquo;t have to be explicit. But it\u0026rsquo;s enough to skew the incentives on the SRE management side. For example, in thinking that to grow their SRE team they need to onboard new services. Even if nobody forces them, it\u0026rsquo;s still a powerful argument to get funding: \u0026ldquo;I need more people if you want me to support more services\u0026rdquo;.\nGiving back the pager removes the lever and goes in the opposite direction. It will prevent new funding or even trigger a team dismantling. This is bad for promo. No SRE director will be promoted to \u0026ldquo;senior director\u0026rdquo; by managing fewer teams, so they don\u0026rsquo;t do it.\nThese forces make sure that SRE holds the pager, no matter what.\nObviously there are exceptions to this. I have seen successful pagers handover, but they are exceptionally rare.\nPlatforms eating SRE Brief history of how production platforms are born.\nIn the beginning there was Chaos. Dev teams maintained their own infrastructure, making the same mistakes over and over. To reign the chaos, SRE was born. They brought a unified perspective to multiple teams, providing guidance by virtue of experiencing what was or wasn\u0026rsquo;t working in production (the \u0026ldquo;Wisdom of Production\u0026rdquo;).\nThen the company grows bigger and SRE teams multiply. They all want to automate themselves out of their job and so develop automation. Many Different Versions Of It. And the same observation applies: we need to provide unified production to the whole company. And a platform team is born.\nThe SREs job now shifts from \u0026ldquo;automate your toil away\u0026rdquo; to \u0026ldquo;bring service X to The Platform\u0026rdquo;. All is well. But what happens next?\nThe SRE book says:\nA production platform with a common service structure, conventions, and software infrastructure made it possible for an SRE team to provide support for the \u0026ldquo;platform\u0026rdquo; infrastructure, while the development teams provide on-call support for functional issues with the service—that is, for bugs in the application code. Under this model, SREs assume responsibility for the development and maintenance of large parts of service software infrastructure, particularly control systems such as load shedding, overload, automation, traffic management, logging, and monitoring.\n\u0026ndash; SRE Book [Chapter 32]\nAll is well? Not quite, as platforms are developed and maintained by a product team, not SRE!\nThe implications are interesting:\nSRE doesn\u0026rsquo;t own the platform, so they can\u0026rsquo;t directly change it to suit their needs. Supporting the platform itself is not a job for 1000 SREs, so only a few get to work on it. Incentives in the platform team are on minimizing maintenance cost, which means a lot of feature requests get shoved under the rug. Platform teams don\u0026rsquo;t need to please their customers so much if they get them through company mandates. This last point is the real problem. The business wants to minimize cost by de-duplicating work. They do so by mandating the use of the Blessed Platform. This makes sense, but creates perverse incentives. Guess who loses in this? SRE, because they are \u0026ldquo;such a small customer\u0026rdquo; compared to the rest of the company. Remember, there are many more devs than SREs. The platform will always try to prioritize problems for the majority of their customers.\nIn addition, platforms ate up a chunk of interesting work from SRE (automation). As fun as it sounds, SREs haven\u0026rsquo;t automated themselves out of their job, but only out of its most interesting part.\nMismatch in expectations SREs are first and foremost engineers. In many cases they come from pure software development and even have PhDs. They come to SRE expecting to have a software engineering job (i.e. building systems, researching cutting edge technologies) and just apply their skills to the \u0026ldquo;reliability domain\u0026rdquo;.\nWell, it often doesn\u0026rsquo;t work that way. Yes, there might be a 50% cap on toil, but what do SREs need to do with the rest of their time? From migrating one technology to the next to changing obscure configuration files, the reality for most of them is not so interesting.\n\u0026ldquo;What about SLOs or monitoring?\u0026rdquo; I hear you saying. PMs should own SLOs because they own the user experience. Platforms should own the SLO and alerting implementation, because there\u0026rsquo;s no need to reinvent the wheel in every team. What\u0026rsquo;s left for SRE? Well, they can put the little number in that config file there.\nTo be fair, not all of it is easy work. Migrations are especially delicate and need planning. Stakes are high and mistakes expensive. But is it interesting work? Changing one number from X to Y and waiting a week for a rollout? This feels like walking on ice for miles and miles. Challenging but just tedious.\nIt\u0026rsquo;s important work. You just don\u0026rsquo;t need a PhD or 15 years of experience to do it.\nThis mismatch in expectations is a very common experience for SRE new hires. Ultimately, most of the frustration comes from a disconnect between what an SRE is paid for (i.e. what the business wants from them) and what they want to be doing. The hard question is what value is SRE adding to the business?\nSuccessful engineers in platforms don\u0026rsquo;t automatically make good incident responders and vice-versa. Insisting on bundling together the two roles has several negative repercussions on teams.\nUnclear business value Signs that an SRE team has unclear business value:\nThe dev organization is too small. This happens when e.g. SREs are on-call for most services of the org and it\u0026rsquo;s hard to negotiate what services to focus on. Services are not critical to the business. Did anyone notice when one of your services was down for two hours? SLOs are always red, paging or ticketing, but no actual user complains about it. These are signs that you are being on-call not to serve the end users (and so the business), but to \u0026ldquo;serve the devs\u0026rdquo; and ease their operational burden.\nThis can also happen for just a part of the services the SREs are on-call for. It\u0026rsquo;s often not a straightforward picture.\nPartial solutions What if we take Chapter 32 of the SRE book to the letter? If the production platform can automate most ops tasks, SRE could give back all the services to the devs and just be on-call for the platform. This would have two immediate effects:\nDrastically reduce the number of SRE teams necessary. Many more devs start to be on-call 24/7. The company saves a bunch of money with the first point, but loses a bunch more with the second. There are way more dev than SRE teams and, contrary to SREs, a dev team is often in a single location. This means that a lot of people will be on-call during the night (because there\u0026rsquo;s no team across the ocean to hand off the pager to), which is very expensive and bad for retention (stressed out people tend to leave).\nMoney is one thing, effectiveness is the other. Can devs actually manage incidents effectively? They can definitely do it when the cause is within the service itself (e.g. a bug). But many outages happen in the cracks between services. The land of nobody. It often happens that an outage is caused by a service dependency, but the people managing the dependency can\u0026rsquo;t see anything wrong with their service.\nIncidents involving multiple teams require coordination. However there\u0026rsquo;s no SRE team to escalate to. It would be wrong to use platform SREs to do that, unless the platform itself is at fault. There won\u0026rsquo;t be a single team in charge of the overall incident, nor any team to escalate this to.\nThis setup overlooks also a third problem, which is operational expertise for critical services. There\u0026rsquo;s always a need for incident response experts for services that are both complicated and critical.1 The developers of those services may not have the skills to be good at that. And for this type of service, incidents must be resolved quickly (i.e. the business requires it).\nWe then have three unresolved problems:\nEveryone being on-call is expensive. Missing expertise for incident response in critical user facing services. Missing incident response on large (i.e. multi-service) outages. Let\u0026rsquo;s fix that?\nIncident Response Teams to the rescue Problem #2 seems to suggest a specialized on-caller role. Yes, critical services could still be managed by their dev teams. BUT, given the criticality, there\u0026rsquo;s still room for incident specialized responders. This is \u0026ndash; surprise surprise \u0026ndash; an ops role.\nProblem #3 is also for an ops role. This clearly requires specialization, but not necessarily SRE. This team needs strong systems understanding and incident response skills, but it doesn\u0026rsquo;t have to go beyond that. There\u0026rsquo;s no need for them to automate anything. In fact, incidents in this category are oftentimes black swan-like events, and as such, difficult to predict and unlikely to reoccur the same way. Their focus is on ops during incidents. Spare time should focus on post-mortems and consultation with the platform team and the business deciders. They are the best people that can answer the question: what are the biggest reliability risks right now?\nMinimizing cost The problem of cost (problem #1) is a bit more complicated to solve. There\u0026rsquo;s no way around having literally everyone building a service also being on-call and responsible for it. Shit happens, but these are the people having the best hand at fixing short and long term problems. If the dev team is paged every night for a month, they\u0026rsquo;ll fucking see at fixing the problem, trust me. The wisdom of production doesn\u0026rsquo;t really apply to SRE only. Wisdom arrives to anyone exposed to how systems behave. SRE shouldn\u0026rsquo;t rob developers this learning opportunity.\nThis, and constant improvements coming from the platform, should drive the need for routine ops work down. Emergencies outside business hours become more painful, so teams will strive to minimize them:\nMinimize critical dependencies. Stronger incentives to not build fucking Rube Goldberg machines, but better systems. No releases on public holidays. … and so on. I think you get the gist. Run your own shit. You\u0026rsquo;ll get the wisdom of production.\nThis will make it so that besides critical services (which should be minimized anyway), there will not be much need for highly reactive work. This will drive the number of teams that need to be on-call with guaranteed response time down (mitigating problem #1). This will not eliminate on-call in dev teams. It will be much less stressful and cheaper.\nIf a service considered not a critical dependency suddenly becomes important in an outage, the global incident response team should have the permissions to fix whatever they need, or page the shit out of anybody that can help. And this should be very rare.\nPutting everything together This is how things would roughly look like:\nIncident Response Teams (IRT) are responsible for both:\nemergency response for critical (i.e. revenue impacting) services, and providing an escalation path during large incidents. Is this the same as SRE by another name? No! Note how:\nRoutine operations are always a responsibility of the devs. Emergency operations are also mostly on the dev teams, except for a small number of critical services. Building a reliable system is still a dev responsibility. IR teams are only responsible for incident management and coordination. The teams can be quite a lot smaller than SRE, thanks to the reduced responsibilities and a pervasive platform that makes operations (e.g. drain a cluster, roll back a release, etc) look the same across the board.\nThis avoids most of the problems presented above: there\u0026rsquo;s no real need to influence the devs to make the system more reliable. It\u0026rsquo;s mostly on them anyway. Expectations are clear: incident response and ops. No engineering work required nor expected. The business value is 100% clear. They only work on incidents that impact revenue, which are important by definition.\nCareer I see one problem with this role: career and prestige. After years of Internet People bashing on ops (because it\u0026rsquo;s manual, inefficient, etc), the profession has now become unattractive. This is a marketing problem. I\u0026rsquo;m firmly convinced that ops will not go away anytime soon. Never mind AIs, bots, automation. Incidents happen and the more automation, the nastier they become. Yes, the profession needs to evolve from turning machines on and off to carefully operating complex tools. But it\u0026rsquo;s still operating things, just at a higher level of abstraction.\nI also see clear career paths where the better one becomes, the higher the influence over the business. From e.g. ops for a single (critical) service to coordinating large scale incidents across multiple teams. There\u0026rsquo;s a clear career progression from service IRT to company-wide IRT. Bigger scope, bigger responsibilities, more expertise required. It is only applicable to companies of a certain size, true. But this is true for most career paths in tech.\nI see a clear evolution (and career) path for Sysadmins here. From ssh-ing and rebooting machines to operating higher-level tools and influencing the business.\nConclusion In the software operations space, SRE and DevOps movements were fundamental innovations. They brought to attention important principles2. There\u0026rsquo;s however a common misconception: you need SRE to apply SRE principles.\nThere\u0026rsquo;s a perfectly valid alternative, which is\u0026hellip; just apply the principles? Developers themselves can run most services. In a world where platforms apply best practices, are mostly automated and devs know how to develop scalable services, the potential benefit of SRE teams is vastly reduced.\nCan the picture be as black for SRE as I paint it? As always, the answer depends. Depends on the maturity of the company and on how much SRE leadership is invested in running things As They Always Did. Some teams see the effects more than others. Some are shielded because they are closer to infrastructure.\nI think most of this really comes from a fact of ownership. SRE doesn\u0026rsquo;t really own systems end-to-end. Results are better when teams are empowered and responsible for the full lifecycle and outcome of their product.\nI also want to stress that this is not specific to Google only, even if that\u0026rsquo;s the place I know best. The pressures are the result of power dynamics that happen in many tech companies. Perhaps not with the same speed, but they are happening. If you are in a small company, you\u0026rsquo;re probably thinking I\u0026rsquo;m talking nonsense. And I am to some extent. This only applies to companies a certain size.\nThe goal of this little piece of mine was not to blame anyone or Rage Against The Machine. This is one of those cases where everyone\u0026rsquo;s best intentions cause a bad situation. I mostly wanted to bring to light some negative dynamics of “day 2 in SRE\u0026quot;.\nPart of the unfortunate situation is also the amount of gaslighting, because so many people have spent lots of time promoting SRE. Talking about the problems is much harder. If you are an SRE leader, you also don\u0026rsquo;t want your teams to shrink to a fraction of what they were, so you fight against the current. And deny the evidence.\nConversely, I don\u0026rsquo;t have a vested interest in any of these dynamics. Or at least, not anymore.\nBy critical I mean either directly impacting revenue (e.g. Ads serving in Google) or be a mandatory dependency of a revenue-critical service (e.g. a load balancer service).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nLike \u0026ldquo;running production is a shared responsibility\u0026rdquo;, or that \u0026ldquo;you need SLOs with consequences\u0026rdquo;.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://blog.mbrt.dev/posts/no-need-sre/","summary":"\u003cp\u003eRunning a Site Reliability Engineering (SRE) organization correctly is difficult\nand expensive. Spare the frustration, perhaps what you need is Sysadmins.\u003c/p\u003e\n\u003cp\u003eSince the term was coined by Ben Treynor in 2003 at Google, lots of ink was\nspent on praising SRE practices. Not enough on \u003cem\u003ewhen\u003c/em\u003e it is appropriate to have\nSREs. This post is a take on that angle.\u003c/p\u003e\n\u003cp\u003eDisclaimer: I was an SRE at Google and this piece represents only my own views.\u003c/p\u003e","title":"Maybe you don't need SRE"},{"content":"This is the story of me trying to win a game of chess against my brother. A single freaking game. What\u0026rsquo;s so special about it? Am I good at chess? Not at all. Did I improve at my game in the process? Also no. Is it a story about \u0026ldquo;the journey rather than the destination\u0026rdquo;? Not really. Did I at least have fun in the process? Not so sure. This is the story of me trying to be unconventional at probably the most studied game in existence and using my software engineering background for something that probably doesn\u0026rsquo;t need it.\nAlthough I\u0026rsquo;m a total disaster at chess and this post is useless to whoever seriously wants to improve at their game, I still think it was worth sharing how it\u0026rsquo;s possible to apply software engineering principles to a problem. Was I successful? You\u0026rsquo;ll see at the end.\nIntro: why I got into chess During the 2020 COVID19 pandemic, my brother, along with many other people, took a passion for playing online chess. After playing for a couple of months he started speaking very enthusiastically about it and challenging other family members, but where my father would budge (and get digitally butchered) I wouldn\u0026rsquo;t. For one thing, I refrained myself from delving into a potentially very time consuming hobby. I knew enough about chess to understand that to become even a mediocre amateur club player you\u0026rsquo;d still need to sink hundreds if not thousands of hours in the game. I admit I also didn\u0026rsquo;t like the thought of losing against my brother, which was certain at the time, since he already had hundreds of games under his belt and I had none.\nOne day I finally gave in and accepted a challenge from him. Needless to say that I lost completely. I knew the rules and the rudimentary of the game, having played a little bit as a kid, but my brother was obviously no match. Looking at the post-game analysis in chess.com I saw that my disadvantage only grew, move by move, until reaching a +9 evaluation (equivalent to having lost one rook, a bishop and a pawn to zero), which is beyond any hope for a comeback, where I resigned. This blueprint was followed during another couple of matches, where I understood I had to do something to avoid being demolished every time.\nThis was when I decided I wanted to study the game a bit more.\nFirst attempt: learn My first attempt in trying to improve at the game was to do the obvious: head to Reddit and YouTube to see what other learners recommended. Between a tutorial from GM Naroditsky, some reading and puzzle solving on Lichess, I also played a few games with random people on the Internet. My rating stayed pretty low nevertheless (1300 - 1400 Rapid on Lichess).\nAfter another couple of lost matches against my brother, it dawned on me that I had no hope of beating him. I was following his same steps to improve (playing, studying the game, watching videos), but I was dedicating much less time. At that point he was playing hundreds of games a month and I was willing to play maybe 10. At that pace I was only going to get further and further behind.\nIt was at that point that I had my second realization: I didn\u0026rsquo;t really care about the game, I didn\u0026rsquo;t want to really improve at chess in general, I only cared about beating a single person: my brother.\nSecond attempt: study the opponent A chess match can be generally divided into three phases: the opening, the middlegame and the endgame. Converting a significant advantage into a victory during an endgame is usually \u0026ldquo;easy\u0026rdquo;, after studying some basic checkmate patterns, so the question for me was how to get that advantage in the first place. Gaining advantage during a middlegame is usually achieved by long-term strategy and tactics. The first can be improved by reading and studying the game principles (something I can enjoy) while the second is only possible by doing puzzles (which I don\u0026rsquo;t particularly like doing). I knew that I would be at a disadvantage here, given that my brother used to do about 20 puzzles a day on chess.com, something I would never be able to catch up to. This only left one possibility: gaining advantage during the opening.\nChess opening theory is humongous and involves memorizing long sequences and variations of moves, along with possible replies from the opponent. Beginners don\u0026rsquo;t really need to memorize much, but some familiarity with the most common openings can go a long way (or so I was told). What I tried then was to look at some random games that my brother played and try to study the openings he was using. I looked at the Italian opening and Sicilian defense on Lichess and tried to memorize the basic ideas behind them. I also watched a bunch of videos on YouTube.\nObviously my brother had already done all of this before me (and better) and so I understandably lost again. Not to mention that memorizing meaningless (at least to me) opening moves is boring and laborious. I didn\u0026rsquo;t really have fun doing any of that. Another issue was that after my opponent deviated from the known book moves, I had no idea about how to react, because I didn\u0026rsquo;t really understand the positions.\nIt was time to step back and think again. I realized then I wasn\u0026rsquo;t really trying to beat my brother, but I was trying to improve my game against opponents that played his same openings perfectly. Could I be more specific? Could I prepare against my brother\u0026rsquo;s weaknesses instead? Obviously this wouldn\u0026rsquo;t work against any player other than him, but that still satisfied my goal.\nThird attempt: engineering The problem to solve became: find positions out of the opening that my brother (PlayerX from now on, for simplicity) would likely reach and be at a disadvantage. Remember that neither of us is an expert of the game and at our level players don\u0026rsquo;t play very accurately. The only way to play against a good player would be to follow book moves accurately, because you at least know in advance there\u0026rsquo;s no move they can make to gain an advantage. The story is different when you play against a club player. You can take risks (i.e. be temporarily at a disadvantage) if you know that your opponent is unlikely to be able to find the correct response and so get into trouble.\nI also had a list of over 500 games my brother played on chess.com. Being a software engineer, it came natural to me to approach this as any other engineering problem.\nI started by downloading the games he played by using the chess.com APIs and split them between black and white games. I focused on the games he played as black, given that I felt I had better chances at guiding the game where I wanted if I played as white.\nimport json import requests def get_month_games(player, yyyy_mm): url = \u0026#39;https://api.chess.com/pub/player/{}/games/{}\u0026#39; r = requests.get(url.format(player, yyyy_mm)) if not r.ok: raise Exception(\u0026#39;get_month_games failed\u0026#39;) games = json.loads(r.content) # Format: {games: [{url, pgn}, ...]} return games[\u0026#39;games\u0026#39;] # ... import chess.pgn import io import json with open(\u0026#39;games.json\u0026#39;) as f: data = json.load(f) games = [] for game in data: pgn = io.StringIO(game) games.append(chess.pgn.read_game(pgn)) black_games = [g for g in games if g.headers[\u0026#34;Black\u0026#34;] == \u0026#34;playerx\u0026#34;] Then I formulated the problem in this way: \u0026ldquo;Given all the positions PlayerX has seen, what are the ones that he\u0026rsquo;s likely to reach out of the opening where he is at a substantial disadvantage?\u0026rdquo;.\nNow I had a well formulated problem and I was finally playing in a domain I was familiar with. I decided to do my analysis in Python and in particular to use a Jupyter notebook, because I didn\u0026rsquo;t really want to create a reusable tool, but only to explore the available data and find one solution. It turns out Python already has pretty amazing libraries to manipulate chess games: python-chess (moves generation, validation, visualization) and python stockfish (bindings to evaluate a chess position by using the famous Stockfish chess engine).\nI translated the problem into a graph problem in this way: A node is a particular chess position (described in FEN notation). An edge links two nodes where the destination is reachable from the source position by doing a valid move. There\u0026rsquo;s a special initial node that is common to all games: the initial position.\nI then constructed the graph of all games played by PlayerX as black, additionally annotating every edge with the number of times the particular move was played.\nclass GamesGraph(): def __init__(self): self.graph = igraph.Graph(directed=True) def add_move(self, start_fen, end_fen, uci): vs = self._ensure_vertex(start_fen) vt = self._ensure_vertex(end_fen) try: e = self.graph.es.find(_source=vs.index, _target=vt.index) e[\u0026#34;count\u0026#34;] += 1 except: e = self.graph.add_edge(vs, vt) e[\u0026#34;uci\u0026#34;] = uci e[\u0026#34;count\u0026#34;] = 1 @property def start_node(self): return self.graph.vs.find(chess.STARTING_FEN) def _ensure_vertex(self, fen): try: return self.graph.vs.find(fen) except: v = self.graph.add_vertex(name=fen) v[\u0026#34;fen\u0026#34;] = fen v[\u0026#34;turn\u0026#34;] = chess.Board(fen).turn return v What results is weighted directed graph (not a tree because a position can be reached with different sequences of moves) similar to this one (a synthetic one because the real one would be too big to fit here):\nHere the initial position is the squared node, the color indicates whether from that position it\u0026rsquo;s black or white\u0026rsquo;s turn.\nI also wanted an evaluation of each position in terms of advantage for white and to do so I used Stockfish. Given that the process of evaluating thousands of positions is somewhat time consuming, I decided to do that separately and create a JSON object mapping each unique FEN position to its Stockfish evaluation.\nfrom stockfish import Stockfish stock = Stockfish(parameters={\u0026#34;Threads\u0026#34;: 8}) stock.set_depth(20) stock.set_skill_level(20) def eval_pos(fen): stock.set_fen_position(fen) return stock.get_evaluation() # fens is a map between a FEN string and a node of the graph. for fen, node in graph.fens.items(): node.eva = eval_pos(fen) The evaluation is returned in centipawn advantage or \u0026ldquo;mate-in X moves\u0026rdquo;, where a positive number means advantage for white and negative is an advantage for black:\n{\u0026#34;type\u0026#34;:\u0026#34;cp\u0026#34;, \u0026#34;value\u0026#34;:12} # 12 centipawns advantage for white. {\u0026#34;type\u0026#34;:\u0026#34;mate\u0026#34;, \u0026#34;value\u0026#34;:-3} # Black has mate in three. 100 centipawns represent the advantage of having one more pawn than your opponent and 300 is a minor piece like a bishop. Note however that Stockfish assigns a value to pieces depending on their position, so it\u0026rsquo;s entirely possible to have an advantage of 1000 even if the pieces on the board are equal.\nI needed to map this evaluation into something more manageable, like a number between 0 and 1. To do so, I decided arbitrarily that an advantage of 300+ is mapped to 1.0 and a disadvantage of 300+ is mapped to 0. Additionally, any mate in X (even if X is 20) is 1 or 0.\n# Returns [-1;1] def rating(ev, fen): val = ev[\u0026#34;value\u0026#34;] if ev[\u0026#34;type\u0026#34;] == \u0026#34;cp\u0026#34;: # Clamp to -300, +300. Winning a piece is enough. val = max(-300, min(300, val)) return val / 300.0 # Mate in X: also max rating. if val \u0026gt; 0: return 1.0 if val \u0026lt; 0: return -1.0 # This is already mate, but is it for white or black? b = chess.Board(fen) return 1.0 if b.turn == chess.WHITE else -1.0 # Returns [0;1], where 0 is min, 1 is max advantage for black. def rating_black(ev, fen): return -rating(ev, fen) * 0.5 + 0.5 The information was then all there, I just needed to find nodes in the graph (i.e. positions) where black was at a disadvantage, along with the sequence of moves that was most likely to reach it. I needed to weigh the edges in such a way that it was possible to easily compute the probability to reach a certain position. My reasoning was as follow:\nAt every position, we can evaluate the probability of doing a certain move by dividing the number of times the corresponding edge was taken by the total number of moves done from that position. Every edge will now have a weight between 0 and 1, where the higher the number, the higher the probability the edge will be taken from that position. The probability of a certain path is then the product of the probability of all the traversed edges. To solve the problem with standard graph algorithms I needed to transform the weights at the edges in such a way that:\nThey represent a distance instead of a probability (i.e. the longer the distance, the lower the probability of the path). The distance between two nodes is the sum of the weights of the traversed edges (as opposed to the product of probabilities). This is actually easier to do than to explain. The actual formula is very simple:\ndistance(e) = -log(prob(e)) Or, in Python:\ndef compute_edges_weight(vertex): all_count = sum(map(lambda x: x[\u0026#34;count\u0026#34;], vertex.out_edges())) for edge in vertex.out_edges(): prob = edge[\u0026#34;count\u0026#34;] / all_count edge[\u0026#34;prob\u0026#34;] = prob edge[\u0026#34;weight\u0026#34;] = -math.log(prob) Taking the logarithm of the probability of an edge will give a negative number, because the probability is between 0 and 1. We don\u0026rsquo;t have to worry about the case of probability zero (which would shoot the logarithm to minus infinity), as every edge of the graph has been taken at least once. The lower the probability, the more negative the logarithm will be, so inverting its sign will make it satisfy our requirements, because:\nThe sum of logarithms is the same as the logarithm of the product of their arguments: log(a) + log(b) = log(a*b). The bigger the result, the lower the underlying probability. Equipped with this data, we can compute the shortest path between the initial node and all other nodes by using Dijkstra\u0026rsquo;s algorithm. The result is a mapping between every node and the shortest path to the initial position, which represents the sequence of moves most likely to land in that position.\nAt that point I arbitrarily chose a minimum advantage and sorted the paths by probability. The first few paths represented my best chances to gain an advantage out of the opening against PlayerX.\nTweaks What did I find? This was a position returned by the algorithm above (white to move):\nAs you can see the situation for black is pretty bad (+8.9 according to Stockfish), because g6, the last move for black, was a mistake. White will go on, take the e5 pawn and the rook. The game for black is pretty much over, as they scramble to save the knight, the h7 pawn and the bishop. Another result was this one (white to move):\nWhich is mate in one move (Scholar\u0026rsquo;s mate).\nThe problem here is that these were mistakes done several times by PlayerX only during his first matches and never repeated again. Early queen attacks are usually carried out by very inexperienced players and they are effective only against players at that level. PlayerX hasn\u0026rsquo;t fallen for that trap for a long time afterwards, because better opponents don\u0026rsquo;t play that kind of move! I knew that I couldn\u0026rsquo;t really use this opening, because PlayerX knew how to defend against it now and would not fall for it anymore.\nAnother problem was related to sequences of moves that happened only once, but coming from common positions. The probability of the final position was the same as the probability of the last common position, because every edge had a probability of 1.0 (given that no other possibilities have been played). In the example below (edges marked with their probabilities), you can follow the edges with 7 and 6 (the most common position at move 2), but then follow one of the edges with a 1. From that point on, all the subsequent moves will have been played only once (because only a single match reached that position) and so every step will have a probability of 1.0.\nAnd this is how the probabilities look like:\nThis is intuitively incorrect, as it\u0026rsquo;s improbable that the same exact sequence of moves will be played with absolute certainty. We don\u0026rsquo;t have enough games being played from those positions to know that.\nThe famous quote (from Brewster?) \u0026ldquo;In theory there is no difference between theory and practice, while in practice there is\u0026rdquo;, was true in this case as well, so I needed a few tweaks and manual inspection to find better candidate positions.\nTo correct the second problem I decided to put an upper bound to the probability of an edge, so long sequences of moves played only once will gradually lose probability.\ndef compute_edges_weight(vertex, prob_ceiling=0.9): all_count = sum(map(lambda x: x[\u0026#34;count\u0026#34;], vertex.out_edges())) for edge in vertex.out_edges(): # Certainty doesn\u0026#39;t exist... Let\u0026#39;s put a probability ceiling (default 90%). prob = min(edge[\u0026#34;count\u0026#34;] / all_count, prob_ceiling) edge[\u0026#34;prob\u0026#34;] = prob edge[\u0026#34;weight\u0026#34;] = -math.log(prob) For the first problem I just manually screened out bad suggestions. At the end of the day I only needed one or two good positions to work on.\nOne more tweak was related to the fact that I didn\u0026rsquo;t want white\u0026rsquo;s probabilities to affect the probability of the paths, because I was playing white and could decide which path to take. For that reason I set all whites probabilities to 1.0 (a zero weight). The end result was a graph like this one:\nPreparation The position I settled on studying was this one:\nAccording to Lichess this is an Alekhine defense (two pawn attack). In this position there\u0026rsquo;s only one good move for black (Nb6) and black is still at a slight disadvantage (+0.6 according to Stockfish). However, from that position PlayerX often plays Nf4, which is bad (+2.3). I created a study in Lichess and started looking at several variations (good moves and moves played by PlayerX). The end result was a tree of possibilities that I tried to memorize and understand. For example I needed to know what a move like d5 was threatening, why the move Nf4 was bad and prepare the best responses.\nI didn\u0026rsquo;t spend much time doing this because I got bored pretty quickly, but I did prepare a bit for the upcoming match.\nThe match As if I were predicting the future, in my match against PlayerX, we got into an Alekhine defense. Put under pressure he did end up blundering his knight at move 5. Turns out even players much better than you end up making one mistake after another when they are at a disadvantage. It\u0026rsquo;s easy to play accurately when you\u0026rsquo;re winning, but can you keep your cool when you are losing? At move 10 I was at a +7.1 advantage, pretty much impossible to lose, but I was also out of my preparation. Look at how cramped black\u0026rsquo;s position is and how my pieces are all pointing towards the enemy\u0026rsquo;s king:\nI started making a bunch of mistakes from that point on, but I nevertheless was able to keep a non trivial advantage until move 27:\nUnfortunately I was very low on time (it was a rapid 10 minutes game) and so I had to move quickly. I ended up messing up completely move 32 and 33, giving my half-dead opponent mate in one :/.\nHere\u0026rsquo;s the full match (blunders and all):1\nConclusion What did I learn from this endeavour? A few things, most of which seem obvious in retrospect:\nPreparing for a specific opponent can give a considerable edge in the opening. Players at lower levels aren\u0026rsquo;t good at punishing dubious moves from the opponent. Getting into tricky positions where only one response is correct are easy ways to gain an advantage. The opening isn\u0026rsquo;t everything. If you are bad at time management and tactics, it\u0026rsquo;s possible to lose completely winning positions. Chess games can be decided by one bad move. Studying the game is important and there\u0026rsquo;s no silver bullet if your opponent is much better than you, but narrowing the skill gap is possible with specific preparation. Applying software engineering principles to chess is fun. Doing it to have a chance at beating your brother is even more fun! I hope I\u0026rsquo;ll be able to do it one day :) You can find the code I used in my GitHub repo. Note that I did not include the data and the code is quite messy, but I hope it can be of some inspiration, especially for folks that are considering whether computer science might be for them or not. Look, you can solve \u0026ldquo;real world\u0026rdquo; problems with it, it\u0026rsquo;s not just moving bits around!\nThat\u0026rsquo;s all folks, I hope I\u0026rsquo;ll be able to win a match against my brother some day, but until then, I\u0026rsquo;ll keep trying\u0026hellip; my own way.\nFeedback on Hacker News.\nOriginal usernames are edited out because I didn\u0026rsquo;t ask my brother\u0026rsquo;s permission to post the match. I also still hope to try this trick one more time on him before he finds out :)\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://blog.mbrt.dev/posts/chess-eng/","summary":"\u003cp\u003eThis is the story of me trying to win a game of chess against my brother. A\nsingle freaking game. What\u0026rsquo;s so special about it? Am I good at chess? Not at\nall. Did I improve at my game in the process? Also no. Is it a story about \u0026ldquo;the\njourney rather than the destination\u0026rdquo;? Not really. Did I at least have fun in\nthe process? Not so sure. This is the story of me trying to be unconventional\nat probably the most studied game in existence and using my software\nengineering background for something that probably doesn\u0026rsquo;t need it.\u003c/p\u003e","title":"Engineering a chess match against my brother"},{"content":"Over the last year, at work I had multiple chances to debug how containers work. Recently we had to solve some networking problems a customer had with Kubernetes, and I decided I wanted to know more. Once the problem was solved, I spent more time on investigating what is actually going on under the hood. After seeing the wonderful Eric Chiang and Laurent Bernaille talks, and reading through the very informative posts by Lizzie Dixon and Julia Evans (that I really really recommend), I got enough information about how a container is created and managed. I\u0026rsquo;m going to rip off and mix some stuff from their awesome posts in the first part of mine.\nWhat I missed in those talks was the networking part. How do containers talk to each other? In Bernaille\u0026rsquo;s talk there is some information, but I after seeing the video I was still not convinced completely. I was especially interested about how Calico works, and for that I could find very little information.\nTo answer this kind of questions I will try to create containers from scratch, by using just standard Linux commands. I will also setup the networking to make them happily communicate, again from scratch. I like this approach because it gets low level enough to demystify things that look very complicated, while it\u0026rsquo;s just a matter of spending some time to understand the basics.\nThis post is an extended version of a talk I gave internally at my company, trying to shed some light on the subject.\nPrerequisites for a good understanding are some basic networking and Linux concepts:\nthe OSI model, and in particular level 2 and 3; IP networking and the CIDR notation; NAT (Network Address Translation). I will link the advanced topics as the post unfolds.\nContainers from scratch Rise your hand if you ever tried the magic of Docker at least once. You pull an image from the Internet, you run it and you are projected inside another OS, with different libraries and applications installed, and all of that in no time. But how magic is a container after all? Is it composed by very complicated tools? Is it a sort of virtual machine? In the first part of this post I\u0026rsquo;m going to create a container from scratch, by using only a Linux shell and standard Linux commands, to try to answer these questions.\nPrepare the image When you do a docker pull you are downloading a container image from the Internet. This image at its core is basically just a root filesystem. You can safely ignore the fact that it\u0026rsquo;s composed by multiple stacked layers, because the end result is just a root filesystem.\nSo we can try to make our own, and for this post I decided to go with Alpine Linux, because it\u0026rsquo;s small and it\u0026rsquo;s different from my distribution.1 Needless to say that for this to work you have to be running on Linux and with a fairly recent Kernel. I haven\u0026rsquo;t checked the specific requirements, but if you updated your system in the last 5 years,2 you\u0026rsquo;re probably good to go.\nBe powerful, be root. You\u0026rsquo;ll save yourself a lot of sudo invocations and annoying \u0026ldquo;permission denied\u0026rdquo; messages:\nsudo su Download the mini root filesystem from the Alpine website and put it somewhere. Then extract it:\nmkdir rootfs cd rootfs tar xf ../alpine-minirootfs-3.6.2-x86_64.tar.gz if you look there, you\u0026rsquo;ll see the root filesystem:\n[root@mike-dell rootfs]# ls -l total 64 drwxr-xr-x 2 root root 4096 Aug 13 16:22 bin drwxr-xr-x 4 root root 4096 Jun 17 11:46 dev drwxr-xr-x 15 root root 4096 Aug 13 16:26 etc drwxr-xr-x 2 root root 4096 Jun 17 11:46 home drwxr-xr-x 5 root root 4096 Aug 13 16:22 lib drwxr-xr-x 5 root root 4096 Jun 17 11:46 media drwxr-xr-x 2 root root 4096 Jun 17 11:46 mnt dr-xr-xr-x 2 root root 4096 Jun 17 11:46 proc drwx------ 2 root root 4096 Aug 13 16:08 root drwxr-xr-x 2 root root 4096 Jun 17 11:46 run drwxr-xr-x 2 root root 4096 Jun 17 11:46 sbin drwxr-xr-x 2 root root 4096 Jun 17 11:46 srv drwxr-xr-x 2 root root 4096 Jun 17 11:46 sys drwxrwxrwt 2 root root 4096 Jun 17 11:46 tmp drwxr-xr-x 7 root root 4096 Jun 17 11:46 usr drwxr-xr-x 13 root root 4096 Aug 13 16:22 var chroot Now let\u0026rsquo;s try to chroot there. In this way we create a process and change its root directory to the one we just created:\nchroot rootfs /bin/ash export PATH=/bin:/usr/bin:/sbin This will execute a shell inside the chroot environment. Side note: exporting a new $PATH (the second command) is wise, because otherwise you\u0026rsquo;d be carrying your host $PATH in the chroot, and this might not be correct there. So where are we exactly?\n/ # cat /etc/os-release NAME=\u0026#34;Alpine Linux\u0026#34; ID=alpine VERSION_ID=3.6.2 PRETTY_NAME=\u0026#34;Alpine Linux v3.6\u0026#34; HOME_URL=\u0026#34;http://alpinelinux.org\u0026#34; BUG_REPORT_URL=\u0026#34;http://bugs.alpinelinux.org\u0026#34; Yes, in Alpine Linux. And you can\u0026rsquo;t reach your host files anymore, because your root directory is now the one we just chroot-ed into.\nLet\u0026rsquo;s now install some useful packages. They\u0026rsquo;ll come in handy for later:\napk add --no-cache python findmnt curl libcap bind-tools Another thing we have to fix now is the /proc filesystem. If you look there you\u0026rsquo;ll see that it\u0026rsquo;s empty so any utility like ps won\u0026rsquo;t work:\nmount -t proc proc /proc Now a question for you: Is this actually a container?\nSort-of, but the isolation is pretty poor. Take a look at ps aux from the \u0026ldquo;container\u0026rdquo;:\n/ # ps aux PID USER TIME COMMAND 1 root 0:03 {systemd} /sbin/init 2 root 0:00 [kthreadd] 3 root 0:00 [kworker/0:0] 4 root 0:00 [kworker/0:0H] 6 root 0:00 [mm_percpu_wq] 7 root 0:00 [ksoftirqd/0] 8 root 0:01 [rcu_preempt] 9 root 0:00 [rcu_sched] 10 root 0:00 [rcu_bh] 11 root 0:00 [migration/0] 12 root 0:00 [watchdog/0] 13 root 0:00 [cpuhp/0] 14 root 0:00 [cpuhp/1] 15 root 0:00 [watchdog/1] 16 root 0:00 [migration/1] 17 root 0:00 [ksoftirqd/1] 19 root 0:00 [kworker/1:0H] ... 2816 1170 0:00 top oops\u0026hellip; I can see all the processes of my host from here. An I can actually kill them:\nkillall top Not only that. Look at the network:\n/ # ip link 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 3: wlan0: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc fq_codel state UP qlen 1000 link/ether 40:49:0f:fe:c3:05 brd ff:ff:ff:ff:ff:ff You can see my WiFi card for example. I could change the IP, take it down, etc. Not nice. The answer is then NO, this is not a container, because it\u0026rsquo;s not isolated enough. This is just a process in a different root filesystem.\nNamespaces Linux has namespaces to the rescue. As man 7 namespaces says:\nA namespace wraps a global system resource in an abstraction that makes it appear to the processes within the namespace that they have their own isolated instance of the global resource. Changes to the global resource are visible to other processes that are members of the namespace, but are invisible to other processes. One use of namespaces is to implement containers.\nor in other words: we take a resource like the list of processes in the machine, we make an isolated copy of it, give it to our process and make sure that any change there is not reflected to the root process list. This is the PID namespace. Is it hard to set up? Judge by yourself:\nunshare -p -f chroot rootfs /usr/bin/env -i \\ HOME=/root \\ PATH=/bin:/usr/bin:/sbin:/usr/sbin \\ /bin/ash -l With this command from the host, we create a new process (the chroot we used before) but we put it in a new PID namespace by prepending the unshare -p invocation. This command is nothing fancy, just a handy wrapper around the unshare Linux system call. The env command executed after the chroot makes sure that the environment is correctly filled, avoiding us to repeat the export command every time.\nLet\u0026rsquo;s take a look at the list of processes now, after we mount /proc again:\n/ # mount -t proc proc /proc / # ps PID USER TIME COMMAND 1 root 0:00 /bin/ash 5 root 0:00 ps Oh yes. Now our shell is actually PID 1. How weird is that? And yes, you won\u0026rsquo;t be able to kill any host process.\nFrom the host you can instead see the containerized process:\n[root@mike-dell micheleb]# ps aux |grep /ash root 8552 0.0 0.0 1540 952 pts/3 S+ 20:06 0:00 /bin/ash and kill it if you want to.\nThe PID is not the only namespace you can create, as you can imagine. The network for example is still the host one:\n/bin # ip link 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 3: wlan0: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc fq_codel state UP qlen 1000 link/ether 40:49:0f:fe:c3:05 brd ff:ff:ff:ff:ff:ff Let\u0026rsquo;s isolate it then. It\u0026rsquo;s just a matter of adding some flags to unshare:\nunshare -pmn -f chroot rootfs /usr/bin/env -i \\ HOME=/root \\ PATH=/bin:/usr/bin:/sbin:/usr/sbin \\ /bin/ash -l here we are isolating the PID, mount and network namespaces, all at once. And here is the result:\n# / ip addr 1: lo: \u0026lt;LOOPBACK\u0026gt; mtu 65536 qdisc noop state DOWN qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 # / ping -c1 8.8.8.8 PING 8.8.8.8 (8.8.8.8): 56 data bytes ping: sendto: Network unreachable Pretty isolated I would say. Topic of the next section will be how to open a little hole in this isolation and get some containers to communicate somehow.\nBefore to move on I\u0026rsquo;d like to put a little disclaimer here. Even though I\u0026rsquo;m done with this section, it doesn\u0026rsquo;t mean that with an unshare command you get a fully secure container. Don\u0026rsquo;t go to your boss and say that you want to toss Docker and use shell scripts because it\u0026rsquo;s the same thing.\nWhat our container is still missing is, for example, resource isolation. We could crash the machine by creating a lot of processes, or slow it down by allocating a lot of memory. For this you need to use cgroups.3 Then there\u0026rsquo;s the problem you are still root inside the container, You are limited but you are still pretty powerful. You could for example change the system clock, reboot the machine, and other scary things. To control them you\u0026rsquo;d need to drop some capabilities.4 I won\u0026rsquo;t dig into these concepts in this post, because they don\u0026rsquo;t affect the networking. All of that involves just simple Linux system calls and some magic in the /proc and /sys/fs/cgroup/ filesystems.\nI point you though to the excellent resources I linked at the beginning, especially Eric Chiang and Lizzie Dixon, if you are more curious. I could also write another post on that in the future.\nI hope I nevertheless convinced you that a container is nothing more than a highly configured Linux process. No virtualization and no crazy stuff is going on here. You could create a container today with just a plain Linux machine, by calling a bunch of Linux syscalls.\nNetworking from scratch Goal of this section will be to break the isolation we put our container in, and make it communicate with:\na container in the same host; a container in another host; the Internet. I\u0026rsquo;m running this experiment in a three nodes cluster. The nodes communicate through a private network under 10.141/16. The head node has two network interfaces, so it\u0026rsquo;s able to communicate with both the external and the internal network. The other two nodes have only one network interface and they can reach the external network by using the head node as gateway. The following schema should clarify the situation:\nCommunicate within the host Right now our container is completely isolated. Let\u0026rsquo;s try to at least ping the same host:\n/# ping 10.141.0.1 PING 10.141.0.1 (10.141.0.1): 56 data bytes ping: sendto: Network unreachable It\u0026rsquo;s not working, so the network is isolated. No matter what you do you won\u0026rsquo;t be able to reach the outside, because the only interface you have there is the loopback (and it\u0026rsquo;s also down).\n/# ip link 1: lo: \u0026lt;LOOPBACK\u0026gt; mtu 65536 qdisc noop state DOWN qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 If you create another container on the same host, you can imagine they\u0026rsquo;re not going to be able to communicate either.\nHow do we solve this problem? We use a veth pair, which stands for Virtual Ethernet pair. As the name suggests, a veth pair is a pair of virtual interfaces, that act as an Ethernet cable. Whatever comes into one end, goes to the other. Sounds useful? Yes, because we can move one end of the pair inside the container, and keep the other end in the host. So we are basically piercing a hole in the container to slide our little virtual wire in.\nIn another shell, same host, let\u0026rsquo;s setup a $CPID variable to help us remember what is the container PID:5\nCPID=$(ps -C ash -o pid= | tr -d \u0026#39; \u0026#39;) Let\u0026rsquo;s create the veth pair with iproute,6 move one end into the container and bring the host end up:\nip link add veth0 type veth peer name veth1 ip link set veth1 netns $CPID ip link set dev veth0 up If you take a look at the interfaces in the container now, you\u0026rsquo;ll see something like:\n/# ip l 1: lo: \u0026lt;LOOPBACK\u0026gt; mtu 65536 qdisc noop state DOWN qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 3: veth1@if4: \u0026lt;BROADCAST,MULTICAST,M-DOWN\u0026gt; mtu 1500 qdisc noop state DOWN qlen 1000 link/ether 8e:7f:62:52:76:71 brd ff:ff:ff:ff:ff:ff Cool! Everything is down, but we have a new interface. Let\u0026rsquo;s also rename it to something less scary, like eth0. You\u0026rsquo;ll feel more home in the container:\nip link set dev veth1 name eth0 address 8e:7f:62:52:76:71 where the address used is the MAC address shown by ip link, or ip addr show dev veth1.7\nNow let\u0026rsquo;s step back for a second. We have a container with this \u0026ldquo;cable\u0026rdquo; pointing out. What kind of IP should we give to the container? What kind of connectivity do we want to provide? The way we are going to set it up is the default Docker way: bridge networking. Containers on the same host live on the same network, but different than the host one. This means that we have to setup a virtual network where containers are able to talk to each other at level 2. This also means that we won\u0026rsquo;t consume any physical IP address from the host network.\nI choose the 172.19.35/24 subnet for the containers, since it doesn\u0026rsquo;t conflict with the cluster private network (10.141/16).8 This means that I have space for 2^8 - 2 = 30 containers in this machine.9\nNow let\u0026rsquo;s give the container an IP and bring it up, along with the loopback interface:\nip addr add dev eth0 172.19.35.2/24 ip link set eth0 up ip link set lo up And this is the current situation:\nNow we want do to the very same thing with another container. So let\u0026rsquo;s create it from the same root filesystem:\nunshare -pmn -f chroot rootfs /usr/bin/env -i \\ HOME=/root \\ PATH=/bin:/usr/bin:/sbin:/usr/sbin \\ /bin/ash -l mount -t proc proc /proc Then in the host we setup another $CPID2 variable with the PID of this new container,10 and then create another veth pair:\nip link add veth2 type veth peer name veth3 ip link set veth3 netns $CPID2 ip link set dev veth2 up Then rename the interface in the container, give it an IP and bring it up as before:\nip link set dev lo up MAC=$(ip addr show dev veth3 | grep \u0026#39;link/ether\u0026#39; | tr -s \u0026#39; \u0026#39; | cut -d\u0026#39; \u0026#39; -f3) ip link set dev veth3 name eth0 address $MAC ip addr add dev eth0 172.19.35.3/24 ip link set eth0 up Note that I\u0026rsquo;m using another IP address in the 172.19.35/24 subnet. This is the situation right now:\nWhat we need to do here is try to link those two veth pairs together, in a way that they can communicate at layer 2. Something like… a bridge! It will take care of linking together the two network segments. It works at level 2 like a switch (so it basically \u0026ldquo;talks Ethernet\u0026rdquo;), by \u0026ldquo;enslaving\u0026rdquo; existing interfaces. You add a bunch of interfaces into a bridge, and they will be communicating with each other thanks to the bridge.\nLet\u0026rsquo;s create the bridge and put the two veth interfaces in it:\nip link add br0 type bridge ip link set veth0 master br0 ip link set veth2 master br0 Now let\u0026rsquo;s give the bridge an IP and bring it up:\nip addr add dev br0 172.19.35.1/24 ip link set br0 up Now we have this topology in place:\nAs you can see, now the containers can ping each other:\n/ # ping 172.19.35.3 -c1 PING 172.19.35.3 (172.19.35.3): 56 data bytes 64 bytes from 172.19.35.3: seq=0 ttl=64 time=0.046 ms --- 172.19.35.3 ping statistics --- 1 packets transmitted, 1 packets received, 0% packet loss round-trip min/avg/max = 0.046/0.046/0.046 ms Let\u0026rsquo;s check the ARP table11 on the first container:\n/ # ip neigh 172.19.35.3 dev eth0 lladdr c6:b3:e3:1d:97:7b used 40/35/10 probes 1 STALE So this means that these two containers are on the same network, and can talk to each other at level 2. And here is indeed the ARP request going through:\n[root@node001 ~]# tcpdump -i any host 172.19.35.3 22:55:37.858611 ARP, Request who-has 172.19.35.3 tell 172.19.35.2, length 28 22:55:37.858639 ARP, Reply 172.19.35.3 is-at c6:b3:e3:1d:97:7b (oui Unknown), length 28 Reach the internet If you try to reach the external network, or even the host IP, you\u0026rsquo;ll see that it\u0026rsquo;s still not working. That\u0026rsquo;s because to reach a different network you need some kind of level 3 communication. The way Docker sets it up by default is with natting.12 In this way, the 172.19.35/24 network will be invisible outside the host and mapped automatically into the host IP address, that in my case is 10.141.0.1 (which by the way is still a private IP, and will be natted by the head node into the public IP).\nLet\u0026rsquo;s first enable IP forwarding, to allow the host to perform routing operations:\necho 1 \u0026gt; /proc/sys/net/ipv4/ip_forward Then insert a NAT rule (also called IP masquerade) in the external interface:\niptables -t nat -A POSTROUTING -o eth0 -j MASQUERADE Then you need to set the default route in the container:\nip route add default via 172.19.35.1 In this way any packet with a destination on a different network will be sent through the gateway, which is the bridge. From there it will be natted by eth0, our physical interface, and then sent through the cluster fabric by using the physical IP as source.\nThis is now the situation:\nIf I ping Google\u0026rsquo;s DNS from the container, I see this from the host:\n[root@node001 ~]# tcpdump -i any host 8.8.8.8 -n 23:27:51.234333 IP 172.19.35.2 \u0026gt; 8.8.8.8: ICMP echo request, id 13824, seq 0, length 64 23:27:51.234360 IP 10.141.0.1 \u0026gt; 8.8.8.8: ICMP echo request, id 13824, seq 0, length 64 23:27:51.242230 IP 8.8.8.8 \u0026gt; 10.141.0.1: ICMP echo reply, id 13824, seq 0, length 64 23:27:51.242251 IP 8.8.8.8 \u0026gt; 172.19.35.2: ICMP echo reply, id 13824, seq 0, length 64 As you can see the packet comes from the container, is translated into the host IP (10.141.0.1) and then when it comes back, the destination is replaced with the container IP (172.19.35.2).\nThis is what I see from the head node, instead:\n[root@head ~]# tcpdump -i any host 8.8.8.8 -n 23:25:20.209922 IP 10.141.0.1 \u0026gt; 8.8.8.8: ICMP echo request, id 13568, seq 0, length 64 23:25:20.209943 IP 192.168.200.172 \u0026gt; 8.8.8.8: ICMP echo request, id 13568, seq 0, length 64 23:25:20.217286 IP 8.8.8.8 \u0026gt; 192.168.200.172: ICMP echo reply, id 13568, seq 0, length 64 23:25:20.217310 IP 8.8.8.8 \u0026gt; 10.141.0.1: ICMP echo reply, id 13568, seq 0, length 64 As you can see the packet comes from the node, it\u0026rsquo;s forwarded through the head node public IP (192.168.200.172), and then comes back the other way around. NAT is also working here.\nReach a remote container Now from a container we are able to communicate with both another local container and with the externa network. The next step is to reach a container in another node, in the same physical private network (the 10.141/16 network the nodes sit in).\nThis is basically the plan:\nThe two nodes communicate through the physical private network 10.141/16. We want to assign a subnet to each node, so each will be able to host some containers. We have already assigned the 172.19.35/24 network to the first host. We can then assign another to the second, for example 172.19.36/24. I could have chosen any other IP range that doesn\u0026rsquo;t conflict with the existing networks, but this one is especially handy, because both of them are part of a bigger 172.19/16 network. We can think of it as the containers\u0026rsquo; network, in which every host gets a slice (a /24 subnet). This means that we can assign 24 - 16 = 8 bits to different hosts, so maximum 255 nodes. Of course you can use different network sizes to accomodate your needs, but that\u0026rsquo;s the way we are going to set it up here. NAT has been already setup in the first host, so we are going to do the same for the second one, and then add routing rules (layer 3) between the two hosts.\nLet\u0026rsquo;s go real quick over the second host, create a container, setup the networking there as we did for the first host:\nunshare -pmn -f chroot rootfs /usr/bin/env -i \\ HOME=/root \\ PATH=/bin:/usr/bin:/sbin:/usr/sbin \\ /bin/ash -l then in the host:\nCPID=$(ps -C ash -o pid= | tr -d \u0026#39; \u0026#39;) ip link add veth0 type veth peer name veth1 ip link set veth1 netns $CPID ip link set dev veth0 up ip link add br0 type bridge ip link set veth0 master br0 ip addr add dev br0 172.19.36.1/24 ip link set br0 up echo 1 \u0026gt; /proc/sys/net/ipv4/ip_forward iptables -t nat -A POSTROUTING -o eth0 -j MASQUERADE Note that I used the 172.19.36.1/24 IP for the bridge. Then in the container:\nip link set dev lo up MAC=$(ip addr show dev veth1 | grep \u0026#39;link/ether\u0026#39; | tr -s \u0026#39; \u0026#39; | cut -d\u0026#39; \u0026#39; -f3) ip link set dev veth1 name eth0 address $MAC ip addr add dev eth0 172.19.36.2/24 ip link set eth0 up ip route add default via 172.19.36.1 and again I use 172.19.36/24 here. Now the container is able to talk to the Internet, as the other one. But, is the first container able to reach this new container?\nTry to think about it.\nThen try to do it. No, it doesn\u0026rsquo;t work, but why? The answer is in the routing table of the first host:\n[root@node001 ~]# ip r default via 10.141.255.254 dev eth0 10.141.0.0/16 dev eth0 proto kernel scope link src 10.141.0.1 172.19.35.0/24 dev br0 proto kernel scope link src 172.19.35.1 There is a default gateway pointing to the head node, and two \u0026ldquo;scope link\u0026rdquo; ranges, for networks reachable at level 2 (unsurprisingly there are the 10.141/16 physical network, and the 172.19.35/24 network for the local containers). As you can see there\u0026rsquo;s no rule for 172.19.36/24. This means the packet will go through the default gateway, and from there it will try to go outside, because the head node doesn\u0026rsquo;t know anything about this IP either.\nWhat we should do is add a routing rule to the node table, telling that any packet for 172.19.36/24 should be forwarded to the second host, listening at 10.141.0.2:\nip route add 172.19.36.0/24 via 10.141.0.2 src 10.141.0.1 The same goes for the other host, but in reverse:\nip route add 172.19.35.0/24 via 10.141.0.1 src 10.141.0.2 And now, both containers are able to talk to each other. If you want to show something fancy, you could run NGINX in one container, and curl the beautiful default page from the other.\nHooray!\nBonus: Calico What I showed in the last section is basically how Docker sets up its bridge networking. The routing rules to make the containers see each other come from me. What Docker Swarm and other networking solutions for Docker use instead is usually overlay networking, like VXLAN. VXLAN encapsulate layer 2 Ethernet frames within layer 3 UDP packets. This provides layer 2 visibility to containers across hosts. I didn\u0026rsquo;t show this approach because the routing rules were simpler, and also because I prefer the Calico approach, that I will present in this section.\nSome of you may already know Kubernetes. It\u0026rsquo;s the most popular (any my favorite) container orchestrator. What it basically does is providing declarative APIs to manage containers. Restarts upon failures, replicas' scaling, upgrading, ingress, and many other things can be managed automatically by Kubernetes. For all this magic to happen, Kubernetes imposes some restrictions on the underlying infrastructure. Here is the section about the networking model:\nall containers can communicate with all other containers without NAT all nodes can communicate with all containers (and vice-versa) without NAT the IP that a container sees itself as is the same IP that others see it as. As the documentation says:\nCoordinating ports across multiple developers is very difficult to do at scale and exposes users to cluster-level issues outside of their control. Dynamic port allocation brings a lot of complications to the system - every application has to take ports as flags, the API servers have to know how to insert dynamic port numbers into configuration blocks, services have to know how to find each other, etc. Rather than deal with this, Kubernetes takes a different approach.\nThe solution we used in the previous section does not satisfy these requirements. In our case the source IP is rewritten by NAT, so the destination container sees only the host IP.\nThere are a number of projects that satisfy the Kubernetes requirements, and among them I really like Project Calico, so I\u0026rsquo;m going to reproduce its setup here, again the hard way, just Linux commands.\nThe Calico\u0026rsquo;s solution is to use layer 3 networking all the way up to the containers. No Docker bridges, no NAT, just pure routing rules and iptables. Interestingly enough, the way Calico distributes the routing rules is through BGP,13 which is the same way the Internet works.\nThe end result we\u0026rsquo;re going to aim at is this:\nLooks familiar? Yes, it\u0026rsquo;s almost the same as the one I used in the previous section. We\u0026rsquo;re going to use the same IP ranges: the host networking under 10.141/16, and we\u0026rsquo;re going to setup a 172.19/16 network for the containers. As before, every host gets a /24 subnet. The difference is in the way the packets are routed. With Calico everything goes at layer 3, so on the wire you\u0026rsquo;ll see packets coming from a 172.19/16 address and going to a 172.19/16 address because, as I said before, no natting or overlays are used.\nSetup the host network Without further ado, let\u0026rsquo;s create our container on the first host:\nunshare -pmn -f chroot rootfs /usr/bin/env -i \\ HOME=/root \\ PATH=/bin:/usr/bin:/sbin:/usr/sbin \\ /bin/ash -l Then, let\u0026rsquo;s create our veth pair, and move one end into the container:\nCPID=$(ps -C ash -o pid= | tr -d \u0026#39; \u0026#39;) ip link add veth0 type veth peer name veth1 ip link set veth1 netns $CPID ip link set dev veth0 up Let\u0026rsquo;s now give the container an IP address:\nip link set dev lo up MAC=$(ip addr show dev veth1 | grep \u0026#39;link/ether\u0026#39; | tr -s \u0026#39; \u0026#39; | cut -d\u0026#39; \u0026#39; -f3) ip link set dev veth1 name eth0 address $MAC ip addr add dev eth0 172.19.35.2/32 ip link set eth0 up Have you noted anything strange? I\u0026rsquo;m using a /32 address for the container IP. This means that whenever I send a packet, even for a container living on the same host, it will need to go through level 3. This allows to get rid of the bridge, and also makes sure that the container doesn\u0026rsquo;t try (and fail) to reach another at level 2, by sending useless ARP requests.\nNow on the host we need to enable ARP proxy for the veth interface.\necho 1 \u0026gt; /proc/sys/net/ipv4/conf/veth0/rp_filter echo 1 \u0026gt; /proc/sys/net/ipv4/conf/veth0/route_localnet echo 1 \u0026gt;/proc/sys/net/ipv4/conf/veth0/proxy_arp echo 0 \u0026gt;/proc/sys/net/ipv4/neigh/veth0/proxy_delay echo 1 \u0026gt;/proc/sys/net/ipv4/conf/veth0/forwarding What this does is basically replying to ARP requests with its own MAC address. In this way, when the container looks for the link local address, veth0 will say: \u0026ldquo;it\u0026rsquo;s me!\u0026rdquo;, replying with it\u0026rsquo;s own MAC address, and the packet will be sent there at layer 2.14\nWe also need to enable IP forwarding on the host\u0026rsquo;s physical interface, to allow routing:\necho 1 \u0026gt;/proc/sys/net/ipv4/conf/eth0/forwarding And inside the container we have to add a couple of routing rules:\nip r add 169.254.1.1 dev eth0 scope link ip r add default via 169.254.1.1 dev eth0 Here we use a local link address, so we don\u0026rsquo;t have to manage the IP of the other pair of the veth. We can assign the same address to all the veths, since the address is valid only within the link, so no routing will be performed by the kernel. We\u0026rsquo;ve also added a default route, that says to use that IP for any address outside of the local range. But since our local range is a /32, no IP is local. So, what we are saying to the kernel in the end is: \u0026ldquo;any time we want to reach something outside the container, just put it on the eth0 link\u0026rdquo;. It seems convoluted, but the idea behind it is quite simple.\nLast bit missing on the host is the rule to reach the container from the host:\nip r add 172.19.35.2 dev veth0 scope link With this we\u0026rsquo;re saying that, to reach the container, the packet has to go through the veth0 interface.\nNow, from the container we\u0026rsquo;re able to ping the host:\nnode001:/# ping 10.141.0.1 -c1 PING 10.141.0.1 (10.141.0.1): 56 data bytes 64 bytes from 10.141.0.1: seq=0 ttl=64 time=0.077 ms --- 10.141.0.1 ping statistics --- 1 packets transmitted, 1 packets received, 0% packet loss round-trip min/avg/max = 0.077/0.077/0.077 ms And this is the traffic passing:\n[root@node001 ~]# tcpdump -i any host 172.19.35.2 -n 16:25:10.439980 IP 172.19.35.2 \u0026gt; 10.141.0.1: ICMP echo request, id 6144, seq 0, length 64 16:25:10.440014 IP 10.141.0.1 \u0026gt; 172.19.35.2: ICMP echo reply, id 6144, seq 0, length 64 ARP goes back and forth to determine the physical address of the local link IP:\n[root@node001 ~]# tcpdump -i any host 172.19.35.2 16:25:15.453847 ARP, Request who-has 169.254.1.1 tell 172.19.35.2, length 28 16:25:15.453882 ARP, Reply 169.254.1.1 is-at f6:5c:53:b4:f8:03 (oui Unknown), length 28 and if you look at the ARP table you\u0026rsquo;ll see the cached reply:\nnode001:/# ip neigh 169.254.1.1 dev eth0 lladdr f6:5c:53:b4:f8:03 ref 1 used 2/2/2 probes 4 REACHABLE The 169.254.1.1 IP is the only one reachable at level 2 from the container, as expected. The MAC address corresponds to the other end of the veth pair, as you can see from the host:\n[root@node001 ~]# ip l show dev veth0 5: veth0@if4: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc pfifo_fast state UP mode DEFAULT qlen 1000 link/ether f6:5c:53:b4:f8:03 brd ff:ff:ff:ff:ff:ff link-netnsid 0 And this is the current situation:\nAnother detail is the blackhole route, to drop packets coming for unexisting containers:\nip r add blackhole 172.19.35.0/24 In this way any packet sent to the host subnet to an IP not present in the host will be dropped. Packets for exising containers still work, because their routing rules are more specific, so they take precedence:\n[root@node001 ~]# ip r default via 10.141.255.254 dev eth0 10.141.0.0/16 dev eth0 proto kernel scope link src 10.141.0.1 169.254.0.0/16 dev eth0 scope link metric 1002 blackhole 172.19.35.0/24 172.19.35.2 dev veth0 scope link In this case, if you send a packet to 172.19.35.2, it will go to veth0. If you instead try to reach 172.19.35.3, it will go to the blackhole and dropped, instead of going to the default gateway.\nReach a remote container To reach a container running on another host, you have to replicate the setup done for this host. You have to assign to that node another /24 subnet from the container network, and use one IP from that subnet to create a container (I used the 172.19.36/24 subnet, the same as Part 2).15\nThen you need to add the routing rules to direct the traffic to the right host. From the first host:\nip route add 172.19.36.0/24 via 10.141.0.2 src 10.141.0.1 and similarly from the second host:\nip route add 172.19.35.0/24 via 10.141.0.1 src 10.141.0.2 Done. Now the containers can reach each other. If you look at the traffic, you\u0026rsquo;ll see that the source and destination IPs are preserved, and not NATted, satisfying the Kubernetes\u0026rsquo; requirements:\n[root@node001 ~]# tcpdump -i any host 172.19.35.2 20:08:02.154031 IP 172.19.35.2 \u0026gt; 172.19.36.2: ICMP echo request, id 17152, seq 0, length 64 20:08:02.154045 IP 172.19.35.2 \u0026gt; 172.19.36.2: ICMP echo request, id 17152, seq 0, length 64 20:08:02.155088 IP 172.19.36.2 \u0026gt; 172.19.35.2: ICMP echo reply, id 17152, seq 0, length 64 20:08:02.155098 IP 172.19.36.2 \u0026gt; 172.19.35.2: ICMP echo reply, id 17152, seq 0, length 64 Success!\nReach the Internet If you are lucky you are able to reach the external network already. This all depends on how NAT is setup in your cluster. A proper setup should allow only packets coming from the physical network to escape.\nFrom my head node (that is also the default gateway of the other nodes), I see:\n[root@mbrt-c-08-13-t-c7u2 ~]# iptables -L -t nat Chain PREROUTING (policy ACCEPT) target prot opt source destination Chain INPUT (policy ACCEPT) target prot opt source destination Chain OUTPUT (policy ACCEPT) target prot opt source destination Chain POSTROUTING (policy ACCEPT) target prot opt source destination MASQUERADE all -- 10.141.0.0/16 anywhere This is precisely my case. Only packets coming from the 10.141/16 network, will be natted. To perform NAT also for packets coming from the containers network, I have to add another rule:\niptables -t nat -A POSTROUTING -o eth1 -j MASQUERADE -s 172.19.0.0/16 Looking this way in the table:\nMASQUERADE all -- 172.19.0.0/16 anywhere Then we need a routing rule in the head node, telling it where it can find the 172.19.35/24 subnet:\nip route add 172.19.35.0/24 via 10.141.0.1 src 10.141.255.254 And now, you can finally ping the outside network from the container!\nMissing pieces Among the feature that I haven\u0026rsquo;t discussed, Calico has a really nice distributed firewall, applied through iptables, but I left it out of scope from this post.\nBonus: Debug container networking In this section I would like to digress a bit and talk about debugging. I hope it\u0026rsquo;s clear at this point that containers aren\u0026rsquo;t magical, and networking isn\u0026rsquo;t magical either. This means that for debugging you can use all the regular tools Linux provides. You don\u0026rsquo;t need to rely on Docker or Calico to provide anything on their end, and even if they would, how do you debug them when they are broken? In the previous section I used ping, iproute and tcpdump, but what happens if your Docker image does not contain these tools?\nnode001:/# ip r /bin/ash: ip: not found This happens many times, and even worse if your Docker image looks like this:\nFROM scratch ADD main / CMD [\u0026#34;/main\u0026#34;] You don\u0026rsquo;t even have a console there. What do you do?\nEnter the nsenter magical world There is a very simple trick you should probably remember: nsenter. This command enters one or more namespaces from the host. You can enter all of them and in that case you would have another console open on the container (similar to the docker exec command):\nnsenter --pid=/proc/$CPID/ns/pid \\ --net=/proc/$CPID/ns/net \\ --mount=/proc/$CPID/ns/mnt \\ /bin/bash and look, we see the same processes as the container do:\n[root@node001 rootfs]# mount -t proc proc /proc [root@node001 rootfs]# ps aux USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND root 1 0.0 0.0 1540 548 pts/0 S+ 16:19 0:00 /bin/ash -l root 97 0.0 0.2 116144 2908 pts/1 S 20:25 0:00 /bin/bash root 127 0.0 0.1 139492 1620 pts/1 R+ 20:28 0:00 ps aux What\u0026rsquo;s most important for our purposes is accessing the network namespace though:\nnsenter --net=/proc/$CPID/ns/net /bin/bash this way you have the same network as the container, but no other restrictions. In particular you have access to the host filesystem:\n[root@node001 ~]# cat /etc/os-release NAME=\u0026#34;CentOS Linux\u0026#34; VERSION=\u0026#34;7 (Core)\u0026#34; ID=\u0026#34;centos\u0026#34; ID_LIKE=\u0026#34;rhel fedora\u0026#34; VERSION_ID=\u0026#34;7\u0026#34; PRETTY_NAME=\u0026#34;CentOS Linux 7 (Core)\u0026#34; ... and all your favorite tools available. But the network you see is the container one:\n[root@node001 ~]# ip r default via 169.254.1.1 dev eth0 169.254.1.1 dev eth0 scope link This, of course works with Docker too. Once you have the PID of your container, you can nsenter it:\n[root@node001 ~]# docker inspect --format \u0026#39;{{.State.Pid}}\u0026#39; my-awesome-container 24028 [root@node001 ~]# nsenter --net=/proc/24028/ns/net /bin/bash So, please, don\u0026rsquo;t install debugging tools in your Docker images anymore. It\u0026rsquo;s not really necessary.\nConcluding remarks With this long post I tried to reproduce two different solutions for container networking, with nothing more than Linux commands. Docker, Calico, Flannel and the others are all nice tools, but they aren\u0026rsquo;t magical. They build on top of standard Linux functionality, and trying to reproduce their behavior helped me (and I hope you too) to understand them better.\nKeep in mind that this is not a complete guide. There are many more interesting topics, like network policies and security in general, then a universe of different solutions, like overlay networks, Ipvlan, macvlan, MacVTap, IPsec, and I don\u0026rsquo;t know how many others. For containers in general there are many other things you want to isolate, like physical resources and capabilities, as I mentioned during the first part of this post. The overwhelming amount of technical terms shouldn\u0026rsquo;t discourage you to explore and expand your knowledge. You might find, like me, that it\u0026rsquo;s not as hard as it seems.\nThat\u0026rsquo;s all folks. Happy debugging!\nFootnotes I run my laptop with Arch Linux and I used CentOS 7 for my demo cluster.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nToo bad CentOS 6 users!\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nAgain, man 7 cgroups is your friend.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nI might be boring: man 7 capabilities.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThis snippet assumes your machine is running only one ash command.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nman ip. If you\u0026rsquo;re not familiar with it, today you have a good change to get started , because ifconfig has been long deprecated.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nHandy if you want to get it from a script, as a quick hack:\nMAC=$(ip addr show dev veth1 | grep \u0026#39;link/ether\u0026#39; | tr -s \u0026#39; \u0026#39; | cut -d\u0026#39; \u0026#39; -f3) \u0026#160;\u0026#x21a9;\u0026#xfe0e; Note that I\u0026rsquo;m using private IPv4 address spaces.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n24 bits are fixed by the network mask so I have only 8 bits to assign to hosts, but 172.19.35.0 is the network address, and 172.19.35.255 is the broadcast, so they aren\u0026rsquo;t usable.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA possibility would be to find it with ps aux, or if you\u0026rsquo;re lazy you could temporarily run a recognizable process and query it\u0026rsquo;s parent process from the host. I\u0026rsquo;m using top here:\nCPID2=$(ps -C ash -o ppid= | tr -d \u0026#39; \u0026#39;) \u0026#160;\u0026#x21a9;\u0026#xfe0e; The Address Resolution Protocol is responsible for translating IP addresses into MAC addresses. Every time a network device wants to communicate with an IP in the same subnet, the ARP protocol kicks in. It basically sends a broadcast packet asking to everybody: \u0026ldquo;how has this IP?\u0026rdquo;, and it saves the answer (IP address, MAC address) into a table. This way every time you need to reach that IP, you know already which MAC address to contact.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nNetwork Address Translation. This is the same mechanism your home router uses to connect you to the Internet. It basically maps all the internal network IPs into the only one that is externally available, and assigned to you by your ISP. Externally, only the router IP will be visible. So, when a packet is sent outside, the source address is rewritten to match the router external IP. When the reply comes back, the natting does the reverse, and replaces the destination address with the original source of the packet.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nSee also the Calico data path for some details.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nSome nice comments are present in the Calico source code about it. See intdataplane/endpoint_mgr.go:\n// Enable strict reverse-path filtering. This prevents a workload from spoofing its // IP address. Non-privileged containers have additional anti-spoofing protection // but VM workloads, for example, can easily spoof their IP. err := m.writeProcSys(fmt.Sprintf(\u0026#34;/proc/sys/net/ipv4/conf/%s/rp_filter\u0026#34;, name), \u0026#34;1\u0026#34;) if err != nil { return err } // Enable routing to localhost. This is required to allow for NAT to the local // host. err = m.writeProcSys(fmt.Sprintf(\u0026#34;/proc/sys/net/ipv4/conf/%s/route_localnet\u0026#34;, name), \u0026#34;1\u0026#34;) if err != nil { return err } // Enable proxy ARP, this makes the host respond to all ARP requests with its own // MAC. This has a couple of advantages: // // - In OpenStack, we\u0026#39;re forced to configure the guest\u0026#39;s networking using DHCP. // Since DHCP requires a subnet and gateway, representing the Calico network // in the natural way would lose a lot of IP addresses. For IPv4, we\u0026#39;d have to // advertise a distinct /30 to each guest, which would use up 4 IPs per guest. // Using proxy ARP, we can advertise the whole pool to each guest as its subnet // but have the host respond to all ARP requests and route all the traffic whether // it is on or off subnet. // // - For containers, we install explicit routes into the containers network // namespace and we use a link-local address for the gateway. Turing on proxy ARP // means that we don\u0026#39;t need to assign the link local address explicitly to each // host side of the veth, which is one fewer thing to maintain and one fewer // thing we may clash over. err = m.writeProcSys(fmt.Sprintf(\u0026#34;/proc/sys/net/ipv4/conf/%s/proxy_arp\u0026#34;, name), \u0026#34;1\u0026#34;) if err != nil { return err } // Normally, the kernel has a delay before responding to proxy ARP but we know // that\u0026#39;s not needed in a Calico network so we disable it. err = m.writeProcSys(fmt.Sprintf(\u0026#34;/proc/sys/net/ipv4/neigh/%s/proxy_delay\u0026#34;, name), \u0026#34;0\u0026#34;) if err != nil { return err } // Enable IP forwarding of packets coming _from_ this interface. For packets to // be forwarded in both directions we need this flag to be set on the fabric-facing // interface too (or for the global default to be set). err = m.writeProcSys(fmt.Sprintf(\u0026#34;/proc/sys/net/ipv4/conf/%s/forwarding\u0026#34;, name), \u0026#34;1\u0026#34;) if err != nil { return err } \u0026#160;\u0026#x21a9;\u0026#xfe0e; For the lazy reader I reported the whole sequence here. Create the container:\nunshare -pmn -f chroot rootfs /usr/bin/env -i \\ HOME=/root \\ PATH=/bin:/usr/bin:/sbin:/usr/sbin \\ /bin/ash -l Then from the host:\nCPID=$(ps -C ash -o pid= | tr -d \u0026#39; \u0026#39;) ip link add veth0 type veth peer name veth1 ip link set veth1 netns $CPID ip link set dev veth0 up echo 1 \u0026gt; /proc/sys/net/ipv4/conf/veth0/rp_filter echo 1 \u0026gt; /proc/sys/net/ipv4/conf/veth0/route_localnet echo 1 \u0026gt;/proc/sys/net/ipv4/conf/veth0/proxy_arp echo 0 \u0026gt;/proc/sys/net/ipv4/neigh/veth0/proxy_delay echo 1 \u0026gt;/proc/sys/net/ipv4/conf/veth0/forwarding echo 1 \u0026gt;/proc/sys/net/ipv4/conf/eth0/forwarding ip r add 172.19.36.2 dev veth0 scope link ip r add blackhole 172.19.36.0/24 and from the container:\nip link set dev lo up MAC=$(ip addr show dev veth1 | grep \u0026#39;link/ether\u0026#39; | tr -s \u0026#39; \u0026#39; | cut -d\u0026#39; \u0026#39; -f3) ip link set dev veth1 name eth0 address $MAC ip addr add dev eth0 172.19.36.2/32 ip link set eth0 up ip r add 169.254.1.1 dev eth0 scope link ip r add default via 169.254.1.1 dev eth0 \u0026#160;\u0026#x21a9;\u0026#xfe0e; ","permalink":"https://blog.mbrt.dev/posts/container-network/","summary":"\u003cp\u003eOver the last year, at work I had multiple chances to debug how containers work.\nRecently we had to solve some networking problems a customer had with\n\u003ca href=\"https://kubernetes.io/\"\u003eKubernetes\u003c/a\u003e, and I decided I wanted to know more. Once\nthe problem was solved, I spent more time on investigating what is actually\ngoing on under the hood. After seeing the wonderful\n\u003ca href=\"https://youtu.be/wyqoi52k5jM\"\u003eEric Chiang\u003c/a\u003e and\n\u003ca href=\"https://youtu.be/b3XDl0YsVsg\"\u003eLaurent Bernaille\u003c/a\u003e talks, and reading through\nthe very informative posts by\n\u003ca href=\"https://blog.lizzie.io/linux-containers-in-500-loc.html\"\u003eLizzie Dixon\u003c/a\u003e and\n\u003ca href=\"http://jvns.ca/blog/2016/10/10/what-even-is-a-container/\"\u003eJulia Evans\u003c/a\u003e\n(that I really really recommend), I got enough information about how a\ncontainer is created and managed. I\u0026rsquo;m going to rip off and mix some stuff from\ntheir awesome posts in the first part of mine.\u003c/p\u003e","title":"Demystifying container networking"},{"content":"Introduction In February 2014 I had a very unpleasant surprise after coming back home from a weekend in Berlin. Me and my girlfriend found out our apartment upside down because of burglars. We suffered the loss not only of many of our belongings, but worse, of our safety. We didn\u0026rsquo;t feel safe in our home and we didn\u0026rsquo;t feel safe to leave it - not even for half an hour - for the fear to come back and find somebody inside. It was with this mood that I started this project: totally motivated to do something about it.\nMy goal was to install in the apartment a security alarm that was cheap, safe and easy to use: something that I could trust. Nowadays there are plenty of alternatives that satisfy all the requirements, but at that moment I couldn\u0026rsquo;t find anything like that. As I am a software engineer, I decided to jump into the project and spend most of my free time on it (my girlfriend was only partially happy with this resolution).\nThe end result was a Raspberry Pi with a camera module and a bunch of software, in part taken from the open-source and in part written by me. As of the time of writing, it\u0026rsquo;s been running in production (namely my apartment) untouched for more than two years now. Ironically my security camera lasted way more than my apartment, because since then I already moved twice. So, quite a success for me.\nAnd, if you\u0026rsquo;re asking yourself if I had any burglars again since then, the answer is no, luckily. But I had the opportunity to test it multiple times when my parents have intruded my apartment without permission\u0026hellip;\nWhy? At this point you can probably see my motivation behind the project, and also the target audience: I needed something cheap that I, and my girlfriend could use. This set of users was important for the design. It meant that the normal usage should be simple, because my girlfriend (who is not a developer) had to use it. The configuration and maintenance was however on me, so I didn\u0026rsquo;t focus on make that simple.\nToday I\u0026rsquo;m open-sourcing the project. You can find it on GitHub under mbrt/antifurto. The name comes from an Italian word that means \u0026ldquo;security alarm\u0026rdquo;. It was a codename at the beginning, waiting to be changed in something better, but in the end I didn\u0026rsquo;t bother. So antifurto is still the name today.\nWhy am I open-sourcing that now, and not before? Why not keeping it closed source? The project started to meet my needs, so I had no reason at the beginning to open source something that was completely tied to my use case. Nobody would have gotten any particular benefit from it. After some time however it grown up into a more featured product. So, since at the time I wasn\u0026rsquo;t happy with my job, I considered starting a business and commercialize it. Long story short, getting funds for startups in Italy is quite hard, so I needed money from myself or my family. I then had to improve many parts of the project because I needed to scale out for multiple customers. It was not only about my apartment anymore. In the meantime competitors started to jump out from nowhere before I even got started. I had an opportunity for a new job, so my motivations kinda vaporized.\nAt the end of the day, there is not a commercial product, but a working prototype, and an interesting experience to share. I decided to open-source it now, because I needed time to put together this writeup and cleanup some documentation. All things that I find boring, so it took me a while.\nWhat? In this writeup I\u0026rsquo;m going to present the interesting bits of the project. I will try to not focus too much on the details, but rather to present clearly the high-level architecture, some design decisions and some interesting implementation bits.\nFeatures The antifurto project is essentially a security camera that allows to monitor what happens through the lenses of a single camera. When the camera detects motion above a certain threshold, it sends notifications through WhatsApp and emails and starts to record pictures. These are in turn saved to the local disk and uploaded to a Dropbox folder. There is also a web portal (optimized for desktop and mobile browsers) from which you can start and stop the monitoring, a live view from which you can see images in real time and an archive page for the past recordings. It\u0026rsquo;s not possible to combine multiple cameras together: single camera, single website.\nArchitecture The project architecture is simple. Everything lives inside a Raspberry Pi, period. There isn\u0026rsquo;t a server component communicating with the camera, or anything else. This is not an ideal architecture from the security point of view, because all the keys, including SSL, Dropbox API secrets, email passwords, etc are inside that box. It was the perfect solution for me, because instead of a device and a server to manage, I had only one device. Moreover, the development time was reduced because the architecture was simpler.\nThis unfortunately cannot work out if you want to provide the project to your mother (assuming she\u0026rsquo;s not a software developer). The Raspberry needs a non-zero amount of maintenance, to provide a minimum amount of security. This includes for example installing OS updates and rebooting the device periodically. The keys need to be safe guarded inside the Raspberry itself, and re-generated in case of leak. Again, this is not good if you want to do it properly, but I preferred to do something quick and get it working as fast as I could.\nNow for the details. The project is divided into three main parts:\nthe main antifurto executable (written in C++), which is responsible for the monitoring and notifications; a web server (Apache + PHP), that serves the website from which you can see the live stream, turn the monitoring on and off and view the archive; a FastCGI component that serves as a bridge between the webserver and the main executable. There are also other small satellite components and scripts, such as:\na bash script to send emails with the mail command; python scripts to send WhatsApp notifications and upload pictures to Dropbox. You can see below a diagram of the high level architecture:\nAs you can see, the pictures come from the camera module and are processed by the antifurto main executable. This decides whether to store the pictures on the local hard drive (an SD card), and upload them on Dropbox or not. It also decides, when to send notifications via email or WhatsApp messages. Whenever the user decides to start a live view from the web interface, or start/stop the monitoring, the backend sends a POSIX signal to the main process. If the desired action was to start the live view, the main executable will start to send the pictures over the zmq channel to the antifurto.fcgi component. Its only task is to forward them to the webserver via an FCGI socket.\nThe design is heavily based on observers, type erasure, composite reuse principle and SOLID principles, to minimize dependencies among components. Well, at least I tried to keep those in mind.\nIn the diagram you can see the architecture of the main executable. Each box represents a class. I didn\u0026rsquo;t represent all of them, but only the most important. For example I left out the utility classes like schedulers, queues, observer lists. The dark boxes represent controller classes, which are responsible for managing specific parts of the application. Controller classes manage all the boxes connected via a \u0026ldquo;tilted square\u0026rdquo; arrow. This means that they both own those classes (so they are responsible for their lifetime) and they know how to operate them. Red boxes don\u0026rsquo;t manage anything, but they provide a functionality either for other classes or talk to external services.\nOne important thing to notice is that each class is owned by one and only one controller. The architecture and the lifetime of the resources are very simple and clear in this way. A consequence is that classes can be tested individually much more easily, since there are no cyclical dependencies, and children don\u0026rsquo;t know anything about their parents.\nIn the diagram you can also see what are the inputs and outputs of each class. Red arrows are inputs, and dark arrows are outputs. You can see that I didn\u0026rsquo;t connect explicitly those arrows. Why? Because they are loose connections. Outputs are provided in the form of observers,1 and classes interested in pictures don\u0026rsquo;t know anything about the Camera class. So, controllers are responsible to \u0026ldquo;wire\u0026rdquo; those connections, by registering themselves to the inputs they need and forward them to the classes they manage. For example the RecordingController class register itself to both alarm notifications (provided by the MotionDetector) and the picture stream (provided by the Camera). It is managed by the MonitorController, so whenever the monitoring functionality is stopped, the recording classes can be safely deleted. The RecordingController then listens to alarm events and whenever one occurs, it forwards the pictures stream directly to the PictureArchive and the DropboxUploader.\nMain executable In this long section I\u0026rsquo;m going to talk about the internal details of the main executable, called antifurto for a very lack of fantasy.\nMain class The main class is called Antifurto, what a surprise! It is responsible to start and stop the monitoring and the live view, by orchestrating the resources involved. It uses a Config structure for the configuration, that comes from the command line and the configuration file. It can be used as an external library, as most of the components in this project, since it is self contained.\nIt contains all the controllers, that are described in the Main controllers section, and the implementation details are hidden from the header file behind a Pimpl.\nThe interface is very simple: it takes a configuration and the user can control when to start and stop monitoring and live view from four public methods:\nclass Antifurto { public: Antifurto(const Configuration\u0026amp; c, bool maintenanceNeeded = true); void startMonitoring(); void stopMonitoring(); void startLiveView(); void stopLiveView(); private: meta::ErasedUniquePtr\u0026lt;AntifurtoImpl\u0026gt; pimpl_; }; So, this class is all about the very high level use cases of configuring, starting and stopping the main functionalities.2\nThese functions are a bit less simple than one can at first imagine. For example the startMonitoring is anynchronous and starts the monitoring only after a configurable timeout. This is because after the start, the person may need to get out the way before the monitoring effectively starts. The default I\u0026rsquo;m using for myself is one minute. At the same time, the function needs to check if the user cancels the start request before the timer goes off. I needed to put some attention in the interaction between start, stop and the destructor. The CameraController lifetime depends on whether one between the monitoring and the live view functionalities are on:\nvoid handleCameraControllerNeed() { if ((liveViewActive_ || monitorActive_) \u0026amp;\u0026amp; !camera_) camera_.reset(new CameraController()); else if (!liveViewActive_ \u0026amp;\u0026amp; !monitorActive_) camera_.reset(); } This method is called by all the four external methods, to factor out this common part.\nMain controllers In this section I\u0026rsquo;m going to describe the three controllers that manage the monitoring, live view and the camera sub-components.\nMonitorController This class controls the monitoring functionality life cycle. It delegates to its sub-components tasks such as motion detection, and notifications. The most important part of its public interface is the examinePicture function:\nvoid examinePicture(const Picture\u0026amp; picture); The Main class calls this function whenever a new picture comes out of the camera.\nAnother interesting bit is the way this class asks for the upper level controller to change the picture capture interval, or to stop the recording altogether. To break cyclical dependencies, the upper level class has to instantiate the MonitorController by passing a couple of callbacks. One of them is the SetPicturesInterval:\nusing SetPicturesInterval = std::function\u0026lt;void(std::chrono::milliseconds)\u0026gt;; that is used whenever some motion is detected. In that case, the MonitorController asks for an increase of the capture frequency. It\u0026rsquo;s also useful whenever nothing is going on, to decrease the capture frequency and so save energy:\nvoid MonitorController::onAlarmStateChanged(MotionDetector::State state) { using State = MotionDetector::State; switch (state) { case State::NO_ALARM: setPicturesInterval_(config::monitorCycleDuration()); break; case State::PRE_ALARM: setPicturesInterval_(config::monitorCycleDurationOnAlarm()); break; default: break; } log::info() \u0026lt;\u0026lt; \u0026#34;Alarm state: \u0026#34; \u0026lt;\u0026lt; state; } CameraController This class is responsible to take pictures from a camera at a given rate. A user of this class can register an observer and specify the rate at which the pictures have to be taken:\nclass CameraController { public: using Subject = meta::Subject\u0026lt;const Picture\u0026amp;\u0026gt;; using Observer = Subject::Observer; using Registration = Subject::Registration; using Period = std::chrono::milliseconds; /// Set the pictures capture rate void setDesiredPeriod(Registration const\u0026amp; r, Period period); /// Add an observer to the pictures flow Registration addObserver(Observer observer, Period desiredPeriod); // ... }; This uses the observer pattern, implemented as an utility in the meta namespace.\nEvery time a picture is taken, the observer callback is called. If multiple observers are interested in different capture rates, the maximum rate is used. This means that an observer specifies the minimum speed, but it could get pictures at a higher speed, if it\u0026rsquo;s necessary for other observers.\nTo implement this functionality, in a separate thread a Metronome class sleeps the required time, and then the Camera class takes a picture. Every time an observer is registered or de-registered, the sleep time is updated.\nLiveViewController This class starts and stops the live view functionality. It doesn\u0026rsquo;t implement the functionality itself; it just controls the lifetime of a LiveView object. From the outside it takes pictures and the start and stop commands.\nWhenever a picture comes, it is forwarded to the internal LiveView object. To detect when the user is not interested in the live view anymore, there is a primitive control flow, which is basically a fixed queue of pictures sent to the browser. When the client doesn\u0026rsquo;t request them, the queue fills up. After a certain timeout with a full queue, the LiveViewController simply stops the live view:\nif (liveView_-\u0026gt;addPicture(p)) lastPictureWrittenTime_ = system_clock::now(); else if (system_clock::now() - lastPictureWrittenTime_ \u0026gt; timeout_) stop(); To do this, the internal LiveView object simply informs whether it has been able to process the image or not, and if not, the timeout is checked.\nThe stop function invokes a callback, that asks to be de-registered from the stream of pictures.\nPicture\u0026rsquo;s capture MotionDetector This class uses the OpenCV library to examine the pictures flow and determine if something is moving. It implements the observer pattern to notify the observers for the current state. The motion detection code is pretty simple:\ncv::absdiff(curr_, p, currDiff_); cv::bitwise_and(prevDiff_, currDiff_, motion_); if (motionHappened()) onMotionDetected(); else onNoMotion(); // save std::swap(prevDiff_, currDiff_); curr_ = p; The code works with three pictures: the current one and the last two. Two images are computed out of them by making a difference (i.e. subtracting the gray values of the pixels one by one) between the first with the second and the second with the third. Then a \u0026ldquo;bitwise and\u0026rdquo; is computed between them. Random noise will be filtered out, since it\u0026rsquo;s unlikely to stay still for three frames, and the image will be almost completely black. Whenever something moves however, certain areas of the pictures will differ among the three frames, and so the difference will produce white pixels. These pixels are then counted in motionHappened(), and if they exceed a certain threshold, then motion is detected.\nThere is an additional layer of protection against errors, and it\u0026rsquo;s a state machine that counts how many consecutive moving frames have been detected. These states are used to better control energy saving, picture capture and alarm notifications.\nEvery time a transition occurs in this state machine, all the observers are notified. It will be up to them to take the right action.\nEverything starts from the IDLE state. Whenever some motion is detected, the state becomes PRE_ALARM. If no more motion frames are detected, the state goes back to IDLE. If the motion continues however, the state machine transitions to ALARM. It stays there while the motion continues. When it stops, the state goes to the STILL state. This means that even though nothing is moving, for some time, the alert level is still on alarm. Indeed, if some motion happens again, the state turns immediately to ALARM again. If instead nothing happens for some time, the state goes back to IDLE.\nIn this way we have decoupled the abstract states in which the system may be with the actions the various components have to take to respond.\nCamera The camera type is statically determined in StaticConfig.hpp. In the Raspberry-Pi case, there is a homegrown version implemented by PiCamera that uses a slightly modified version of the picam library, that I found here. This library is a simple interface on top of the Raspberry userland library I forked just to ease the build. To capture images outside the Raspberry world I instead opted for the OpenCV library and implemented CvCamera. Now, I have to admit that the CvCaptureRAII class might look a bit weird, but it was an attempt to implement the camera resource through RAII. I took inspiration from Martinho Fernandez rule of zero blog post and the concern about the rule of zero by Scott Meyers. To discuss this in detail I would need an entire blog post in itself, so I\u0026rsquo;ll just point you to these valuable resources. To be honest I\u0026rsquo;m not very satisfied by its look and feel now.\nWith the same spirit I implemented the capture resource for PiCamera, which is just a one liner:\nstd::unique_ptr\u0026lt;CCamera, void(*)(CCamera*)\u0026gt; capture_; It uses the non-so-well-known custom deleter feature of std::unique_ptr. Again, look at the Fernandez\u0026rsquo;s post for an explanation on why I didn\u0026rsquo;t just implemented a stupid destructor for PiCamera. Everything is handled automatically, since in the constructor I pass the resource, and the deleter function to be called in destruction (namely picam_stop_camera):\nPiCamera::PiCamera(int width, int height) : width_(width), height_(height) , capture_(::picam_start_camera(width, height, 10, 1, false), \u0026amp;::picam_stop_camera) { // ... } These two different implementations of the camera resource were not intended to be used at the same time: one was only for the Raspberry Pi hardware, and the other for PC\u0026rsquo;s with USB cameras. For this reason I didn\u0026rsquo;t introduce any common interface, and just used a compile time define and a typedef to switch between them:\nnamespace antifurto { namespace config { #if defined(ANTIFURTO_RASPBERRY) using Camera = antifurto::PiCamera; #else using Camera = antifurto::CvCamera; #endif }} The code will simply refer to the antifurto::config::Camera type to get a capture resource. I just needed to make sure their public interface (i.e. the public methods) are the same, so the two classes could be used interchangeably.\nThis trick is quite handy if you don\u0026rsquo;t need runtime polymorphism, but honestly it\u0026rsquo;s a bit overkill for this project.\nLiveView This class is managed by the LiveViewController and is responsible to forward pictures to a ZeroMQ socket. It has a single producer / single consumer queue (see the concurrency section) and a worker thread to offload the communication.\nThe interesting part about this class is the use of a non-blocking lock-free queue, that allows minimum interruption for the producer. Whenever the queue is full, the images are discarded, and the caller is notified, in order to make some control flow, without interrupting the images flow.\nFor the communication to the webserver we use the request-reply pattern in ZeroMQ. It\u0026rsquo;s a simple protocol where at very request corresponds one reply. Reconnections are implemented in the FastCGI backend, with the ZmqLazyPirateClient class.\nPicture recording RecordingController This class is responsible for managing the registration of the pictures while an alarm is active. It accepts pictures with the void addPicture(Picture p) method and registers itself to the MotionDetector to know when to start and stop the recording. This is done by saving Jpeg pictures on the local file system (by using PictureArchive) and uploading them to Dropbox (by using DropboxUploader).\nThe state machine is quite simple:\nvoid RecordingController::onAlarmStateChanged(MotionDetector::State state) { using State = MotionDetector::State; switch (state) { case State::NO_MOTION: archive_.stopSaving(); break; case State::NO_ALARM: enqueueOlderPictures(); break; case State::ALARM: archive_.startSaving(); break; case State::PRE_ALARM: default: break; } } Whenever the motion detector notifies this class about an alarm, it starts to save the pictures. When there is no motion involved (even if the alarm is still active), the recording is stopped.\nSaving pictures in real time is important, both on disk and online. If there is a slow upload for any reason, the queue between the producer (the Camera) and the consumer (the uploader), grows. This would mean that by looking at the pictures online, the delay between capture and upload will grow more and more over time during alarms. To avoid this behavior, the queue size is limited, and whenever it\u0026rsquo;s full, the coming pictures are queued in a secondary one:\nvoid RecordingController::onPictureSaved(const std::string\u0026amp; fileName) { if (!uploadWorker_.enqueue(fileName)) { log::info() \u0026lt;\u0026lt; \u0026#34;Failed to upload picture to Dropbox: queue is full\u0026#34;; std::unique_lock\u0026lt;std::mutex\u0026gt; lock(toUploadAfterQueueMutex_); toUploadAfterQueue_.emplace(fileName); } } This ensures a fixed maximum delay between capture and upload, just by skipping pictures now and then, when the queue is full. All the missing pictures are instead uploaded when the alarm is not active anymore (the case State::NO_ALARM: above):\nwhile (!toUploadAfterQueue_.empty()) { if (uploadWorker_.enqueue(toUploadAfterQueue_.front())) toUploadAfterQueue_.pop(); else break; } // if the queue is not empty, we need to schedule another upload cycle if (!toUploadAfterQueue_.empty()) { log::info() \u0026lt;\u0026lt; \u0026#34;Cannot empty the upload queue. Schedule a new upload\u0026#34;; scheduler_.scheduleAfter(std::chrono::minutes(10), [this] { enqueueOlderPictures(); }); } The logic is a bit brutal but it works. While there is still something to upload, it adds the pictures to the upload queue. If the queue gets full again, a new procedure is scheduled after 10 minutes.\nThere is another maintenance procedure, to avoid a full hard drive. Every 24 hours, older pictures are removed. Depending on the configuration, only a certain amount of days are kept:\n// schedule maintenance at every midnight using namespace std::chrono; auto maintenanceWork = [this] { performMaintenance(); }; scheduler_.scheduleAt(concurrency::tomorrow() + minutes(1), [=] { performMaintenance(); scheduler_.scheduleEvery(hours(24), maintenanceWork); }); PictureArchive This class saves pictures in Jpeg format to a given folder. It takes a stream of pictures and two commands: startSaving and stopSaving. When the recording is started, not only the next picture is saved, but also some of the previous. This object has indeed a fixed sized circular buffer that allows to retroactively save the images right before an alarm popped up. It also allows observers to register for when a picture is saved to disk, getting the file name.\nvoid PictureArchive::save(Picture\u0026amp; p, Clock t) { std::string filename{ fs::concatPaths(currentFolder_, text::toString(t, text::ToStringFormat::FULL, \u0026#39;-\u0026#39;, \u0026#39;_\u0026#39;) + \u0026#34;.jpg\u0026#34;)}; cv::putText(p, text::toString(t, text::ToStringFormat::SHORT, \u0026#39;/\u0026#39;, \u0026#39; \u0026#39;), cv::Point(30,30), CV_FONT_HERSHEY_COMPLEX_SMALL, 0.8, cv::Scalar(200,200,250), 1, CV_AA); cv::imwrite(filename, p, {CV_IMWRITE_JPEG_QUALITY, 90}); notifyObservers(filename); } The picture gets a timestamp text overlay on the top left corner and then is saved on disk.\nOn the bad side there is the ring buffer, which is actually not a ring buffer at all. Pictures are pushed to the end of a vector. The beginning is then deleted by moving all the other elements at the previous index. Not pretty, not fast, but all in all it works. Moving to a proper circular buffer should not be very hard.\nDropboxUploader This class is responsible for uploading files to a Dropbox account, by using an external dropbox_uploader.sh script. It just generates a configure file for it, starting from the Antifurto\u0026rsquo;s configuration, and uploads a file when requested, by launching an external process. Nothing fancy here, I just forked andreafabrizi/Dropbox-Uploader.\nNotifications Two types of notifications are supported: WhatsApp and emails. WhatsApp have been historically fighting against bots. For this reason the phone numbers I used as source for notifications have been banned. I don\u0026rsquo;t recommend using it for this reason. A much more sane approach would have been to implement a Telegram bot instead, but at that time they didn\u0026rsquo;t exist. Email notifications are instead much more safe and reliable to use. For those two functionalities we have two very similar controllers: WhatsappNotificationController and MailNotificationController, that register themselves to the MotionDetector and whenever there is an alarm, they try to use their counterpart (WhatsappNotifier and MailNotifier) to send the notifications asynchronously. They also take care of retrials in case of errors, and avoid sending too many of them in a short period of time, to avoid flooding the receivers.\nWhatsappNotifier This class manages WhatsApp notifications. Whenever send(std::string const\u0026amp; dest, std::string const\u0026amp; msg) is called, it sends a message with yowsup-cli by spawning an external process. This class just generates the configuration file needed by Yowsup from the main process configuration and takes care of its execution.\nMailNotifier This class is responsible for sending emails.\nvoid send(ContactList const\u0026amp; dest, std::string const\u0026amp; sender, std::string const\u0026amp; subject, std::string const\u0026amp; body); It calls an external bash script that uses the Unix mail utility, to send the mail.\nUtility libraries Here I present some random notes on the utility namespaces that help with design patterns, concurrency, filesystem and logging. Some of them are a bit over-engineered but in hobby projects you also need to have some fun, don\u0026rsquo;t you? :)\nmeta namespace This namespace contains some generic patterns and algorithms that do not depend on the specific details of the project itself. In Observer.hpp you can find a generic implementation of the observer pattern. A Subject wants to provide observers the possibility to register for events. The class takes a variadic number of type parameters, that will be used in the notification. For example:\nSubject\u0026lt;int, float\u0026gt; s; auto reg = s.registerObserver([](int a, float b) { print(a, b); }); s.notify(3, 3.14); in this example we want to notify our observer with an integer and a float. To do that we just need to declare Subject with the right parameters. This will in turn be able to accept observers that respect the std::function\u0026lt;void(int, float)\u0026gt; signature.\nInteresting:\nthe registration returns a token that when goes out of scope unregisters the observer automatically; it is possible to register and unregister observers within notification callbacks (re-entrant calls are supported). Other small utilities are also present, like ErasedUniquePtr, which provides a unique pointer with an erased deleter. This is an useful workaround to a subtle problem when you want to forward declare a class and use it in an unique pointer. For more details see the type erasure post of Andrzej\u0026rsquo;s blog.\nfs This namespace contains simple path manipulation utilities to concatenate multiple paths with a single call:\nstd::string p = fs::concatPaths(\u0026#34;/var/log\u0026#34;, bar, \u0026#34;file.txt\u0026#34;); This is similar to what boost::filesystem does, but in a more functional way.\nlog This namespace contains logging utilities. The focus of this library was to provide a fast and simple logging without using macro shenanigans.\nYou can use it with a call to a free function, that will return the proper logger:\nlog::debug() \u0026lt;\u0026lt; \u0026#34;my log here \u0026#34; \u0026lt;\u0026lt; 15; There is also a reload function. When a log rotation occurs it will simply close the old file (that has been rotated) and open a new file in the same place. Ignored log levels are implemented by returning a logger that writes to a NullSink, which simply does nothing. Interestingly cryptic is the implementation of an std::outstream that does nothing. You can find it in log/NullStream.hpp.\nconcurrency This namespace contains some classes that deal with concurrency. An interesting one is SpScQueue, that wraps a worker thread and allows to enqueue work items for it. The type of the work item is templated, to maximize reusability. The queue is a lock-free implementation that can be chosen at compile time among a fixed-size and a dynamically allocated one. The former is preferred in case the maximum queue size is known at compile time.\nAs a side note I would like to add here that since the project deals with real-time data, avoiding dynamic allocations can be critical. We used fixed bound queues in all places for this reason.\nAnother interesting class in this namespace is the TaskScheduler. It provides the possibility to schedule tasks at certain time points, either one-shot or periodic:\nvoid scheduleAt(Clock::time_point t, Task w); void scheduleAfter(Clock::duration d, Task w); void scheduleEvery(Clock::duration d, Task w); The work items are processed one after the other in a worker thread, so delays added by one task impact on the next ones. It is for this reason used only for short tasks.\nipc This namespace contains classes related to child processes and inter-process communication. There is a forkAndCall function, that forks the process, calls a the given function and returns the function result by using the child process exit code:\n/// This function fork the process, calls the function in the child process, /// wait for completion and returns the function return value. ChildProcess forkAndCall(std::function\u0026lt;int()\u0026gt; f); The child process itself can be killed or waited. In the latter case, the function return code will be returned.\nIn this namespace there is also a NamedPipe class that provides Linux named pipes. The constructor creates a FIFO with the given file name, and the destructor removes it.\nThere is also an interesting PosixSignalHandler class, that handles POSIX signals safely. You need to use it carefully though: initialize it at the beginning of the main function, before any thread creation, and register all the signal handlers as soon as possible, by using:\nvoid setSignalHandler(int signal, Handler h); where an handler is a callback that takes the signal that just happened:\nusing Handler = std::function\u0026lt;void(int)\u0026gt;; The POSIX standard says that a lot of functions are not safe to be used within signal handlers. For example it\u0026rsquo;s not possible to allocate heap memory and call many standard library functions. We need however to support arbitrary code execution in the handlers, so to workaround this we use a vector of atomic booleans, one for each possible signal. Whenever a signal is sent to the process, the handler flips the corresponding boolean to true. A separate thread polls that vector, and executes the registered handlers, if any were given. This allows the signal handler to return immediately and in a safe way:\nstd::vector\u0026lt;std::atomic\u0026lt;bool\u0026gt;\u0026gt; signalsToBeHandled(SIGRTMAX); void sigactionHandler(int sig, siginfo_t* , void* ) { signalsToBeHandled[sig].store(true, std::memory_order_release); } and the user-defined handler is called asynchronously in a separate thread. This allows to execute arbitrary code.\ntext In this namespace we have some string manipulation utilities, like toString. This free function converts any list of printable objects in an std::string, e.g.\nstd::string s = text::toString(\u0026#34;my \u0026#34;, std::string(\u0026#34;s\u0026#34;), 15, true); Allowing to both covert objects into strings and concatenate them, without the need of odd std::ostringstream objects all around the codebase.\nA TextReplace class allows to do replace variable occurrences in a text with user specified values. For example:\nstd::ifstream f(\u0026#34;file.txt\u0026#34;); std::ostringstream out; text::TextReplace r; r.addVariable(\u0026#34;var\u0026#34;, \u0026#34;X\u0026#34;); r.addVariable(\u0026#34;foo\u0026#34;, \u0026#34;BAR\u0026#34;); r.replaceVariables(f, out); and suppose file.txt contains:\nreplace ${var} variables with ${foo} their values ${p}. the result of the replacement will be:\nreplace X variables with BAR their values ${p}. Note that unknown variables are left untouched.\nWebsite I am not so proud of the website code, and I don\u0026rsquo;t recommend looking at it in detail. I did not have much experience in web development at that time, but I am still quite happy with the result. Year ago it was not so obvious that a website was mobile ready:\nThe website is just a bunch HTML + JavaScript pages. For the styling and the responsive design I went with the immortals Bootstrap and JQuery, while for the server side part I used the now infamous PHP.\nCommands like start and stop monitoring and live view are issued by the frontend by doing GET requests to pages under the controller/ path. The PHP backend listening that endpoint sends POSIX signals to the main executable. The communication is not more complicated than that, because this first implementation worked fine. I didn\u0026rsquo;t bother changing it in something more complicated.\nThe funniest part of the frontend is the live view though. Also in this case the first implementation was good enough :). Basically the frontend uses an infinite loop of Ajax requests3 to a special live.jpg picture, which is served by a custom FastCGI backend, written in C++. This is the one described in the FastCGI backend section.\nfunction loadImage(url, imageObj, target) { imageObj.onload = function() { target.setAttribute(\u0026#39;src\u0026#39;, this.src); loadImage(url, imageObj, target); }; imageObj.src = url + \u0026#39;?_=\u0026#39; + new Date().getTime(); } $.ajax({ url: \u0026#39;../controller/live.php\u0026#39;, dataType: \u0026#39;json\u0026#39;, cache: false }) .done(function(data) { if (data.result == 0) { $(\u0026#39;.live-container\u0026#39;).html( \u0026#39;\u0026lt;img id=\u0026#34;liveimg\u0026#34; class=\u0026#34;img-responsive\u0026#34;\u0026gt;\u0026lt;/img\u0026gt;\u0026#39; ); var img = document.getElementById(\u0026#39;liveimg\u0026#39;); loadImage(\u0026#39;live.jpg\u0026#39;, img, new Image); } else displayMessage(\u0026#39;.live-container\u0026#39;, \u0026#39;\u0026lt;h4\u0026gt;Ooops...\u0026lt;/h4\u0026gt;\u0026#39; + \u0026#39;\u0026lt;p\u0026gt;\u0026#39; + data.log + \u0026#39;\u0026lt;/p\u0026gt;\u0026#39;, \u0026#39;alert-danger\u0026#39;); }) .fail(function(jqxhr, textStatus, errorThrown) { displayMessage(\u0026#39;.live-container\u0026#39;, \u0026#39;\u0026lt;h4\u0026gt;Ooops...\u0026lt;/h4\u0026gt;\u0026#39; + \u0026#39;\u0026lt;p\u0026gt;\u0026#39; + errorThrown + \u0026#39;\u0026lt;/p\u0026gt;\u0026#39;, \u0026#39;alert-danger\u0026#39;); }); Yes, that\u0026rsquo;s it. I didn\u0026rsquo;t even have to shorten the code. Something that I couldn\u0026rsquo;t explain myself here was that in loadImage I couldn\u0026rsquo;t use JQuery, because it was much slower than the old style setAttribute and image.src = url. So I decided to live with that.\nThe archive page shows pictures from previous alarms. Just don\u0026rsquo;t look at the PHP code behind that, it\u0026rsquo;s really horrible crap. It can give you nightmares for days.\nThis is the carusel view:\nThis is the mobile version:\nAnd this is the date selector for the alarm, in the mobile version:\nFastCGI backend One of the website backend components is ironically in a folder called frontend. The name is unfortunate but it was meant to suggest an interface to the main executable. It communicates with it via a ZeroMQ socket, and with the web server through FastCGI.\nInterestingly enough, the first implementation was in Python, but it was too slow. I had to re-implement it in C++, and now it\u0026rsquo;s about three orders of magnitude faster (yes, I really mean 1000X).\nThe main.cpp file contains all the logic:\nA webserver request is directed to the executable through the standard input (which is ignored); a picture is requested to the main antifurto executable through a ZeroMQ request; as soon as a reply arrives, it is immediately written to the standard output, that is read by the webserver. There are a bunch of utility classes that have been used to make the code cleaner, described in the following sections.\nZmqLazyPirateClient This class implements the Lazy pirate pattern in ZeroMQ, which is a request-reply transition supporting socket reconnections. This allows to start and stop the main executable and the webserver independently; the connection between them will catch up automatically. When a request-reply transaction is needed, this class will send the request and wait until the reply comes, or a timeout expires. On timeout, the request is sent again, until the maximum number of retrials is reached. At that point the transaction is considered failed.\nStream utilities The StreamRedirector class is responsible for redirecting the standard input and output to FastCGI stream buffers, while StreamReader allows to buffer reads from a stream (in this case the standard input). I actually don\u0026rsquo;t remember because it\u0026rsquo;s a class instead of a simple function. Probably it\u0026rsquo;s a non-sense.\nConclusion In this post we had a look at the pet project I worked on for a while some years ago. By skimming through this post again I realized that it is mostly a random collection of impressions, design decisions and code snippets, so I don\u0026rsquo;t know how effective that is for a reader. However, for me it was important to wrap up, because after all the time and effort spent, I didn\u0026rsquo;t want to forget it, and I also wanted to share my insights with the community.4\nMy takeaways are that with this project I learned some stuff and I did something useful for myself. I would definitely recommend working on things you really need, as opposed to experimenting with technologies purposelessly. It really helps to get them done (to a certain extent at least). Or at least that\u0026rsquo;s the only way I found preventing me to give up projects too early.5\nI hope this post gave you some interesting insights and maybe inspire you some extensions, related projects or ideas. The code is open source on GitHub, under mbrt/antifurto, as I wrote earlier. I encourage you to take a look yourself to some of the classes. You can also build it and use it as is for your own security alarm. The deployment is kind of a pain right now, because there are many dependencies and configuring the external services is not exactly easy to do (Dropbox, mails, WhatsApp, etc). The documentation is also somehow lacking; apologies for that.\nThat\u0026rsquo;s all folks!\nTake a look at the meta namespace for the implementation.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nIf you are curious, the ErasedUniquePtr class is briefly described in the meta namespace section.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nYes, I know WebSocket existed already years ago, but really, at that time my phone didn\u0026rsquo;t support them, and I didn\u0026rsquo;t feel like developing two different protocols.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThe license is GPL.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nSee do finish your stuff by Martin Sústrik.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://blog.mbrt.dev/posts/antifurto/","summary":"\u003ch1 id=\"introduction\"\u003eIntroduction\u003c/h1\u003e\n\u003cp\u003eIn February 2014 I had a very unpleasant surprise after coming back home from a\nweekend in Berlin. Me and my girlfriend found out our apartment upside down\nbecause of burglars. We suffered the loss not only of many of our belongings,\nbut worse, of our safety. We didn\u0026rsquo;t feel safe in our home and we didn\u0026rsquo;t feel\nsafe to leave it - not even for half an hour - for the fear to come back and\nfind somebody inside. It was with this mood that I started this project: totally\nmotivated to do something about it.\u003c/p\u003e","title":"Antifurto: home made security camera"},{"content":"I\u0026rsquo;ve been playing around with Rust for a year and a half, and the best part of it, like many others say, has been the very helpful community. There are a lot of online resources that help you to get started: the Rust book, the Rustonomicon and many blog posts and stack overflow questions. After I learned the basics I felt a bit lost though. I couldn\u0026rsquo;t find enough resources for intermediate-level-Rustaceans. I\u0026rsquo;m a C++ developer in my daily job, and so I\u0026rsquo;m used with books like Effective C++ from Scott Meyers, the Herb Sutter\u0026rsquo;s blog and a lot of online resources that always helped me with advanced C++ topics (that are a lot\u0026hellip; :sigh:). Those resources teach you how to get the best from the language, how to use it properly, and how to structure your code to be more clear and effective. Those resources are not completely absent in the Rust community, but neither common.\nHow do you learn those things then? Well, there are two ways in my opinion: you spend a lot of time and learn by doing, or you look at some good code. I think code reviews are incredibly useful; you can see how other people reason about problems you also struggled with, and how they have solved them. This post attempts to target those intermediate-level-Rustaceans (like me), by looking at the ripgrep crate by Andrew Gallant, a great example of good Rust.\nIntroduction I\u0026rsquo;m not going to explain everything about the crate, since there is already a very good blog post by Andrew himself, explaining how the application works from a functional perspective, and some used algorithms. We are going instead to walk through the crate architecture. I\u0026rsquo;m going to take for granted some of the basics, so if you need a refresher you can take a look at the resources I mentioned above.\nWe are going to look at this specific version of the crate:\n$ git describe 0.2.5-4-gf728708 which is the last one at the time of writing. By the time you are reading this, however, the crate might have evolved, so if you want to look at the code by yourself while reading, you should checkout this specific version:\n$ git clone https://github.com/BurntSushi/ripgrep.git $ cd ripgrep $ git checkout f728708 and without further ado, let\u0026rsquo;s get started.\nThe big picture ripgrep is a command line tool for searching file contents using regular expressions, similarly to GNU grep. The tool is split across four crates: the main one (ripgrep), ignore, grep and globset.\nThe grep crate provides line-by-line regex searching from a buffer and it is used only by the main crate. The globset crate uses regex to perform glob matching over paths. It is used by the main and the ignore crates. The ignore crate implements directory walking, ignore and include patterns. It uses the glob crate for that. Finally, the main crate, that glues everything together, implements command line argument parsing, output handling and multi-threading.\nOne clear advantage of splitting an application in multiple crates is that this forces you to keep your code scoped. It\u0026rsquo;s easy to create a mess of dependencies among the components if everything is in the same crate (or, even worse, in the same module). If you instead take a part of your application and try to give it a meaning by itself, you\u0026rsquo;ll end up with a more generic, usable and clearer interface. Embrace the Single responsibility principle and let it be your guide, like ripgrep clearly does.\nMain Everything starts from the ripgrep main function:\nfn main() { match Args::parse().and_then(run) { Ok(count) if count == 0 =\u0026gt; process::exit(1), Ok(_) =\u0026gt; process::exit(0), Err(err) =\u0026gt; { eprintln!(\u0026#34;{}\u0026#34;, err); process::exit(1); } } } It is very concise: it parses the command line arguments and then passes them to the run function. In between, there is the Result::and_then combinator, so the match statement gets to the Ok branch only if both operations succeed. If not, it selects the Err branch, handling errors for both the first and the second operation. Then the exit code depends on whether the count for matches is not zero.\nfn run(args: Args) -\u0026gt; Result\u0026lt;u64\u0026gt; { // ... } The run function at first decides if it\u0026rsquo;s worth to spawn threads or not, and if so, this is the way it setups the things:\nThe main thread, controlled by the run function digs files from the file system, and pushes them into a deque. This is a Single-producer / Multiple-consumers queue, from which multiple worker threads can pull at the same time. They will in turn perform the search operations. Here is the workers initialization in the run function:\nlet workq = { let (workq, stealer) = deque::new(); for _ in 0..threads { let worker = MultiWorker { chan_work: stealer.clone(), // initialize other fields... }; workers.push(thread::spawn(move || worker.run())); } workq }; As you can see, the deque::new() returns two objects. The queue is indeed composed by two ends: one is the workq from which the main thread can push, and the other end is the stealer, from which all the workers can pull. The loop creates a bunch of workers and move them to new threads, along with a stealer. Note that the stealer is cloneable, but this doesn\u0026rsquo;t mean that the queue itself is cloned. Internally indeed the stealer contains an Arc to the queue:\npub struct Stealer\u0026lt;T: Send\u0026gt; { deque: Arc\u0026lt;Deque\u0026lt;T\u0026gt;\u0026gt;, } To note here is the beauty of the deque interface. To express the fact that the producer is only one, but the consumers can be multiple, the type is split in two: the producer is then Send but not Sync, nor Clone. There is no way to use it from multiple threads, since you can move it to another thread, but in that case you lose your reference to it. The Stealer, which is the other end, is instead both Send and Clone. You can then pass it around by cloning and sending the copies off to other threads; they all refer to the same queue. There is no way to use this interface incorrectly.\nAnother thing to note here is that the workq variable is initialized by a block that returns just the producer part of a new deque. Inside the block, the workers along with their stealers are moved into new worker threads and those are in turn pushed into a vector. Using a block that just returns what it\u0026rsquo;s needed for the rest of the function is a good practice. In this way the run function is not polluted with variables that are not usable anymore because their values have been moved.\nThis is the MultiWorker struct, that runs in a separate thread:\nstruct MultiWorker { chan_work: Stealer\u0026lt;Work\u0026gt;, quiet_matched: QuietMatched, out: Arc\u0026lt;Mutex\u0026lt;Out\u0026gt;\u0026gt;, #[cfg(not(windows))] outbuf: Option\u0026lt;ColoredTerminal\u0026lt;term::TerminfoTerminal\u0026lt;Vec\u0026lt;u8\u0026gt;\u0026gt;\u0026gt;\u0026gt;, #[cfg(windows)] outbuf: Option\u0026lt;ColoredTerminal\u0026lt;WindowsBuffer\u0026gt;\u0026gt;, worker: Worker, } the first field is the stealer. As you can see from its type, the stealer receives Work structs:\nenum Work { Stdin, File(DirEntry), Quit, } The main thread will push them from its workq variable:\nfor dent in args.walker() { if quiet_matched.has_match() { break; } paths_searched += 1; if dent.is_stdin() { workq.push(Work::Stdin); } else { workq.push(Work::File(dent)); } } The args.walker() is an iterator over the files to search, or the standard input stream, if the - argument is passed. In the former case it pushes a Work::File entry with the path, in the latter a Work::Stdin entry. The items are received in the MultiWorker::run function. It is a loop that pops items from the deque and process them one by one:\nloop { if self.quiet_matched.has_match() { break; } let work = match self.chan_work.steal() { Stolen::Empty | Stolen::Abort =\u0026gt; continue, Stolen::Data(Work::Quit) =\u0026gt; break, Stolen::Data(Work::Stdin) =\u0026gt; WorkReady::Stdin, Stolen::Data(Work::File(ent)) =\u0026gt; { match File::open(ent.path()) { Ok(file) =\u0026gt; WorkReady::DirFile(ent, file), Err(err) =\u0026gt; { eprintln!(\u0026#34;{}: {}\u0026#34;, ent.path().display(), err); continue; } } } }; // ... } The steal() method tries to pop from the deque and returns a Stolen instance:\npub enum Stolen\u0026lt;T\u0026gt; { /// The deque was empty at the time of stealing Empty, /// The stealer lost the race for stealing data, and a retry may return more /// data. Abort, /// The stealer has successfully stolen some data. Data(T), } The outcome is matched against the different possibilities, but only Stolen::Data contains a Work entry. Both Stdin and File entry types are then translated into a WorkReady instance. In the second case the file is then opened with an std::fs::File. The work variable is later consumed by a Worker instance:\nself.worker.do_work(\u0026amp;mut printer, work); We\u0026rsquo;ll get back to that in a moment, but let\u0026rsquo;s first backtrack to the MultiWorker::run loop. The Work::Quit case breaks it, so the thread terminates:\nlet work = match self.chan_work.steal() { // ... Stolen::Data(Work::Quit) =\u0026gt; break, // ... }; This value is pushed by the main thread after it walks through all the files.\nfor _ in 0..workers.len() { workq.push(Work::Quit); } let mut match_count = 0; for worker in workers { match_count += worker.join().unwrap(); } The threads are all guaranteed to terminate because the number of Quit messages pushed is the same as the number of workers. A worker can only consume one of them and then quit. This implies, since no messages can be lost, that all the workers will get the message at some point, and then terminate. All the workers threads are then joined, waiting for completion.\nTo recap, this is a the multi-threading pattern used:\na deque in between a producer (that provides the work items) and a bunch of consumers (that do the heavy lifting) in separate threads; the deque carries an enumeration of the things to do, and one of them is the Quit action; the producer will eventually push a bunch of Quit messages to terminate the worker threads (one per thread). In case you just have one type of job, it makes perfect sense to use an Option\u0026lt;Stuff\u0026gt; as work item, instead of an enumeration. The workers have then to terminate in case None is passed. The Option can be used also in the ripgrep case instead of the Quit message, but I\u0026rsquo;m not sure the code would be more readable:\nlet work = match self.chan_work.steal() { Stolen::Empty | Stolen::Abort =\u0026gt; continue, Stolen::Data(None) =\u0026gt; break, Stolen::Data(Some(Work::Stdin)) =\u0026gt; WorkReady::Stdin, Stolen::Data(Some(Work::File(ent)) =\u0026gt; { // ... } }; Mono thread ripgrep can also operate in a single thread, in case there is only one file to search or only one core to use, or the user says so. The run function checks that:\nlet threads = cmp::max(1, args.threads() - 1); let isone = paths.len() == 1 \u0026amp;\u0026amp; (paths[0] == Path::new(\u0026#34;-\u0026#34;) || paths[0].is_file()); // ... if threads == 1 || isone { return run_one_thread(args.clone()); } and calls the run_one_thread function (I have removed some uninteresting details from it):\nfn run_one_thread(args: Arc\u0026lt;Args\u0026gt;) -\u0026gt; Result\u0026lt;u64\u0026gt; { let mut worker = Worker { args: args.clone(), inpbuf: args.input_buffer(), grep: args.grep(), match_count: 0, }; // ... for dent in args.walker() { // ... if dent.is_stdin() { worker.do_work(\u0026amp;mut printer, WorkReady::Stdin); } else { let file = match File::open(dent.path()) { Ok(file) =\u0026gt; file, Err(err) =\u0026gt; { eprintln!(\u0026#34;{}: {}\u0026#34;, dent.path().display(), err); continue; } }; worker.do_work(\u0026amp;mut printer, WorkReady::DirFile(dent, file)); } } // ... } As you can see, the function uses a single Worker and if you remember, this struct is used by MultiWorker too. The files to search are iterated by args.walker() as before and each entry is passed to the worker, as before. The use of Worker in both cases allows code reuse to a great extent.\nThe file listing We are now going to look over the file listing functional block.\nThe default operation mode of ripgrep is to search recursively for non-binary, non-ignored files starting from the current directory (or from the user specified paths). To enumerate the files and feed the search engine, ripgrep uses the ignore crate.\nBut let\u0026rsquo;s start from the beginning: the walker function. It returns a Walk instance, it is constructed by Args and used by the run function in main:\npub fn walker(\u0026amp;self) -\u0026gt; Walk; Walk is just a simple wrapper around the ignore::Walk struct. A value of this struct can be created by using its new method:\npub fn new\u0026lt;P: AsRef\u0026lt;Path\u0026gt;\u0026gt;(path: P) -\u0026gt; Walk; or with a WalkBuilder, that implements the builder pattern. This allows to customize the behavior without annoying the users of the library, since it frees them from the burden to provide a lot of parameters to the constructor, when just the default values are needed:\nlet w = WalkBuilder::new(path).ignore(true).max_depth(Some(5)).build(); In this example we have created a WalkBuilder with default arguments and just override the ignore and max_depth options.\nThe implementation of the type is not very interesting from our point of view. It is basically an Iterator that walks through the file system by using the walkdir crate, but ignores the files and directories listed in .gitignore and .ignore files possibly present, with the help of the Ignore type. We\u0026rsquo;ll look at that type a bit later. Let\u0026rsquo;s look at the Error type first:\n/// Represents an error that can occur when parsing a gitignore file. #[derive(Debug)] pub enum Error { Partial(Vec\u0026lt;Error\u0026gt;), WithLineNumber { line: u64, err: Box\u0026lt;Error\u0026gt; }, WithPath { path: PathBuf, err: Box\u0026lt;Error\u0026gt; }, Io(io::Error), Glob(String), UnrecognizedFileType(String), InvalidDefinition, } This error type has an interesting recursive definition. The Partial case of the enumeration contains a vector of Error instances, for example. WithLineNumber adds line information to an Error.1 Then the error::Error, fmt::Display and Fromio::Error traits are implemented, to make it a proper error type and to easily construct it out an io::Error. Here, the necessary boilerplate to crank up the error type are handcrafted. Another possibility could have been to use the quick-error macro, which reduces the burden to implement error types to a minimum.2\nIgnore patterns Ignore patterns are handled within the ignore crate by the Ignore struct. This type connects directory traversal with ignore semantics. In practice it builds a tree-like data structure that mimics the directories tree, in which nodes are ignore contexts. The implementation is quite complicated, but let\u0026rsquo;s give it a brief look:3\n#[derive(Clone, Debug)] pub struct Ignore(Arc\u0026lt;IgnoreInner\u0026gt;); #[derive(Clone, Debug)] struct IgnoreInner { compiled: Arc\u0026lt;RwLock\u0026lt;HashMap\u0026lt;OsString, Ignore\u0026gt;\u0026gt;\u0026gt;, dir: PathBuf, overrides: Arc\u0026lt;Override\u0026gt;, types: Arc\u0026lt;Types\u0026gt;, parent: Option\u0026lt;Ignore\u0026gt;, is_absolute_parent: bool, absolute_base: Option\u0026lt;Arc\u0026lt;PathBuf\u0026gt;\u0026gt;, explicit_ignores: Arc\u0026lt;Vec\u0026lt;Gitignore\u0026gt;\u0026gt;, ignore_matcher: Gitignore, git_global_matcher: Arc\u0026lt;Gitignore\u0026gt;, git_ignore_matcher: Gitignore, git_exclude_matcher: Gitignore, has_git: bool, opts: IgnoreOptions, } The Ignore struct is a wrapper around an atomic reference counter to the actual data (namely, the IgnoreInner). A first interesting field inside that struct is parent, that is an Option\u0026lt;Ignore\u0026gt;. It points to a parent entry if present. So, this is where the tree structure comes from: the Arc can be shared, so multiple Ignore can share the same parent. But that\u0026rsquo;s not all; they can also be cached in the compiled field, that has a quite complex type:\nArc\u0026lt;RwLock\u0026lt;HashMap\u0026lt;OsString, Ignore\u0026gt;\u0026gt;\u0026gt; This is the cache of Ignore instances that is shared among all of them. Let\u0026rsquo;s try to break it down:\nthe HashMap maps paths to Ignore instances (as expected); the RwLock allows the map to be shared and modified across different threads, without causing data races; and finally the Arc allow the cache to be owned safely by different owners in different threads. Every time a new Ignore instance has to be built and added to a tree, the implementation first looks in the cache, trying to reuse the existing instances. The tree is built dynamically, while crawling the directories, looking for the specific ignore files (e.g. .gitignore, .ignore, or .rgignore). The tree gets also custom ignore patterns from the command line, and adds them to the tree too.\nAnother interesting bit here is the add_parents signature for Ignore:\npub fn add_parents\u0026lt;P: AsRef\u0026lt;Path\u0026gt;\u0026gt;(\u0026amp;self, path: P) -\u0026gt; (Ignore, Option\u0026lt;Error\u0026gt;); Instead of returning a Result\u0026lt;Ignore, Error\u0026gt;, it returns a pair, that contains always a result and optionally an error. In this way partial failures are allowed. If you remember, the error value can also be a vector of errors, so the function can collect them while working, but then it can also return a (maybe partial) result in the end. I found this approach very interesting.\nThe search process In this section we will look at how the regex search inside a file is implemented. This process involves some modules in ripgrep and also the grep crate.\nEverything starts from Worker::do_work in main.rs. Based on the type of the file passed in, it calls search or search_mmap. The first function is used to read the input one chunk at a time and then search, while the second is used to search into a memory mapped input. In this case there is no need to read the file into a buffer, because it is already available in memory, or more precisely, the kernel will take care of this illusion.\nThe search function just creates a new Searcher and calls run on it.\nimpl\u0026lt;\u0026#39;a, R: io::Read, W: Terminal + Send\u0026gt; Searcher\u0026lt;\u0026#39;a, R, W\u0026gt; { pub fn run(mut self) -\u0026gt; Result\u0026lt;u64, Error\u0026gt;; } The first interesting thing to note here is that the run function actually consumes self, so you can\u0026rsquo;t actually run the method twice. Why is that? Let\u0026rsquo;s have a look at the new method, that creates this struct:\nimpl\u0026lt;\u0026#39;a, R: io::Read, W: Terminal + Send\u0026gt; Searcher\u0026lt;\u0026#39;a, R, W\u0026gt; { pub fn new(inp: \u0026amp;\u0026#39;a mut InputBuffer, printer: \u0026amp;\u0026#39;a mut Printer\u0026lt;W\u0026gt;, grep: \u0026amp;\u0026#39;a Grep, path: \u0026amp;\u0026#39;a Path, haystack: R) -\u0026gt; Searcher\u0026lt;\u0026#39;a, R, W\u0026gt;; } It takes a bunch of arguments and stores them into a new Searcher instance. All the arguments to Searcher are passed as reference, except haystack which is the Read stream representing the file. This means that when this struct will be destroyed, the file will be gone too. Whenever you complete the search for a file, you don\u0026rsquo;t have to do it again, indeed. You can enforce this usage by consuming the input file in the run function, or take its ownership in the constructor and force the run function to consume self.\nSince we cannot run the search twice using the same Searcher instance, why don\u0026rsquo;t we just use a function then? The approach used here has several advantages:\nyou get the behavior that the search cannot be run twice with the same file (but that\u0026rsquo;s nothing that a free function could not do); you can split the function among different private functions, without passing around all the arguments; they will all take self by reference (maybe also \u0026amp;mut self) and just use the member variables. So, instead of:\nfn helper1(inp: \u0026amp;mut InputBuffer, printer: \u0026amp;mut Printer\u0026lt;W\u0026gt;, grep: \u0026amp;Grep, path: \u0026amp;Path, haystack: \u0026amp;mut R) { // do something with path, grep, etc } we have:\nfn helper1(\u0026amp;mut self) { // do something with self.path, self.grep, etc } The end result is much nicer.\nThe first variable that the Searcher takes is an InputBuffer. It is defined in the same search_stream module, and it provides buffering for the input file. It has the interesting feature to be able to keep part of the data across reads. This is needed, for example, when the user requests context lines, or when a single read is not enough to reach the next end of line.\nThe fill function in the InputBuffer, reads from the input and optionally rolls over the contents of the buffer starting from the keep_from index:\nfn fill\u0026lt;R: io::Read\u0026gt;(\u0026amp;mut self, rdr: \u0026amp;mut R, keep_from: usize) -\u0026gt; Result\u0026lt;bool, io::Error\u0026gt;; The interesting implementation bit here is that the buffer grows whenever it needs more room, but it never shrinks. This avoids some re-allocations, at the expense of some memory. This approach is perfectly fine in this case, since the application is intended to work in one shot and then terminate. In a long running application such as a webserver, this is probably not what you want to do.\nAfter the buffer has been filled, the Grep matcher runs, and in case of a match, it prints the results according to the options (context lines, line numbers, etc.).\nNote that Searcher takes the input buffer by mutable reference. This means that it can be reused for the next file, without allocating new memory for the buffer with a new Searcher instance.\nI\u0026rsquo;ll be skipping most of the implementation review here, even if the code may be interesting. Most of it however is not very relevant outside this specific case. If you are interesting you can skim through the search_stream module code.\nThe other case is covered by the search_mmap function, that creates a BufferSearcher, defined in the search_buffer module, and calls run on it, like in the Searcher case:\nimpl\u0026lt;\u0026#39;a, W: Send + Terminal\u0026gt; BufferSearcher\u0026lt;\u0026#39;a, W\u0026gt; { pub fn run(mut self) -\u0026gt; u64; } The same reasoning applies here: the struct is created and used only once for one file, because the run function takes self by value. The purpose of the search_buffer module is to search inside a file completely contained in a single buffer, instead of a stream. This buffer is provided by a memory mapped file, and it\u0026rsquo;s used only when a stream would be slower.4 This module reuses some types provided by the search_stream module:\nuse search_stream::{IterLines, Options, count_lines, is_binary}; Notably, it does not use the InputBuffer, since there is nothing to buffer here: everything is already available in the given array. The implementation is very basic, and it doesn\u0026rsquo;t support some of the features the other module does (like showing context lines).\nNo big surprises here. The only minor weak point for me is that this module depends on the search_stream one. It doesn\u0026rsquo;t actually build on top of it, but it just imports some functionality. I\u0026rsquo;d rather try to move the common implementation in another module from which they can both import. This makes sense, since the common stuff is indeed not specific to either of the modules.\nThe grep crate The grep crate provides all you need to regex search into a line. It builds on top of the Rust regex crate and adds some optimizations in the literal module. The result of a search is a Match instance, which is simply a position inside that buffer:\n#[derive(Clone, Debug, Default, Eq, PartialEq)] pub struct Match { start: usize, end: usize, } The Grep type is cloneable. This is important, since it can be built once (which is an expensive operation) and then cloned to all the worker threads:\n#[derive(Clone, Debug)] pub struct Grep { re: Regex, required: Option\u0026lt;Regex\u0026gt;, opts: Options, } I won\u0026rsquo;t dig into the implementation details, since they are already very well covered in the already mentioned Andrew\u0026rsquo;s blog post.\nOutput handling The last bit we are going to investigate now is the output handling. The challenge here is that ripgrep needs to write from multiple threads to a single console avoiding to interleave the results.\nHere is how the run function in our MultiWorker handles that:\nlet mut outbuf = self.outbuf.take().unwrap(); outbuf.clear(); let mut printer = self.worker.args.printer(outbuf); self.worker.do_work(\u0026amp;mut printer, work); // ... let outbuf = printer.into_inner(); if !outbuf.get_ref().is_empty() { let mut out = self.out.lock().unwrap(); out.write(\u0026amp;outbuf); } self.outbuf = Some(outbuf); An output buffer is taken from self and passed to a printer. The printer is then passed to the worker, that uses it to print the results. So far all the output went to the buffer, and not to the actual console. Then, if anything has been buffered, lock the output, that is shared across all the workers, and write everything. The output buffer is reused in this interesting way: it is kept as an Option field inside the MultiWorker itself. For every file, it is taken from the option, passed by value to a Printer, and then when the Printer is done, put it back in the Option. This allows to keep it mutable and pass it around by value without creating it every time.\nThe trick used here, to avoid to interleave the prints, is to buffer all the matches found in a file into a \u0026ldquo;virtual terminal\u0026rdquo; that doesn\u0026rsquo;t print to the console. After the search in that file is done, the output is written in one shot, by locking a shared Out object and write the buffer contents to the actual console.\nLet\u0026rsquo;s take a look at the various types involved. The MultiWorker keeps a ColoredTerminal instance in its self.outbuf field. Its type depends on the platform:\n#[cfg(not(windows))] outbuf: Option\u0026lt;ColoredTerminal\u0026lt;term::TerminfoTerminal\u0026lt;Vec\u0026lt;u8\u0026gt;\u0026gt;\u0026gt;\u0026gt;, #[cfg(windows)] outbuf: Option\u0026lt;ColoredTerminal\u0026lt;WindowsBuffer\u0026gt;\u0026gt;, The self.out is the same in all the platforms:\nlet out: Arc\u0026lt;Mutex\u0026lt;Out\u0026gt;\u0026gt;; As you can see, it can be shared and mutated by multiple threads, because it is wrapped in a Mutex and an Arc. Inside an Out instance, there is the terminal used to write directly to the console:\n#[cfg(not(windows))] let term: ColoredTerminal\u0026lt;term::TerminfoTerminal\u0026lt;io::BufWriter\u0026lt;io::Stdout\u0026gt;\u0026gt;\u0026gt;; #[cfg(windows)] let term: ColoredTerminal\u0026lt;WinConsole\u0026lt;io::Stdout\u0026gt;\u0026gt;; A ColoredTerminal that refers to a TerminfoTerminal on Linux, and to a WinConsole on Windows. They are both structs defined in the term crate.\nBut let\u0026rsquo;s step back a and describe all these types a little bit better. The Searcher uses a Printer whenever a match is found and the output is enabled. The Printer is defined in the printer module and it encapsulates the general output logic. It knows how to print a match, given some options, and forwards the writes to an inner Terminal type.\npub struct Printer\u0026lt;W\u0026gt; { wtr: W, has_printed: bool, column: bool, context_separator: Vec\u0026lt;u8\u0026gt;, eol: u8, file_separator: Option\u0026lt;Vec\u0026lt;u8\u0026gt;\u0026gt;, heading: bool, line_per_match: bool, null: bool, replace: Option\u0026lt;Vec\u0026lt;u8\u0026gt;\u0026gt;, with_filename: bool, color_choice: ColorChoice } Note that I took the comments out to make it shorter. As you can see, there is a generic writer W (that is taken by value) and a lot of other options. This generic parameter is expected to implement term::Terminal and Send, as you can see in the implementation:\nimpl\u0026lt;W: Terminal + Send\u0026gt; Printer\u0026lt;W\u0026gt; { // printer implementation } The struct uses the builder pattern again, but in a slightly different flavor. The new method takes only a Terminal and sets all the options with a default value. To change them, the user needs to call the various builder methods, directly on the Printer itself, not on another builder helper. For example:\npub fn heading(mut self, yes: bool) -\u0026gt; Printer\u0026lt;W\u0026gt; { self.heading = yes; self } takes self by mutable value and, after changing the heading option, returns self by value again.\nThe implementation is simple. The public interface provides some methods to print the various match components, like the path, the context separator and the line contents. The only thing that is still not clear to me is why the Send trait is also needed, since I don\u0026rsquo;t see any threading in the struct implementation, and all the print methods require a mutable self, e.g.:\npub fn context_separate(\u0026amp;mut self) { // N.B. We can\u0026#39;t use `write` here because of borrowing restrictions. if self.context_separator.is_empty() { return; } self.has_printed = true; let _ = self.wtr.write_all(\u0026amp;self.context_separator); let _ = self.wtr.write_all(\u0026amp;[self.eol]); } In any case, the implementation is more or less straight forward, and in the end all the writes are directed to the inner Terminal.\nIn the Linux case, the Terminal is the default one provided by the term crate itself: TerminfoTerminal. On Windows ripgrep provides a custom implementation, since the coloring needs a special treatment, to avoid performance hurt:\nThis particular implementation is a bit idiosyncratic, and the \u0026#34;in-memory\u0026#34; specification is to blame. In particular, on Windows, coloring requires communicating with the console synchronously as data is written to stdout. This is anathema to how ripgrep fundamentally works: by writing search results to intermediate thread local buffers in order to maximize parallelism. Eliminating parallelism on Windows isn\u0026#39;t an option, because that would negate a tremendous performance benefit just for coloring. We\u0026#39;ve worked around this by providing an implementation of `term::Terminal` that records precisely where a color or a reset should be invoked, according to a byte offset in the in memory buffer. When the buffer is actually printed, we copy the bytes from the buffer to stdout incrementally while invoking the corresponding console APIs for coloring at the right location. The implementation is provided by WindowsBuffer:\n/// An in-memory buffer that provides Windows console coloring. #[derive(Clone, Debug)] pub struct WindowsBuffer { buf: Vec\u0026lt;u8\u0026gt;, pos: usize, colors: Vec\u0026lt;WindowsColor\u0026gt;, } /// A color associated with a particular location in a buffer. #[derive(Clone, Debug)] struct WindowsColor { pos: usize, opt: WindowsOption, } /// A color or reset directive that can be translated into an instruction to /// the Windows console. #[derive(Clone, Debug)] enum WindowsOption { Foreground(Color), Background(Color), Reset, } This struct implements terminfo::Terminal as we said before, and it contains a buffer of characters to print, a position on the buffer itself, and a vector of colors and positions. Whenever the write is called, the output is buffered in self.buf:\nimpl io::Write for WindowsBuffer { fn write(\u0026amp;mut self, buf: \u0026amp;[u8]) -\u0026gt; io::Result\u0026lt;usize\u0026gt; { let n = try!(self.buf.write(buf)); self.pos += n; Ok(n) } fn flush(\u0026amp;mut self) -\u0026gt; io::Result\u0026lt;()\u0026gt; { Ok(()) } } and whenever a coloring option is passed, it is pushed into the colors vector, along with the current position:\nimpl Terminal for WindowsBuffer { type Output = Vec\u0026lt;u8\u0026gt;; fn fg(\u0026amp;mut self, fg: Color) -\u0026gt; term::Result\u0026lt;()\u0026gt; { self.push(WindowsOption::Foreground(fg)); Ok(()) } // ... } Then, when the higher level logic decides it\u0026rsquo;s time to print everything, the print_stdout is called, passing another terminal (the real one, linked with the console):\n/// Print the contents to the given terminal. pub fn print_stdout\u0026lt;T: Terminal + Send\u0026gt;(\u0026amp;self, tt: \u0026amp;mut T) { if !tt.supports_color() { let _ = tt.write_all(\u0026amp;self.buf); let _ = tt.flush(); return; } let mut last = 0; for col in \u0026amp;self.colors { let _ = tt.write_all(\u0026amp;self.buf[last..col.pos]); match col.opt { WindowsOption::Foreground(c) =\u0026gt; { let _ = tt.fg(c); } WindowsOption::Background(c) =\u0026gt; { let _ = tt.bg(c); } WindowsOption::Reset =\u0026gt; { let _ = tt.reset(); } } last = col.pos; } let _ = tt.write_all(\u0026amp;self.buf[last..]); let _ = tt.flush(); } Here, if the terminal does not support coloring, there is nothing special to do, and all the buffer contents are written. Otherwise, for every color option, it writes the buffer contents until the recorded position for that option, and than it applies the option. This is repeated until the end of the buffer.\nThe terminal is not used as is by the higher level logic, but wrapped inside a ColoredTerminal instance:\n#[derive(Clone, Debug)] pub enum ColoredTerminal\u0026lt;T: Terminal + Send\u0026gt; { Colored(T), NoColor(T::Output), } The purpose of this type is simple: determine if the current terminal supports coloring, and if so use it. If not, just drop the terminal and use its internal writer type. Determine color support is a costly operation, so it\u0026rsquo;s done only once, and the result is cached in a static variable, with the help of the lazy_static crate:\nlazy_static! { // Only pay for parsing the terminfo once. static ref TERMINFO: Option\u0026lt;TermInfo\u0026gt; = { match TermInfo::from_env() { Ok(info) =\u0026gt; Some(info), Err(err) =\u0026gt; { debug!(\u0026#34;error loading terminfo for coloring: {}\u0026#34;, err); None } } }; } The type then implements some specialized constructors for a bunch of types:\nWindowsBuffer; WinConsole\u0026lt;io::Stdout\u0026gt;; and the one for the generic writer W: io::Write + Send. If the terminal then supports coloring, it uses the Colored(T) enum value (where T is T: Terminal + Send). In this case the ColoredTerminal instance contains a Terminal. In the other case, the NoColor(T::Output) value is selected and a plain io::Write is used. ColoredTerminal then implements Terminal itself in this way:\nimpl\u0026lt;T: Terminal + Send\u0026gt; term::Terminal for ColoredTerminal\u0026lt;T\u0026gt; { type Output = T::Output; fn fg(\u0026amp;mut self, fg: term::color::Color) -\u0026gt; term::Result\u0026lt;()\u0026gt; { self.map_result(|w| w.fg(fg)) } // other very similar implementations... } The intended behavior here is to forward the function to the inner terminal, if present, or return an error. A possible solution would have been to match self in this way:\nmatch *self { ColoredTerminal::Colored(ref mut w) =\u0026gt; w.fg(fg), ColoredTerminal::NoColor(_) =\u0026gt; Err(term::Error::NotSupported), } for all the functions. The solution adopted here is more elegant, as it Implements a map_result that applies the given function to the inner Terminal if it\u0026rsquo;s present and returns an error otherwise:\nimpl\u0026lt;T: Terminal + Send\u0026gt; ColoredTerminal\u0026lt;T\u0026gt; { fn map_result\u0026lt;F\u0026gt;(\u0026amp;mut self, mut f: F) -\u0026gt; term::Result\u0026lt;()\u0026gt; where F: FnMut(\u0026amp;mut T) -\u0026gt; term::Result\u0026lt;()\u0026gt; { match *self { ColoredTerminal::Colored(ref mut w) =\u0026gt; f(w), ColoredTerminal::NoColor(_) =\u0026gt; Err(term::Error::NotSupported), } } } In this way the whole Terminal implementation is just a bunch of one-liners.\nThe missing piece of this puzzle is the Out struct. The comment on top of the struct speaks for itself:\n/// Out controls the actual output of all search results for a particular file /// to the end user. /// /// (The difference between Out and Printer is that a Printer works with /// individual search results where as Out works with search results for each /// file as a whole. For example, it knows when to print a file separator.) pub struct Out { #[cfg(not(windows))] term: ColoredTerminal\u0026lt;term::TerminfoTerminal\u0026lt;io::BufWriter\u0026lt;io::Stdout\u0026gt;\u0026gt;\u0026gt;, #[cfg(windows)] term: ColoredTerminal\u0026lt;WinConsole\u0026lt;io::Stdout\u0026gt;\u0026gt;, printed: bool, file_separator: Option\u0026lt;Vec\u0026lt;u8\u0026gt;\u0026gt;, } The implementation is straightforward: whenever write is called with a ColoredTerminal as a buffer, it prints a separator (except for the first file), then prints the buffer contents and then flushes the terminal. Here is the Unix version:\n#[cfg(not(windows))] pub fn write(\u0026amp;mut self, buf: \u0026amp;ColoredTerminal\u0026lt;term::TerminfoTerminal\u0026lt;Vec\u0026lt;u8\u0026gt;\u0026gt;\u0026gt;) { self.write_sep(); match *buf { ColoredTerminal::Colored(ref tt) =\u0026gt; { let _ = self.term.write_all(tt.get_ref()); } ColoredTerminal::NoColor(ref buf) =\u0026gt; { let _ = self.term.write_all(buf); } } self.write_done(); } A very similar but not exactly equal version is provided for Windows, so there is some code duplication. It would be better to abstract these details in ColoredTerminal, providing a write_all method there, or in alternative, to introduce a new trait used by ColoredTerminal itself that does the same and than make TerminfoTerminal, WindowsBuffer and WindowsConsole to implement it.\nConcluding remarks In this post we have done a ripgrep code review, with the main focus on the design decisions and the interesting implementation solutions. The review is far from being complete, but my goal was to look at the patterns and break them down, in hope that they can be used in similar contexts by other projects.\nIn general the code is very clean, a part certain functions that would benefit from some more comments. There is however an extensive usage of #[inline(always)] and #[inline(never)] directives in the code, that I could not explain. I wonder if they have been added after profiling and if so, why the compiler have failed to identify them correctly. A possible use case is intra-crate inlining, but compiling with rustc -C lto already allows to inline everything (by slowing down compilation).5\nIn any case, I found the ripgrep crate a beautiful piece of software, from which I could learn a lot. I hope I was able to convey this beauty with this post.\nFeedback Andrew posted his feedback on Twitter and on HN. I report his comment here, because it\u0026rsquo;s relevant for some of the remarks I made:\nripgrep author here! This is a great review, thanks for doing it! I\u0026rsquo;d like to respond to a few of the bad things pointed out. :P\nThe search code is indeed in a less than ideal state. I\u0026rsquo;ve mostly avoided refactoring it because I want to move it to its own separate crate. I\u0026rsquo;ve been steadily doing this for other things. Namely, ripgrep used to be a single main crate plus a small regex handling crate (grep), but now it\u0026rsquo;s several: globset, grep, ignore, termcolor and wincolor. I\u0026rsquo;d like to roll the search code into the grep crate so that others can use it. Once that\u0026rsquo;s done, ripgrep proper will be a pretty small, limited mostly to argv handling and output handling.\nI do sometimes get overzealous with inline(always) and inline(never). Both are almost always a result of trying things while profiling, and then forgetting to remove them. If you look closely, most of them are in the core searching code where performance is quite important!\nFinally, this code review was done while I was in the middle of moving more of ripgrep code out into the `ignore` and `termcolor` crates. The `ignore` crate does all the gitignore handling (which is quite tricky and is now being used by the tokei project) and provides a parallel recursive directory iterator, which made ripgrep even faster! The `termcolor` crate handles cross platform coloring shenanigans, including Windows consoles and mintty. It wasn\u0026rsquo;t fun: issue #94… — The author did a great job reviewing the previous solution I used for colors though, and was something I really wasn\u0026rsquo;t proud of!\n— burntsushi@ on HN\nI have corrected a typo, thanks @toquetos.\nThere is some discussion going on in the r/rust subreddit, and on HN. Thank you guys for the feedback and the kind words, and thanks to Andrew for his great work.\nThat\u0026rsquo;s all folks.\nIn this case Box\u0026lt;Error\u0026gt;, since a recursive type cannot embed itself, otherwise it would be impossible to compute the size of the type.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nYou can find a good reference on the error handling topic in the Rust book.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nPlease bear in mind that I have taken out the comments to make it shorter.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nGenerally this happens when searching into a single huge file.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nSee When should I use inline.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://blog.mbrt.dev/posts/ripgrep/","summary":"\u003cp\u003eI\u0026rsquo;ve been playing around with \u003ca href=\"https://www.rust-lang.org\"\u003eRust\u003c/a\u003e for a year and a\nhalf, and the best part of it, like many others say, has been the very helpful\ncommunity. There are a lot of online resources that help you to get started: the\n\u003ca href=\"https://doc.rust-lang.org/book/\"\u003eRust book\u003c/a\u003e, the\n\u003ca href=\"https://doc.rust-lang.org/nomicon/\"\u003eRustonomicon\u003c/a\u003e and many\n\u003ca href=\"https://this-week-in-rust.org/\"\u003eblog posts\u003c/a\u003e and\n\u003ca href=\"http://stackoverflow.com/questions/tagged/rust\"\u003estack overflow questions\u003c/a\u003e.\nAfter I learned the\nbasics I felt a bit lost though. I couldn\u0026rsquo;t find enough resources for\nintermediate-level-Rustaceans. I\u0026rsquo;m a C++ developer in my daily job, and so I\u0026rsquo;m\nused with books like \u003ca href=\"http://www.aristeia.com/books.html\"\u003eEffective C++\u003c/a\u003e from\nScott Meyers, the \u003ca href=\"https://herbsutter.com/\"\u003eHerb Sutter\u0026rsquo;s blog\u003c/a\u003e and a lot of\nonline resources that always helped me with advanced C++ topics (that are a\nlot\u0026hellip; :sigh:). Those resources teach you how to get the best from the\nlanguage, how to use it properly, and how to structure your code to be more\nclear and effective. Those resources are not completely absent in the Rust\ncommunity, but neither common.\u003c/p\u003e","title":"Ripgrep code review"},{"content":"This was published in the Diary of a reverse-engineer as a guest post:\nKeygenning with KLEE\n","permalink":"https://blog.mbrt.dev/posts/keygen/","summary":"\u003cp\u003eThis was published in the\n\u003ca href=\"https://doar-e.github.io\"\u003eDiary of a reverse-engineer\u003c/a\u003e as a guest post:\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://doar-e.github.io/blog/2015/08/18/keygenning-with-klee/\"\u003eKeygenning with KLEE\u003c/a\u003e\u003c/p\u003e","title":"Keygenning with KLEE [doar-e]"}]